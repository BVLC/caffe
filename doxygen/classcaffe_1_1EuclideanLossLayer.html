<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.8"/>
<title>Caffe: caffe::EuclideanLossLayer&lt; Dtype &gt; Class Template Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Caffe
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.8 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Class&#160;List</span></a></li>
      <li><a href="classes.html"><span>Class&#160;Index</span></a></li>
      <li><a href="hierarchy.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&#160;Members</span></a></li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Classes</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Variables</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><b>caffe</b></li><li class="navelem"><a class="el" href="classcaffe_1_1EuclideanLossLayer.html">EuclideanLossLayer</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pro-methods">Protected Member Functions</a> &#124;
<a href="#pro-attribs">Protected Attributes</a> &#124;
<a href="classcaffe_1_1EuclideanLossLayer-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">caffe::EuclideanLossLayer&lt; Dtype &gt; Class Template Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>Computes the Euclidean (L2) loss <img class="formulaInl" alt="$ E = \frac{1}{2N} \sum\limits_{n=1}^N \left| \left| \hat{y}_n - y_n \right| \right|_2^2 $" src="form_25.png"/> for real-valued regression tasks.  
 <a href="classcaffe_1_1EuclideanLossLayer.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="loss__layers_8hpp_source.html">loss_layers.hpp</a>&gt;</code></p>
<div class="dynheader">
Inheritance diagram for caffe::EuclideanLossLayer&lt; Dtype &gt;:</div>
<div class="dyncontent">
 <div class="center">
  <img src="classcaffe_1_1EuclideanLossLayer.png" usemap="#caffe::EuclideanLossLayer&lt; Dtype &gt;_map" alt=""/>
  <map id="caffe::EuclideanLossLayer&lt; Dtype &gt;_map" name="caffe::EuclideanLossLayer&lt; Dtype &gt;_map">
<area href="classcaffe_1_1LossLayer.html" title="An interface for Layers that take two Blobs as input – usually (1) predictions and (2) ground-truth ..." alt="caffe::LossLayer&lt; Dtype &gt;" shape="rect" coords="0,56,219,80"/>
<area href="classcaffe_1_1Layer.html" title="An interface for the units of computation which can be composed into a Net. " alt="caffe::Layer&lt; Dtype &gt;" shape="rect" coords="0,0,219,24"/>
</map>
 </div></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:aea3a6d5454ee1a0db7cdb6c59bcfc5c8"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="aea3a6d5454ee1a0db7cdb6c59bcfc5c8"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>EuclideanLossLayer</b> (const LayerParameter &amp;param)</td></tr>
<tr class="separator:aea3a6d5454ee1a0db7cdb6c59bcfc5c8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a58a4db1e251eeb1e45d2f957a38038e5"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1EuclideanLossLayer.html#a58a4db1e251eeb1e45d2f957a38038e5">LayerSetUp</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;bottom, vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; *top)</td></tr>
<tr class="memdesc:a58a4db1e251eeb1e45d2f957a38038e5"><td class="mdescLeft">&#160;</td><td class="mdescRight">Does layer-specific setup: your layer should implement this.  <a href="#a58a4db1e251eeb1e45d2f957a38038e5">More...</a><br /></td></tr>
<tr class="separator:a58a4db1e251eeb1e45d2f957a38038e5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac2ef07f6aa25f31c1e955f66b60b4e84"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ac2ef07f6aa25f31c1e955f66b60b4e84"></a>
virtual LayerParameter_LayerType&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1EuclideanLossLayer.html#ac2ef07f6aa25f31c1e955f66b60b4e84">type</a> () const </td></tr>
<tr class="memdesc:ac2ef07f6aa25f31c1e955f66b60b4e84"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the layer type as an enum value. <br /></td></tr>
<tr class="separator:ac2ef07f6aa25f31c1e955f66b60b4e84"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3c954fd7c15596fd2f59e0f79601905c"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1EuclideanLossLayer.html#a3c954fd7c15596fd2f59e0f79601905c">AllowForceBackward</a> (const int bottom_index) const </td></tr>
<tr class="separator:a3c954fd7c15596fd2f59e0f79601905c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classcaffe_1_1LossLayer"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classcaffe_1_1LossLayer')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classcaffe_1_1LossLayer.html">caffe::LossLayer&lt; Dtype &gt;</a></td></tr>
<tr class="memitem:a16e133050e2d97c6f024ea74e3ba4ead inherit pub_methods_classcaffe_1_1LossLayer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a16e133050e2d97c6f024ea74e3ba4ead"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>LossLayer</b> (const LayerParameter &amp;param)</td></tr>
<tr class="separator:a16e133050e2d97c6f024ea74e3ba4ead inherit pub_methods_classcaffe_1_1LossLayer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8a2e16d4691640c34e589aac4ec42e28 inherit pub_methods_classcaffe_1_1LossLayer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1LossLayer.html#a8a2e16d4691640c34e589aac4ec42e28">ExactNumBottomBlobs</a> () const </td></tr>
<tr class="memdesc:a8a2e16d4691640c34e589aac4ec42e28 inherit pub_methods_classcaffe_1_1LossLayer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the exact number of bottom blobs required by the layer, or -1 if no exact number is required.  <a href="#a8a2e16d4691640c34e589aac4ec42e28">More...</a><br /></td></tr>
<tr class="separator:a8a2e16d4691640c34e589aac4ec42e28 inherit pub_methods_classcaffe_1_1LossLayer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad272e6792a781ce4f66a65057cc829d1 inherit pub_methods_classcaffe_1_1LossLayer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ad272e6792a781ce4f66a65057cc829d1"></a>
virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1LossLayer.html#ad272e6792a781ce4f66a65057cc829d1">AutoTopBlobs</a> () const </td></tr>
<tr class="memdesc:ad272e6792a781ce4f66a65057cc829d1 inherit pub_methods_classcaffe_1_1LossLayer"><td class="mdescLeft">&#160;</td><td class="mdescRight">For convenience and backwards compatibility, instruct the <a class="el" href="classcaffe_1_1Net.html" title="Connects Layers together into a directed acyclic graph (DAG) specified by a NetParameter. ">Net</a> to automatically allocate a single top <a class="el" href="classcaffe_1_1Blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> for LossLayers, into which they output their singleton loss, (even if the user didn't specify one in the prototxt, etc.). <br /></td></tr>
<tr class="separator:ad272e6792a781ce4f66a65057cc829d1 inherit pub_methods_classcaffe_1_1LossLayer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af8dca16967e8e979ebead4e80664dc10 inherit pub_methods_classcaffe_1_1LossLayer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1LossLayer.html#af8dca16967e8e979ebead4e80664dc10">ExactNumTopBlobs</a> () const </td></tr>
<tr class="memdesc:af8dca16967e8e979ebead4e80664dc10 inherit pub_methods_classcaffe_1_1LossLayer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the exact number of top blobs required by the layer, or -1 if no exact number is required.  <a href="#af8dca16967e8e979ebead4e80664dc10">More...</a><br /></td></tr>
<tr class="separator:af8dca16967e8e979ebead4e80664dc10 inherit pub_methods_classcaffe_1_1LossLayer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classcaffe_1_1Layer"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classcaffe_1_1Layer')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classcaffe_1_1Layer.html">caffe::Layer&lt; Dtype &gt;</a></td></tr>
<tr class="memitem:a7b4e4ccea08c7b8b15acc6829d5735f6 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a7b4e4ccea08c7b8b15acc6829d5735f6">Layer</a> (const LayerParameter &amp;param)</td></tr>
<tr class="separator:a7b4e4ccea08c7b8b15acc6829d5735f6 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4f7809d8e708c4408a96af0752aec481 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a4f7809d8e708c4408a96af0752aec481">SetUp</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;bottom, vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; *top)</td></tr>
<tr class="memdesc:a4f7809d8e708c4408a96af0752aec481 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implements common layer setup functionality.  <a href="#a4f7809d8e708c4408a96af0752aec481">More...</a><br /></td></tr>
<tr class="separator:a4f7809d8e708c4408a96af0752aec481 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a31d00947960a1334be6e45c9a43d8d58 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">Dtype&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a31d00947960a1334be6e45c9a43d8d58">Forward</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;bottom, vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; *top)</td></tr>
<tr class="memdesc:a31d00947960a1334be6e45c9a43d8d58 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Given the bottom blobs, compute the top blobs and the loss.  <a href="#a31d00947960a1334be6e45c9a43d8d58">More...</a><br /></td></tr>
<tr class="separator:a31d00947960a1334be6e45c9a43d8d58 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a38823aa5348f83afd589ec3ac954657e inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a38823aa5348f83afd589ec3ac954657e">Backward</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;top, const vector&lt; bool &gt; &amp;propagate_down, vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; *bottom)</td></tr>
<tr class="memdesc:a38823aa5348f83afd589ec3ac954657e inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Given the top blob error gradients, compute the bottom blob error gradients.  <a href="#a38823aa5348f83afd589ec3ac954657e">More...</a><br /></td></tr>
<tr class="separator:a38823aa5348f83afd589ec3ac954657e inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaf4524ce8641a30a8a4784aee1b2b4c8 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="aaf4524ce8641a30a8a4784aee1b2b4c8"></a>
vector&lt; shared_ptr&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a><br class="typebreak" />
&lt; Dtype &gt; &gt; &gt; &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#aaf4524ce8641a30a8a4784aee1b2b4c8">blobs</a> ()</td></tr>
<tr class="memdesc:aaf4524ce8641a30a8a4784aee1b2b4c8 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the vector of learnable parameter blobs. <br /></td></tr>
<tr class="separator:aaf4524ce8641a30a8a4784aee1b2b4c8 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af475062fe280614b18f642c4ccf50b40 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="af475062fe280614b18f642c4ccf50b40"></a>
const LayerParameter &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#af475062fe280614b18f642c4ccf50b40">layer_param</a> () const </td></tr>
<tr class="memdesc:af475062fe280614b18f642c4ccf50b40 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the layer parameter. <br /></td></tr>
<tr class="separator:af475062fe280614b18f642c4ccf50b40 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4a1754828dda22cc8daa2f63377f3579 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a4a1754828dda22cc8daa2f63377f3579"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a4a1754828dda22cc8daa2f63377f3579">ToProto</a> (LayerParameter *param, bool write_diff=false)</td></tr>
<tr class="memdesc:a4a1754828dda22cc8daa2f63377f3579 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Writes the layer parameter to a protocol buffer. <br /></td></tr>
<tr class="separator:a4a1754828dda22cc8daa2f63377f3579 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a964ccba33b9a4b69391a72508f764eaf inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a964ccba33b9a4b69391a72508f764eaf"></a>
Dtype&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a964ccba33b9a4b69391a72508f764eaf">loss</a> (const int top_index) const </td></tr>
<tr class="memdesc:a964ccba33b9a4b69391a72508f764eaf inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the scalar loss associated with a top blob at a given index. <br /></td></tr>
<tr class="separator:a964ccba33b9a4b69391a72508f764eaf inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a899b09f4b91ada8545b3a43ee91e0d69 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a899b09f4b91ada8545b3a43ee91e0d69"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a899b09f4b91ada8545b3a43ee91e0d69">set_loss</a> (const int top_index, const Dtype value)</td></tr>
<tr class="memdesc:a899b09f4b91ada8545b3a43ee91e0d69 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the loss associated with a top blob at a given index. <br /></td></tr>
<tr class="separator:a899b09f4b91ada8545b3a43ee91e0d69 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a29dba205196d9aeaa1cfeba4dc891093 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a29dba205196d9aeaa1cfeba4dc891093"></a>
virtual const string &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a29dba205196d9aeaa1cfeba4dc891093">type_name</a> () const </td></tr>
<tr class="memdesc:a29dba205196d9aeaa1cfeba4dc891093 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the layer type name. <br /></td></tr>
<tr class="separator:a29dba205196d9aeaa1cfeba4dc891093 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ade3eee97cc743c4e68fff7eba6484916 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#ade3eee97cc743c4e68fff7eba6484916">MinBottomBlobs</a> () const </td></tr>
<tr class="memdesc:ade3eee97cc743c4e68fff7eba6484916 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the minimum number of bottom blobs required by the layer, or -1 if no minimum number is required.  <a href="#ade3eee97cc743c4e68fff7eba6484916">More...</a><br /></td></tr>
<tr class="separator:ade3eee97cc743c4e68fff7eba6484916 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6408ef3939f1abed1abcec46ff219289 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a6408ef3939f1abed1abcec46ff219289">MaxBottomBlobs</a> () const </td></tr>
<tr class="memdesc:a6408ef3939f1abed1abcec46ff219289 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the maximum number of bottom blobs required by the layer, or -1 if no maximum number is required.  <a href="#a6408ef3939f1abed1abcec46ff219289">More...</a><br /></td></tr>
<tr class="separator:a6408ef3939f1abed1abcec46ff219289 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8bb143d58a740345fa2dc3d4204d553b inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a8bb143d58a740345fa2dc3d4204d553b">MinTopBlobs</a> () const </td></tr>
<tr class="memdesc:a8bb143d58a740345fa2dc3d4204d553b inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the minimum number of top blobs required by the layer, or -1 if no minimum number is required.  <a href="#a8bb143d58a740345fa2dc3d4204d553b">More...</a><br /></td></tr>
<tr class="separator:a8bb143d58a740345fa2dc3d4204d553b inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adeff774663c6ec94424901d2746e2f03 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#adeff774663c6ec94424901d2746e2f03">MaxTopBlobs</a> () const </td></tr>
<tr class="memdesc:adeff774663c6ec94424901d2746e2f03 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the maximum number of top blobs required by the layer, or -1 if no maximum number is required.  <a href="#adeff774663c6ec94424901d2746e2f03">More...</a><br /></td></tr>
<tr class="separator:adeff774663c6ec94424901d2746e2f03 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad412187a0483c310bd59fd5f957faf0d inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#ad412187a0483c310bd59fd5f957faf0d">EqualNumBottomTopBlobs</a> () const </td></tr>
<tr class="memdesc:ad412187a0483c310bd59fd5f957faf0d inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns true if the layer requires an equal number of bottom and top blobs.  <a href="#ad412187a0483c310bd59fd5f957faf0d">More...</a><br /></td></tr>
<tr class="separator:ad412187a0483c310bd59fd5f957faf0d inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1a3708013b0231e71d725252e10ce6e3 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a1a3708013b0231e71d725252e10ce6e3">param_propagate_down</a> (const int param_id)</td></tr>
<tr class="memdesc:a1a3708013b0231e71d725252e10ce6e3 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specifies whether the layer should compute gradients w.r.t. a parameter at a particular index given by param_id.  <a href="#a1a3708013b0231e71d725252e10ce6e3">More...</a><br /></td></tr>
<tr class="separator:a1a3708013b0231e71d725252e10ce6e3 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9a6fcb843803ed556f0a69cc2864379b inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a9a6fcb843803ed556f0a69cc2864379b"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a9a6fcb843803ed556f0a69cc2864379b">set_param_propagate_down</a> (const int param_id, const bool value)</td></tr>
<tr class="memdesc:a9a6fcb843803ed556f0a69cc2864379b inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets whether the layer should compute gradients w.r.t. a parameter at a particular index given by param_id. <br /></td></tr>
<tr class="separator:a9a6fcb843803ed556f0a69cc2864379b inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pro-methods"></a>
Protected Member Functions</h2></td></tr>
<tr class="memitem:a864b0ebb8cc013347d7fa7ca69822e64"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1EuclideanLossLayer.html#a864b0ebb8cc013347d7fa7ca69822e64">Forward_cpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;bottom, vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; *top)</td></tr>
<tr class="memdesc:a864b0ebb8cc013347d7fa7ca69822e64"><td class="mdescLeft">&#160;</td><td class="mdescRight">Computes the Euclidean (L2) loss <img class="formulaInl" alt="$ E = \frac{1}{2N} \sum\limits_{n=1}^N \left| \left| \hat{y}_n - y_n \right| \right|_2^2 $" src="form_25.png"/> for real-valued regression tasks.  <a href="#a864b0ebb8cc013347d7fa7ca69822e64">More...</a><br /></td></tr>
<tr class="separator:a864b0ebb8cc013347d7fa7ca69822e64"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af6dcbdf17106aa4f574823424811179d"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="af6dcbdf17106aa4f574823424811179d"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1EuclideanLossLayer.html#af6dcbdf17106aa4f574823424811179d">Forward_gpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;bottom, vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; *top)</td></tr>
<tr class="memdesc:af6dcbdf17106aa4f574823424811179d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Using the GPU device, compute the layer output. Fall back to <a class="el" href="classcaffe_1_1EuclideanLossLayer.html#a864b0ebb8cc013347d7fa7ca69822e64" title="Computes the Euclidean (L2) loss  for real-valued regression tasks. ">Forward_cpu()</a> if unavailable. <br /></td></tr>
<tr class="separator:af6dcbdf17106aa4f574823424811179d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aad0ba82244d43962ce5f0727021c3942"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1EuclideanLossLayer.html#aad0ba82244d43962ce5f0727021c3942">Backward_cpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;top, const vector&lt; bool &gt; &amp;propagate_down, vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; *bottom)</td></tr>
<tr class="memdesc:aad0ba82244d43962ce5f0727021c3942"><td class="mdescLeft">&#160;</td><td class="mdescRight">Computes the Euclidean error gradient w.r.t. the inputs.  <a href="#aad0ba82244d43962ce5f0727021c3942">More...</a><br /></td></tr>
<tr class="separator:aad0ba82244d43962ce5f0727021c3942"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a09e462e9f748a8a68eab86f46d8ec962"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a09e462e9f748a8a68eab86f46d8ec962"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1EuclideanLossLayer.html#a09e462e9f748a8a68eab86f46d8ec962">Backward_gpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;top, const vector&lt; bool &gt; &amp;propagate_down, vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; *bottom)</td></tr>
<tr class="memdesc:a09e462e9f748a8a68eab86f46d8ec962"><td class="mdescLeft">&#160;</td><td class="mdescRight">Using the GPU device, compute the gradients for any parameters and for the bottom blobs if propagate_down is true. Fall back to <a class="el" href="classcaffe_1_1EuclideanLossLayer.html#aad0ba82244d43962ce5f0727021c3942" title="Computes the Euclidean error gradient w.r.t. the inputs. ">Backward_cpu()</a> if unavailable. <br /></td></tr>
<tr class="separator:a09e462e9f748a8a68eab86f46d8ec962"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pro_methods_classcaffe_1_1Layer"><td colspan="2" onclick="javascript:toggleInherit('pro_methods_classcaffe_1_1Layer')"><img src="closed.png" alt="-"/>&#160;Protected Member Functions inherited from <a class="el" href="classcaffe_1_1Layer.html">caffe::Layer&lt; Dtype &gt;</a></td></tr>
<tr class="memitem:adaa95e30dff155409a25ffcb5c8c885e inherit pro_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#adaa95e30dff155409a25ffcb5c8c885e">CheckBlobCounts</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;top)</td></tr>
<tr class="separator:adaa95e30dff155409a25ffcb5c8c885e inherit pro_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5bebaf079cff5bff7016be1733bb996e inherit pro_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a5bebaf079cff5bff7016be1733bb996e">SetLossWeights</a> (vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; *top)</td></tr>
<tr class="separator:a5bebaf079cff5bff7016be1733bb996e inherit pro_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a90730e177e4b3b3516b1b69ba2f6b06a inherit pro_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a90730e177e4b3b3516b1b69ba2f6b06a"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>DISABLE_COPY_AND_ASSIGN</b> (<a class="el" href="classcaffe_1_1Layer.html">Layer</a>)</td></tr>
<tr class="separator:a90730e177e4b3b3516b1b69ba2f6b06a inherit pro_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pro-attribs"></a>
Protected Attributes</h2></td></tr>
<tr class="memitem:a47ec68365879c820f9e18e456f93376a"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a47ec68365879c820f9e18e456f93376a"></a>
<a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt;&#160;</td><td class="memItemRight" valign="bottom"><b>diff_</b></td></tr>
<tr class="separator:a47ec68365879c820f9e18e456f93376a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pro_attribs_classcaffe_1_1Layer"><td colspan="2" onclick="javascript:toggleInherit('pro_attribs_classcaffe_1_1Layer')"><img src="closed.png" alt="-"/>&#160;Protected Attributes inherited from <a class="el" href="classcaffe_1_1Layer.html">caffe::Layer&lt; Dtype &gt;</a></td></tr>
<tr class="memitem:a7ed12bb2df25c887e41d7ea9557fc701 inherit pro_attribs_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">LayerParameter&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a7ed12bb2df25c887e41d7ea9557fc701">layer_param_</a></td></tr>
<tr class="separator:a7ed12bb2df25c887e41d7ea9557fc701 inherit pro_attribs_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8073fcf2c139b47eb99ce71b346b1321 inherit pro_attribs_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">vector&lt; shared_ptr&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a><br class="typebreak" />
&lt; Dtype &gt; &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a8073fcf2c139b47eb99ce71b346b1321">blobs_</a></td></tr>
<tr class="separator:a8073fcf2c139b47eb99ce71b346b1321 inherit pro_attribs_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acd4a05def9ff3b42ad72404210613ef7 inherit pro_attribs_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">vector&lt; bool &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#acd4a05def9ff3b42ad72404210613ef7">param_propagate_down_</a></td></tr>
<tr class="separator:acd4a05def9ff3b42ad72404210613ef7 inherit pro_attribs_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af6d347229a139500994e7a926c680486 inherit pro_attribs_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">vector&lt; Dtype &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#af6d347229a139500994e7a926c680486">loss_</a></td></tr>
<tr class="separator:af6d347229a139500994e7a926c680486 inherit pro_attribs_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><h3>template&lt;typename Dtype&gt;<br />
class caffe::EuclideanLossLayer&lt; Dtype &gt;</h3>

<p>Computes the Euclidean (L2) loss <img class="formulaInl" alt="$ E = \frac{1}{2N} \sum\limits_{n=1}^N \left| \left| \hat{y}_n - y_n \right| \right|_2^2 $" src="form_25.png"/> for real-valued regression tasks. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">bottom</td><td>input <a class="el" href="classcaffe_1_1Blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 2)<ol type="1">
<li><img class="formulaInl" alt="$ (N \times C \times H \times W) $" src="form_2.png"/> the predictions <img class="formulaInl" alt="$ \hat{y} \in [-\infty, +\infty]$" src="form_26.png"/></li>
<li><img class="formulaInl" alt="$ (N \times C \times H \times W) $" src="form_2.png"/> the targets <img class="formulaInl" alt="$ y \in [-\infty, +\infty]$" src="form_27.png"/> </li>
</ol>
</td></tr>
    <tr><td class="paramname">top</td><td>output <a class="el" href="classcaffe_1_1Blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 1)<ol type="1">
<li><img class="formulaInl" alt="$ (1 \times 1 \times 1 \times 1) $" src="form_28.png"/> the computed Euclidean loss: <img class="formulaInl" alt="$ E = \frac{1}{2n} \sum\limits_{n=1}^N \left| \left| \hat{y}_n - y_n \right| \right|_2^2 $" src="form_29.png"/></li>
</ol>
</td></tr>
  </table>
  </dd>
</dl>
<p>This can be used for least-squares regression tasks. An <a class="el" href="classcaffe_1_1InnerProductLayer.html" title="Also known as a &quot;fully-connected&quot; layer, computes an inner product with a set of learned weights...">InnerProductLayer</a> input to a <a class="el" href="classcaffe_1_1EuclideanLossLayer.html" title="Computes the Euclidean (L2) loss  for real-valued regression tasks. ">EuclideanLossLayer</a> exactly formulates a linear least squares regression problem. With non-zero weight decay the problem becomes one of ridge regression &ndash; see src/caffe/test/test_sgd_solver.cpp for a concrete example wherein we check that the gradients computed for a <a class="el" href="classcaffe_1_1Net.html" title="Connects Layers together into a directed acyclic graph (DAG) specified by a NetParameter. ">Net</a> with exactly this structure match hand-computed gradient formulas for ridge regression.</p>
<p>(Note: <a class="el" href="classcaffe_1_1Caffe.html">Caffe</a>, and SGD in general, is certainly <b>not</b> the best way to solve linear least squares problems! We use it only as an instructive example.) </p>
</div><h2 class="groupheader">Member Function Documentation</h2>
<a class="anchor" id="a3c954fd7c15596fd2f59e0f79601905c"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual bool <a class="el" href="classcaffe_1_1EuclideanLossLayer.html">caffe::EuclideanLossLayer</a>&lt; Dtype &gt;::AllowForceBackward </td>
          <td>(</td>
          <td class="paramtype">const int&#160;</td>
          <td class="paramname"><em>bottom_index</em></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p>Unlike most loss layers, in the <a class="el" href="classcaffe_1_1EuclideanLossLayer.html" title="Computes the Euclidean (L2) loss  for real-valued regression tasks. ">EuclideanLossLayer</a> we can backpropagate to both inputs &ndash; override to return true and always allow force_backward. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1LossLayer.html#ad02fe695b06451ac8e6f21db0cba1dad">caffe::LossLayer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a class="anchor" id="aad0ba82244d43962ce5f0727021c3942"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classcaffe_1_1EuclideanLossLayer.html">caffe::EuclideanLossLayer</a>&lt; Dtype &gt;::Backward_cpu </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;&#160;</td>
          <td class="paramname"><em>top</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; bool &gt; &amp;&#160;</td>
          <td class="paramname"><em>propagate_down</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; *&#160;</td>
          <td class="paramname"><em>bottom</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Computes the Euclidean error gradient w.r.t. the inputs. </p>
<p>Unlike other children of <a class="el" href="classcaffe_1_1LossLayer.html" title="An interface for Layers that take two Blobs as input – usually (1) predictions and (2) ground-truth ...">LossLayer</a>, <a class="el" href="classcaffe_1_1EuclideanLossLayer.html" title="Computes the Euclidean (L2) loss  for real-valued regression tasks. ">EuclideanLossLayer</a> <b>can</b> compute gradients with respect to the label inputs bottom[1] (but still only will if propagate_down[1] is set, due to being produced by learnable parameters or if force_backward is set). In fact, this layer is "commutative" &ndash; the result is the same regardless of the order of the two bottoms.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">top</td><td>output <a class="el" href="classcaffe_1_1Blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 1), providing the error gradient with respect to the outputs<ol type="1">
<li><img class="formulaInl" alt="$ (1 \times 1 \times 1 \times 1) $" src="form_28.png"/> This <a class="el" href="classcaffe_1_1Blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a>'s diff will simply contain the loss_weight* <img class="formulaInl" alt="$ \lambda $" src="form_70.png"/>, as <img class="formulaInl" alt="$ \lambda $" src="form_70.png"/> is the coefficient of this layer's output <img class="formulaInl" alt="$\ell_i$" src="form_71.png"/> in the overall <a class="el" href="classcaffe_1_1Net.html" title="Connects Layers together into a directed acyclic graph (DAG) specified by a NetParameter. ">Net</a> loss <img class="formulaInl" alt="$ E = \lambda_i \ell_i + \mbox{other loss terms}$" src="form_72.png"/>; hence <img class="formulaInl" alt="$ \frac{\partial E}{\partial \ell_i} = \lambda_i $" src="form_73.png"/>. (*Assuming that this top <a class="el" href="classcaffe_1_1Blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> is not used as a bottom (input) by any other layer of the <a class="el" href="classcaffe_1_1Net.html" title="Connects Layers together into a directed acyclic graph (DAG) specified by a NetParameter. ">Net</a>.) </li>
</ol>
</td></tr>
    <tr><td class="paramname">propagate_down</td><td>see <a class="el" href="classcaffe_1_1Layer.html#a38823aa5348f83afd589ec3ac954657e" title="Given the top blob error gradients, compute the bottom blob error gradients. ">Layer::Backward</a>. </td></tr>
    <tr><td class="paramname">bottom</td><td>input <a class="el" href="classcaffe_1_1Blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 2)<ol type="1">
<li><img class="formulaInl" alt="$ (N \times C \times H \times W) $" src="form_2.png"/> the predictions <img class="formulaInl" alt="$\hat{y}$" src="form_74.png"/>; Backward fills their diff with gradients <img class="formulaInl" alt="$ \frac{\partial E}{\partial \hat{y}} = \frac{1}{n} \sum\limits_{n=1}^N (\hat{y}_n - y_n) $" src="form_75.png"/> if propagate_down[0]</li>
<li><img class="formulaInl" alt="$ (N \times C \times H \times W) $" src="form_2.png"/> the targets <img class="formulaInl" alt="$y$" src="form_76.png"/>; Backward fills their diff with gradients <img class="formulaInl" alt="$ \frac{\partial E}{\partial y} = \frac{1}{n} \sum\limits_{n=1}^N (y_n - \hat{y}_n) $" src="form_77.png"/> if propagate_down[1] </li>
</ol>
</td></tr>
  </table>
  </dd>
</dl>

<p>Implements <a class="el" href="classcaffe_1_1Layer.html#a223d932dacb7ff4c010e982f57e775b6">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a class="anchor" id="a864b0ebb8cc013347d7fa7ca69822e64"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classcaffe_1_1EuclideanLossLayer.html">caffe::EuclideanLossLayer</a>&lt; Dtype &gt;::Forward_cpu </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; *&#160;</td>
          <td class="paramname"><em>top</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Computes the Euclidean (L2) loss <img class="formulaInl" alt="$ E = \frac{1}{2N} \sum\limits_{n=1}^N \left| \left| \hat{y}_n - y_n \right| \right|_2^2 $" src="form_25.png"/> for real-valued regression tasks. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">bottom</td><td>input <a class="el" href="classcaffe_1_1Blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 2)<ol type="1">
<li><img class="formulaInl" alt="$ (N \times C \times H \times W) $" src="form_2.png"/> the predictions <img class="formulaInl" alt="$ \hat{y} \in [-\infty, +\infty]$" src="form_26.png"/></li>
<li><img class="formulaInl" alt="$ (N \times C \times H \times W) $" src="form_2.png"/> the targets <img class="formulaInl" alt="$ y \in [-\infty, +\infty]$" src="form_27.png"/> </li>
</ol>
</td></tr>
    <tr><td class="paramname">top</td><td>output <a class="el" href="classcaffe_1_1Blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 1)<ol type="1">
<li><img class="formulaInl" alt="$ (1 \times 1 \times 1 \times 1) $" src="form_28.png"/> the computed Euclidean loss: <img class="formulaInl" alt="$ E = \frac{1}{2n} \sum\limits_{n=1}^N \left| \left| \hat{y}_n - y_n \right| \right|_2^2 $" src="form_29.png"/></li>
</ol>
</td></tr>
  </table>
  </dd>
</dl>
<p>This can be used for least-squares regression tasks. An <a class="el" href="classcaffe_1_1InnerProductLayer.html" title="Also known as a &quot;fully-connected&quot; layer, computes an inner product with a set of learned weights...">InnerProductLayer</a> input to a <a class="el" href="classcaffe_1_1EuclideanLossLayer.html" title="Computes the Euclidean (L2) loss  for real-valued regression tasks. ">EuclideanLossLayer</a> exactly formulates a linear least squares regression problem. With non-zero weight decay the problem becomes one of ridge regression &ndash; see src/caffe/test/test_sgd_solver.cpp for a concrete example wherein we check that the gradients computed for a <a class="el" href="classcaffe_1_1Net.html" title="Connects Layers together into a directed acyclic graph (DAG) specified by a NetParameter. ">Net</a> with exactly this structure match hand-computed gradient formulas for ridge regression.</p>
<p>(Note: <a class="el" href="classcaffe_1_1Caffe.html">Caffe</a>, and SGD in general, is certainly <b>not</b> the best way to solve linear least squares problems! We use it only as an instructive example.) </p>

<p>Implements <a class="el" href="classcaffe_1_1Layer.html#a7157e270d38711581246bea58ac77a4f">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a class="anchor" id="a58a4db1e251eeb1e45d2f957a38038e5"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classcaffe_1_1EuclideanLossLayer.html">caffe::EuclideanLossLayer</a>&lt; Dtype &gt;::LayerSetUp </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; *&#160;</td>
          <td class="paramname"><em>top</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Does layer-specific setup: your layer should implement this. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">bottom</td><td>the preshaped input blobs, whose data fields store the input data for this layer </td></tr>
    <tr><td class="paramname">top</td><td>the allocated but unshaped output blobs, to be initialized by LayerSetUp</td></tr>
  </table>
  </dd>
</dl>
<p>This method should be used to do layer-specific setup. At a minimum, this includes reshaping the empty top blobs to the shape as dictated by the shapes of the bottom blobs and any relevant parameters from the <code>layer_param_</code>. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1LossLayer.html#a00f614a20793dcd2a70d93ac0c0a053a">caffe::LossLayer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>include/caffe/<a class="el" href="loss__layers_8hpp_source.html">loss_layers.hpp</a></li>
<li>src/caffe/layers/euclidean_loss_layer.cpp</li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Mon Sep 8 2014 12:11:01 for Caffe by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.8
</small></address>
</body>
</html>
