<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>Caffe: caffe::RecurrentLayer&lt; Dtype &gt; Class Template Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Caffe
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Class&#160;List</span></a></li>
      <li><a href="classes.html"><span>Class&#160;Index</span></a></li>
      <li><a href="hierarchy.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&#160;Members</span></a></li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacecaffe.html">caffe</a></li><li class="navelem"><a class="el" href="classcaffe_1_1RecurrentLayer.html">RecurrentLayer</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pro-methods">Protected Member Functions</a> &#124;
<a href="#pro-attribs">Protected Attributes</a> &#124;
<a href="classcaffe_1_1RecurrentLayer-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">caffe::RecurrentLayer&lt; Dtype &gt; Class Template Reference<span class="mlabels"><span class="mlabel">abstract</span></span></div>  </div>
</div><!--header-->
<div class="contents">

<p>An abstract class for implementing recurrent behavior inside of an unrolled network. This <a class="el" href="classcaffe_1_1Layer.html" title="An interface for the units of computation which can be composed into a Net. ">Layer</a> type cannot be instantiated &ndash; instead, you should use one of its implementations which defines the recurrent architecture, such as <a class="el" href="classcaffe_1_1RNNLayer.html" title="Processes time-varying inputs using a simple recurrent neural network (RNN). Implemented as a network...">RNNLayer</a> or <a class="el" href="classcaffe_1_1LSTMLayer.html" title="Processes sequential inputs using a &quot;Long Short-Term Memory&quot; (LSTM) [1] style recurrent neural networ...">LSTMLayer</a>.  
 <a href="classcaffe_1_1RecurrentLayer.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="recurrent__layer_8hpp_source.html">recurrent_layer.hpp</a>&gt;</code></p>
<div class="dynheader">
Inheritance diagram for caffe::RecurrentLayer&lt; Dtype &gt;:</div>
<div class="dyncontent">
 <div class="center">
  <img src="classcaffe_1_1RecurrentLayer.png" usemap="#caffe::RecurrentLayer_3C_20Dtype_20_3E_map" alt=""/>
  <map id="caffe::RecurrentLayer_3C_20Dtype_20_3E_map" name="caffe::RecurrentLayer&lt; Dtype &gt;_map">
<area href="classcaffe_1_1Layer.html" title="An interface for the units of computation which can be composed into a Net. " alt="caffe::Layer&lt; Dtype &gt;" shape="rect" coords="101,0,294,24"/>
<area href="classcaffe_1_1LSTMLayer.html" title="Processes sequential inputs using a &quot;Long Short-Term Memory&quot; (LSTM) [1] style recurrent neural networ..." alt="caffe::LSTMLayer&lt; Dtype &gt;" shape="rect" coords="0,112,193,136"/>
<area href="classcaffe_1_1RNNLayer.html" title="Processes time-varying inputs using a simple recurrent neural network (RNN). Implemented as a network..." alt="caffe::RNNLayer&lt; Dtype &gt;" shape="rect" coords="203,112,396,136"/>
</map>
 </div></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a3f02919dbb32c07c89bfda4ea68c09df"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a3f02919dbb32c07c89bfda4ea68c09df"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>RecurrentLayer</b> (const LayerParameter &amp;param)</td></tr>
<tr class="separator:a3f02919dbb32c07c89bfda4ea68c09df"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aafa788d0a3535c80478fee74b9dbb62b"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#aafa788d0a3535c80478fee74b9dbb62b">LayerSetUp</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;top)</td></tr>
<tr class="memdesc:aafa788d0a3535c80478fee74b9dbb62b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Does layer-specific setup: your layer should implement this function as well as Reshape.  <a href="#aafa788d0a3535c80478fee74b9dbb62b">More...</a><br /></td></tr>
<tr class="separator:aafa788d0a3535c80478fee74b9dbb62b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a07a6d838be5330334335256811d2b6f6"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#a07a6d838be5330334335256811d2b6f6">Reshape</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;top)</td></tr>
<tr class="memdesc:a07a6d838be5330334335256811d2b6f6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adjust the shapes of top blobs and internal buffers to accommodate the shapes of the bottom blobs.  <a href="#a07a6d838be5330334335256811d2b6f6">More...</a><br /></td></tr>
<tr class="separator:a07a6d838be5330334335256811d2b6f6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9f0bf24a571da40f490b9b78a51d9393"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a9f0bf24a571da40f490b9b78a51d9393"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><b>Reset</b> ()</td></tr>
<tr class="separator:a9f0bf24a571da40f490b9b78a51d9393"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab3bc55d48b2acc9d6cf3ec03008ef513"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ab3bc55d48b2acc9d6cf3ec03008ef513"></a>
virtual const char *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#ab3bc55d48b2acc9d6cf3ec03008ef513">type</a> () const </td></tr>
<tr class="memdesc:ab3bc55d48b2acc9d6cf3ec03008ef513"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the layer type. <br /></td></tr>
<tr class="separator:ab3bc55d48b2acc9d6cf3ec03008ef513"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1deb403821dc383f13e8bf2a1eeafdf9"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#a1deb403821dc383f13e8bf2a1eeafdf9">MinBottomBlobs</a> () const </td></tr>
<tr class="memdesc:a1deb403821dc383f13e8bf2a1eeafdf9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the minimum number of bottom blobs required by the layer, or -1 if no minimum number is required.  <a href="#a1deb403821dc383f13e8bf2a1eeafdf9">More...</a><br /></td></tr>
<tr class="separator:a1deb403821dc383f13e8bf2a1eeafdf9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a10254a9cb58d36d029b2da9337ad16e9"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#a10254a9cb58d36d029b2da9337ad16e9">MaxBottomBlobs</a> () const </td></tr>
<tr class="memdesc:a10254a9cb58d36d029b2da9337ad16e9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the maximum number of bottom blobs required by the layer, or -1 if no maximum number is required.  <a href="#a10254a9cb58d36d029b2da9337ad16e9">More...</a><br /></td></tr>
<tr class="separator:a10254a9cb58d36d029b2da9337ad16e9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac53fa5447232a9067ef30028ad9da1cc"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#ac53fa5447232a9067ef30028ad9da1cc">ExactNumTopBlobs</a> () const </td></tr>
<tr class="memdesc:ac53fa5447232a9067ef30028ad9da1cc"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the exact number of top blobs required by the layer, or -1 if no exact number is required.  <a href="#ac53fa5447232a9067ef30028ad9da1cc">More...</a><br /></td></tr>
<tr class="separator:ac53fa5447232a9067ef30028ad9da1cc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac8642c8d7f418b6513c93daffa5eb15e"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#ac8642c8d7f418b6513c93daffa5eb15e">AllowForceBackward</a> (const int bottom_index) const </td></tr>
<tr class="memdesc:ac8642c8d7f418b6513c93daffa5eb15e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return whether to allow force_backward for a given bottom blob index.  <a href="#ac8642c8d7f418b6513c93daffa5eb15e">More...</a><br /></td></tr>
<tr class="separator:ac8642c8d7f418b6513c93daffa5eb15e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classcaffe_1_1Layer"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classcaffe_1_1Layer')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classcaffe_1_1Layer.html">caffe::Layer&lt; Dtype &gt;</a></td></tr>
<tr class="memitem:a7b4e4ccea08c7b8b15acc6829d5735f6 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a7b4e4ccea08c7b8b15acc6829d5735f6">Layer</a> (const LayerParameter &amp;param)</td></tr>
<tr class="separator:a7b4e4ccea08c7b8b15acc6829d5735f6 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac427a267f4c5ba93caac53b7ba64841d inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#ac427a267f4c5ba93caac53b7ba64841d">SetUp</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;top)</td></tr>
<tr class="memdesc:ac427a267f4c5ba93caac53b7ba64841d inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implements common layer setup functionality.  <a href="#ac427a267f4c5ba93caac53b7ba64841d">More...</a><br /></td></tr>
<tr class="separator:ac427a267f4c5ba93caac53b7ba64841d inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6ec5d3be0fc8348f8b67fccb697d4aa9 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a6ec5d3be0fc8348f8b67fccb697d4aa9"></a>
virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a6ec5d3be0fc8348f8b67fccb697d4aa9">ShareInParallel</a> () const </td></tr>
<tr class="memdesc:a6ec5d3be0fc8348f8b67fccb697d4aa9 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Whether a layer should be shared by multiple nets during data parallelism. By default, all layers except for data layers should not be shared. data layers should be shared to ensure each worker solver access data sequentially during data parallelism. <br /></td></tr>
<tr class="separator:a6ec5d3be0fc8348f8b67fccb697d4aa9 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9dd73d843654bb15c9394ca2b43e8970 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a9dd73d843654bb15c9394ca2b43e8970"></a>
bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a9dd73d843654bb15c9394ca2b43e8970">IsShared</a> () const </td></tr>
<tr class="memdesc:a9dd73d843654bb15c9394ca2b43e8970 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return whether this layer is actually shared by other nets. If <a class="el" href="classcaffe_1_1Layer.html#a6ec5d3be0fc8348f8b67fccb697d4aa9" title="Whether a layer should be shared by multiple nets during data parallelism. By default, all layers except for data layers should not be shared. data layers should be shared to ensure each worker solver access data sequentially during data parallelism. ">ShareInParallel()</a> is true and using more than one GPU and the net has TRAIN phase, then this function is expected return true. <br /></td></tr>
<tr class="separator:a9dd73d843654bb15c9394ca2b43e8970 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6d4caf38c9f0f31c9c92af1bbefc04e4 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a6d4caf38c9f0f31c9c92af1bbefc04e4"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a6d4caf38c9f0f31c9c92af1bbefc04e4">SetShared</a> (bool is_shared)</td></tr>
<tr class="memdesc:a6d4caf38c9f0f31c9c92af1bbefc04e4 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set whether this layer is actually shared by other nets If <a class="el" href="classcaffe_1_1Layer.html#a6ec5d3be0fc8348f8b67fccb697d4aa9" title="Whether a layer should be shared by multiple nets during data parallelism. By default, all layers except for data layers should not be shared. data layers should be shared to ensure each worker solver access data sequentially during data parallelism. ">ShareInParallel()</a> is true and using more than one GPU and the net has TRAIN phase, then is_shared should be set true. <br /></td></tr>
<tr class="separator:a6d4caf38c9f0f31c9c92af1bbefc04e4 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa5fc9ddb31b58958653372bdaaccde94 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">Dtype&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#aa5fc9ddb31b58958653372bdaaccde94">Forward</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;top)</td></tr>
<tr class="memdesc:aa5fc9ddb31b58958653372bdaaccde94 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Given the bottom blobs, compute the top blobs and the loss.  <a href="#aa5fc9ddb31b58958653372bdaaccde94">More...</a><br /></td></tr>
<tr class="separator:aa5fc9ddb31b58958653372bdaaccde94 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a53df1e081767e07bfb4c81657f4acd0a inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a53df1e081767e07bfb4c81657f4acd0a">Backward</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;top, const vector&lt; bool &gt; &amp;propagate_down, const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;bottom)</td></tr>
<tr class="memdesc:a53df1e081767e07bfb4c81657f4acd0a inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Given the top blob error gradients, compute the bottom blob error gradients.  <a href="#a53df1e081767e07bfb4c81657f4acd0a">More...</a><br /></td></tr>
<tr class="separator:a53df1e081767e07bfb4c81657f4acd0a inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaf4524ce8641a30a8a4784aee1b2b4c8 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="aaf4524ce8641a30a8a4784aee1b2b4c8"></a>
vector&lt; shared_ptr&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; &gt; &gt; &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#aaf4524ce8641a30a8a4784aee1b2b4c8">blobs</a> ()</td></tr>
<tr class="memdesc:aaf4524ce8641a30a8a4784aee1b2b4c8 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the vector of learnable parameter blobs. <br /></td></tr>
<tr class="separator:aaf4524ce8641a30a8a4784aee1b2b4c8 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af475062fe280614b18f642c4ccf50b40 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="af475062fe280614b18f642c4ccf50b40"></a>
const LayerParameter &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#af475062fe280614b18f642c4ccf50b40">layer_param</a> () const </td></tr>
<tr class="memdesc:af475062fe280614b18f642c4ccf50b40 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the layer parameter. <br /></td></tr>
<tr class="separator:af475062fe280614b18f642c4ccf50b40 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4a1754828dda22cc8daa2f63377f3579 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a4a1754828dda22cc8daa2f63377f3579"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a4a1754828dda22cc8daa2f63377f3579">ToProto</a> (LayerParameter *param, bool write_diff=false)</td></tr>
<tr class="memdesc:a4a1754828dda22cc8daa2f63377f3579 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Writes the layer parameter to a protocol buffer. <br /></td></tr>
<tr class="separator:a4a1754828dda22cc8daa2f63377f3579 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a964ccba33b9a4b69391a72508f764eaf inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a964ccba33b9a4b69391a72508f764eaf"></a>
Dtype&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a964ccba33b9a4b69391a72508f764eaf">loss</a> (const int top_index) const </td></tr>
<tr class="memdesc:a964ccba33b9a4b69391a72508f764eaf inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the scalar loss associated with a top blob at a given index. <br /></td></tr>
<tr class="separator:a964ccba33b9a4b69391a72508f764eaf inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a899b09f4b91ada8545b3a43ee91e0d69 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a899b09f4b91ada8545b3a43ee91e0d69"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a899b09f4b91ada8545b3a43ee91e0d69">set_loss</a> (const int top_index, const Dtype value)</td></tr>
<tr class="memdesc:a899b09f4b91ada8545b3a43ee91e0d69 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the loss associated with a top blob at a given index. <br /></td></tr>
<tr class="separator:a899b09f4b91ada8545b3a43ee91e0d69 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a45c7a7943a8a6735ac433c9be11e0240 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a45c7a7943a8a6735ac433c9be11e0240">ExactNumBottomBlobs</a> () const </td></tr>
<tr class="memdesc:a45c7a7943a8a6735ac433c9be11e0240 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the exact number of bottom blobs required by the layer, or -1 if no exact number is required.  <a href="#a45c7a7943a8a6735ac433c9be11e0240">More...</a><br /></td></tr>
<tr class="separator:a45c7a7943a8a6735ac433c9be11e0240 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8bb143d58a740345fa2dc3d4204d553b inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a8bb143d58a740345fa2dc3d4204d553b">MinTopBlobs</a> () const </td></tr>
<tr class="memdesc:a8bb143d58a740345fa2dc3d4204d553b inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the minimum number of top blobs required by the layer, or -1 if no minimum number is required.  <a href="#a8bb143d58a740345fa2dc3d4204d553b">More...</a><br /></td></tr>
<tr class="separator:a8bb143d58a740345fa2dc3d4204d553b inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adeff774663c6ec94424901d2746e2f03 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#adeff774663c6ec94424901d2746e2f03">MaxTopBlobs</a> () const </td></tr>
<tr class="memdesc:adeff774663c6ec94424901d2746e2f03 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the maximum number of top blobs required by the layer, or -1 if no maximum number is required.  <a href="#adeff774663c6ec94424901d2746e2f03">More...</a><br /></td></tr>
<tr class="separator:adeff774663c6ec94424901d2746e2f03 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad412187a0483c310bd59fd5f957faf0d inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#ad412187a0483c310bd59fd5f957faf0d">EqualNumBottomTopBlobs</a> () const </td></tr>
<tr class="memdesc:ad412187a0483c310bd59fd5f957faf0d inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns true if the layer requires an equal number of bottom and top blobs.  <a href="#ad412187a0483c310bd59fd5f957faf0d">More...</a><br /></td></tr>
<tr class="separator:ad412187a0483c310bd59fd5f957faf0d inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad732ca94cb21b4c4e0d6372a530ededf inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#ad732ca94cb21b4c4e0d6372a530ededf">AutoTopBlobs</a> () const </td></tr>
<tr class="memdesc:ad732ca94cb21b4c4e0d6372a530ededf inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return whether "anonymous" top blobs are created automatically by the layer.  <a href="#ad732ca94cb21b4c4e0d6372a530ededf">More...</a><br /></td></tr>
<tr class="separator:ad732ca94cb21b4c4e0d6372a530ededf inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1a3708013b0231e71d725252e10ce6e3 inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a1a3708013b0231e71d725252e10ce6e3">param_propagate_down</a> (const int param_id)</td></tr>
<tr class="memdesc:a1a3708013b0231e71d725252e10ce6e3 inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specifies whether the layer should compute gradients w.r.t. a parameter at a particular index given by param_id.  <a href="#a1a3708013b0231e71d725252e10ce6e3">More...</a><br /></td></tr>
<tr class="separator:a1a3708013b0231e71d725252e10ce6e3 inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9a6fcb843803ed556f0a69cc2864379b inherit pub_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a9a6fcb843803ed556f0a69cc2864379b"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a9a6fcb843803ed556f0a69cc2864379b">set_param_propagate_down</a> (const int param_id, const bool value)</td></tr>
<tr class="memdesc:a9a6fcb843803ed556f0a69cc2864379b inherit pub_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets whether the layer should compute gradients w.r.t. a parameter at a particular index given by param_id. <br /></td></tr>
<tr class="separator:a9a6fcb843803ed556f0a69cc2864379b inherit pub_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pro-methods"></a>
Protected Member Functions</h2></td></tr>
<tr class="memitem:ae4265b4472d827a5120f743903eb8abf"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ae4265b4472d827a5120f743903eb8abf"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#ae4265b4472d827a5120f743903eb8abf">FillUnrolledNet</a> (NetParameter *net_param) const  =0</td></tr>
<tr class="memdesc:ae4265b4472d827a5120f743903eb8abf"><td class="mdescLeft">&#160;</td><td class="mdescRight">Fills net_param with the recurrent network architecture. Subclasses should define this &ndash; see <a class="el" href="classcaffe_1_1RNNLayer.html" title="Processes time-varying inputs using a simple recurrent neural network (RNN). Implemented as a network...">RNNLayer</a> and <a class="el" href="classcaffe_1_1LSTMLayer.html" title="Processes sequential inputs using a &quot;Long Short-Term Memory&quot; (LSTM) [1] style recurrent neural networ...">LSTMLayer</a> for examples. <br /></td></tr>
<tr class="separator:ae4265b4472d827a5120f743903eb8abf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0d254ce7d99d054c221ad5efc72c29c5"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a0d254ce7d99d054c221ad5efc72c29c5"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#a0d254ce7d99d054c221ad5efc72c29c5">RecurrentInputBlobNames</a> (vector&lt; string &gt; *names) const  =0</td></tr>
<tr class="memdesc:a0d254ce7d99d054c221ad5efc72c29c5"><td class="mdescLeft">&#160;</td><td class="mdescRight">Fills names with the names of the 0th timestep recurrent input <a class="el" href="classcaffe_1_1Blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a>&amp;s. Subclasses should define this &ndash; see <a class="el" href="classcaffe_1_1RNNLayer.html" title="Processes time-varying inputs using a simple recurrent neural network (RNN). Implemented as a network...">RNNLayer</a> and <a class="el" href="classcaffe_1_1LSTMLayer.html" title="Processes sequential inputs using a &quot;Long Short-Term Memory&quot; (LSTM) [1] style recurrent neural networ...">LSTMLayer</a> for examples. <br /></td></tr>
<tr class="separator:a0d254ce7d99d054c221ad5efc72c29c5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a312ffe2d9894e849cd7895bd5ddd34fc"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a312ffe2d9894e849cd7895bd5ddd34fc"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#a312ffe2d9894e849cd7895bd5ddd34fc">RecurrentInputShapes</a> (vector&lt; BlobShape &gt; *shapes) const  =0</td></tr>
<tr class="memdesc:a312ffe2d9894e849cd7895bd5ddd34fc"><td class="mdescLeft">&#160;</td><td class="mdescRight">Fills shapes with the shapes of the recurrent input <a class="el" href="classcaffe_1_1Blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a>&amp;s. Subclasses should define this &ndash; see <a class="el" href="classcaffe_1_1RNNLayer.html" title="Processes time-varying inputs using a simple recurrent neural network (RNN). Implemented as a network...">RNNLayer</a> and <a class="el" href="classcaffe_1_1LSTMLayer.html" title="Processes sequential inputs using a &quot;Long Short-Term Memory&quot; (LSTM) [1] style recurrent neural networ...">LSTMLayer</a> for examples. <br /></td></tr>
<tr class="separator:a312ffe2d9894e849cd7895bd5ddd34fc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a33986e72d9e004b51e52699e97c3360f"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a33986e72d9e004b51e52699e97c3360f"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#a33986e72d9e004b51e52699e97c3360f">RecurrentOutputBlobNames</a> (vector&lt; string &gt; *names) const  =0</td></tr>
<tr class="memdesc:a33986e72d9e004b51e52699e97c3360f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Fills names with the names of the Tth timestep recurrent output <a class="el" href="classcaffe_1_1Blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a>&amp;s. Subclasses should define this &ndash; see <a class="el" href="classcaffe_1_1RNNLayer.html" title="Processes time-varying inputs using a simple recurrent neural network (RNN). Implemented as a network...">RNNLayer</a> and <a class="el" href="classcaffe_1_1LSTMLayer.html" title="Processes sequential inputs using a &quot;Long Short-Term Memory&quot; (LSTM) [1] style recurrent neural networ...">LSTMLayer</a> for examples. <br /></td></tr>
<tr class="separator:a33986e72d9e004b51e52699e97c3360f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1834f25673d313a7d0b678b05f1112a2"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a1834f25673d313a7d0b678b05f1112a2"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#a1834f25673d313a7d0b678b05f1112a2">OutputBlobNames</a> (vector&lt; string &gt; *names) const  =0</td></tr>
<tr class="memdesc:a1834f25673d313a7d0b678b05f1112a2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Fills names with the names of the output blobs, concatenated across all timesteps. Should return a name for each top <a class="el" href="classcaffe_1_1Blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a>. Subclasses should define this &ndash; see <a class="el" href="classcaffe_1_1RNNLayer.html" title="Processes time-varying inputs using a simple recurrent neural network (RNN). Implemented as a network...">RNNLayer</a> and <a class="el" href="classcaffe_1_1LSTMLayer.html" title="Processes sequential inputs using a &quot;Long Short-Term Memory&quot; (LSTM) [1] style recurrent neural networ...">LSTMLayer</a> for examples. <br /></td></tr>
<tr class="separator:a1834f25673d313a7d0b678b05f1112a2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af914a79d0cd024d6962b75325ab5ee94"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#af914a79d0cd024d6962b75325ab5ee94">Forward_cpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;top)</td></tr>
<tr class="separator:af914a79d0cd024d6962b75325ab5ee94"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a824c38cd40b7743a45882b14c64377bc"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a824c38cd40b7743a45882b14c64377bc"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#a824c38cd40b7743a45882b14c64377bc">Forward_gpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;top)</td></tr>
<tr class="memdesc:a824c38cd40b7743a45882b14c64377bc"><td class="mdescLeft">&#160;</td><td class="mdescRight">Using the GPU device, compute the layer output. Fall back to <a class="el" href="classcaffe_1_1RecurrentLayer.html#af914a79d0cd024d6962b75325ab5ee94">Forward_cpu()</a> if unavailable. <br /></td></tr>
<tr class="separator:a824c38cd40b7743a45882b14c64377bc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a43ed5f8fce46753bf6a928d6d126d287"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a43ed5f8fce46753bf6a928d6d126d287"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#a43ed5f8fce46753bf6a928d6d126d287">Backward_cpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;top, const vector&lt; bool &gt; &amp;propagate_down, const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;bottom)</td></tr>
<tr class="memdesc:a43ed5f8fce46753bf6a928d6d126d287"><td class="mdescLeft">&#160;</td><td class="mdescRight">Using the CPU device, compute the gradients for any parameters and for the bottom blobs if propagate_down is true. <br /></td></tr>
<tr class="separator:a43ed5f8fce46753bf6a928d6d126d287"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pro_methods_classcaffe_1_1Layer"><td colspan="2" onclick="javascript:toggleInherit('pro_methods_classcaffe_1_1Layer')"><img src="closed.png" alt="-"/>&#160;Protected Member Functions inherited from <a class="el" href="classcaffe_1_1Layer.html">caffe::Layer&lt; Dtype &gt;</a></td></tr>
<tr class="memitem:a9275e5b8196feac9cf22803973c890f9 inherit pro_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a9275e5b8196feac9cf22803973c890f9"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a9275e5b8196feac9cf22803973c890f9">Backward_gpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;top, const vector&lt; bool &gt; &amp;propagate_down, const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;bottom)</td></tr>
<tr class="memdesc:a9275e5b8196feac9cf22803973c890f9 inherit pro_methods_classcaffe_1_1Layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Using the GPU device, compute the gradients for any parameters and for the bottom blobs if propagate_down is true. Fall back to <a class="el" href="classcaffe_1_1Layer.html#a64d15855f882af4b82e83fa993c4e7c6" title="Using the CPU device, compute the gradients for any parameters and for the bottom blobs if propagate_...">Backward_cpu()</a> if unavailable. <br /></td></tr>
<tr class="separator:a9275e5b8196feac9cf22803973c890f9 inherit pro_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adaa95e30dff155409a25ffcb5c8c885e inherit pro_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#adaa95e30dff155409a25ffcb5c8c885e">CheckBlobCounts</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;top)</td></tr>
<tr class="separator:adaa95e30dff155409a25ffcb5c8c885e inherit pro_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8bd62d1505dd35d6a3a25954ae9e6014 inherit pro_methods_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a8bd62d1505dd35d6a3a25954ae9e6014">SetLossWeights</a> (const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;top)</td></tr>
<tr class="separator:a8bd62d1505dd35d6a3a25954ae9e6014 inherit pro_methods_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pro-attribs"></a>
Protected Attributes</h2></td></tr>
<tr class="memitem:aa9b6cb6658e3bc8bfdf39441e751919a"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="aa9b6cb6658e3bc8bfdf39441e751919a"></a>
shared_ptr&lt; <a class="el" href="classcaffe_1_1Net.html">Net</a>&lt; Dtype &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#aa9b6cb6658e3bc8bfdf39441e751919a">unrolled_net_</a></td></tr>
<tr class="memdesc:aa9b6cb6658e3bc8bfdf39441e751919a"><td class="mdescLeft">&#160;</td><td class="mdescRight">A <a class="el" href="classcaffe_1_1Net.html" title="Connects Layers together into a directed acyclic graph (DAG) specified by a NetParameter. ">Net</a> to implement the Recurrent functionality. <br /></td></tr>
<tr class="separator:aa9b6cb6658e3bc8bfdf39441e751919a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4bf3c3a87b2a740987aec46e40717907"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a4bf3c3a87b2a740987aec46e40717907"></a>
int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#a4bf3c3a87b2a740987aec46e40717907">N_</a></td></tr>
<tr class="memdesc:a4bf3c3a87b2a740987aec46e40717907"><td class="mdescLeft">&#160;</td><td class="mdescRight">The number of independent streams to process simultaneously. <br /></td></tr>
<tr class="separator:a4bf3c3a87b2a740987aec46e40717907"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a02f79bca0ccde7543ecf172b328c860f"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a02f79bca0ccde7543ecf172b328c860f"></a>
int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#a02f79bca0ccde7543ecf172b328c860f">T_</a></td></tr>
<tr class="memdesc:a02f79bca0ccde7543ecf172b328c860f"><td class="mdescLeft">&#160;</td><td class="mdescRight">The number of timesteps in the layer's input, and the number of timesteps over which to backpropagate through time. <br /></td></tr>
<tr class="separator:a02f79bca0ccde7543ecf172b328c860f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7da45d2f90a99fe6e4250ffa6a533d97"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a7da45d2f90a99fe6e4250ffa6a533d97"></a>
bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#a7da45d2f90a99fe6e4250ffa6a533d97">static_input_</a></td></tr>
<tr class="memdesc:a7da45d2f90a99fe6e4250ffa6a533d97"><td class="mdescLeft">&#160;</td><td class="mdescRight">Whether the layer has a "static" input copied across all timesteps. <br /></td></tr>
<tr class="separator:a7da45d2f90a99fe6e4250ffa6a533d97"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0a7a7d94ed74d4199b9d7b8445d5aadb"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a0a7a7d94ed74d4199b9d7b8445d5aadb"></a>
int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#a0a7a7d94ed74d4199b9d7b8445d5aadb">last_layer_index_</a></td></tr>
<tr class="memdesc:a0a7a7d94ed74d4199b9d7b8445d5aadb"><td class="mdescLeft">&#160;</td><td class="mdescRight">The last layer to run in the network. (Any later layers are losses added to force the recurrent net to do backprop.) <br /></td></tr>
<tr class="separator:a0a7a7d94ed74d4199b9d7b8445d5aadb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abfafaacb1fece0309e750e0d307fb76e"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="abfafaacb1fece0309e750e0d307fb76e"></a>
bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1RecurrentLayer.html#abfafaacb1fece0309e750e0d307fb76e">expose_hidden_</a></td></tr>
<tr class="memdesc:abfafaacb1fece0309e750e0d307fb76e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Whether the layer's hidden state at the first and last timesteps are layer inputs and outputs, respectively. <br /></td></tr>
<tr class="separator:abfafaacb1fece0309e750e0d307fb76e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9da1173c6ccdb913c5a8ac8f4ca7f891"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a9da1173c6ccdb913c5a8ac8f4ca7f891"></a>
vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt;&#160;</td><td class="memItemRight" valign="bottom"><b>recur_input_blobs_</b></td></tr>
<tr class="separator:a9da1173c6ccdb913c5a8ac8f4ca7f891"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a53b1f9cd694e7dfcae00774ddfef0222"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a53b1f9cd694e7dfcae00774ddfef0222"></a>
vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt;&#160;</td><td class="memItemRight" valign="bottom"><b>recur_output_blobs_</b></td></tr>
<tr class="separator:a53b1f9cd694e7dfcae00774ddfef0222"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae0b7323dd7121854d05f85847e2f86c4"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ae0b7323dd7121854d05f85847e2f86c4"></a>
vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt;&#160;</td><td class="memItemRight" valign="bottom"><b>output_blobs_</b></td></tr>
<tr class="separator:ae0b7323dd7121854d05f85847e2f86c4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af32cf697eeee1edd5de1d7b359602ec2"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="af32cf697eeee1edd5de1d7b359602ec2"></a>
<a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; *&#160;</td><td class="memItemRight" valign="bottom"><b>x_input_blob_</b></td></tr>
<tr class="separator:af32cf697eeee1edd5de1d7b359602ec2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa918e06200e98287778abc5f3d97eeb1"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="aa918e06200e98287778abc5f3d97eeb1"></a>
<a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; *&#160;</td><td class="memItemRight" valign="bottom"><b>x_static_input_blob_</b></td></tr>
<tr class="separator:aa918e06200e98287778abc5f3d97eeb1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3b5f3118aa5a48d54fb624c04b98bd1d"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a3b5f3118aa5a48d54fb624c04b98bd1d"></a>
<a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; *&#160;</td><td class="memItemRight" valign="bottom"><b>cont_input_blob_</b></td></tr>
<tr class="separator:a3b5f3118aa5a48d54fb624c04b98bd1d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pro_attribs_classcaffe_1_1Layer"><td colspan="2" onclick="javascript:toggleInherit('pro_attribs_classcaffe_1_1Layer')"><img src="closed.png" alt="-"/>&#160;Protected Attributes inherited from <a class="el" href="classcaffe_1_1Layer.html">caffe::Layer&lt; Dtype &gt;</a></td></tr>
<tr class="memitem:a7ed12bb2df25c887e41d7ea9557fc701 inherit pro_attribs_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">LayerParameter&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a7ed12bb2df25c887e41d7ea9557fc701">layer_param_</a></td></tr>
<tr class="separator:a7ed12bb2df25c887e41d7ea9557fc701 inherit pro_attribs_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1d04ad7f595a82a1c811f102d68b8a19 inherit pro_attribs_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">Phase&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a1d04ad7f595a82a1c811f102d68b8a19">phase_</a></td></tr>
<tr class="separator:a1d04ad7f595a82a1c811f102d68b8a19 inherit pro_attribs_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8073fcf2c139b47eb99ce71b346b1321 inherit pro_attribs_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">vector&lt; shared_ptr&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#a8073fcf2c139b47eb99ce71b346b1321">blobs_</a></td></tr>
<tr class="separator:a8073fcf2c139b47eb99ce71b346b1321 inherit pro_attribs_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acd4a05def9ff3b42ad72404210613ef7 inherit pro_attribs_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">vector&lt; bool &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#acd4a05def9ff3b42ad72404210613ef7">param_propagate_down_</a></td></tr>
<tr class="separator:acd4a05def9ff3b42ad72404210613ef7 inherit pro_attribs_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af6d347229a139500994e7a926c680486 inherit pro_attribs_classcaffe_1_1Layer"><td class="memItemLeft" align="right" valign="top">vector&lt; Dtype &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1Layer.html#af6d347229a139500994e7a926c680486">loss_</a></td></tr>
<tr class="separator:af6d347229a139500994e7a926c680486 inherit pro_attribs_classcaffe_1_1Layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><h3>template&lt;typename Dtype&gt;<br />
class caffe::RecurrentLayer&lt; Dtype &gt;</h3>

<p>An abstract class for implementing recurrent behavior inside of an unrolled network. This <a class="el" href="classcaffe_1_1Layer.html" title="An interface for the units of computation which can be composed into a Net. ">Layer</a> type cannot be instantiated &ndash; instead, you should use one of its implementations which defines the recurrent architecture, such as <a class="el" href="classcaffe_1_1RNNLayer.html" title="Processes time-varying inputs using a simple recurrent neural network (RNN). Implemented as a network...">RNNLayer</a> or <a class="el" href="classcaffe_1_1LSTMLayer.html" title="Processes sequential inputs using a &quot;Long Short-Term Memory&quot; (LSTM) [1] style recurrent neural networ...">LSTMLayer</a>. </p>
</div><h2 class="groupheader">Member Function Documentation</h2>
<a class="anchor" id="ac8642c8d7f418b6513c93daffa5eb15e"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual bool <a class="el" href="classcaffe_1_1RecurrentLayer.html">caffe::RecurrentLayer</a>&lt; Dtype &gt;::AllowForceBackward </td>
          <td>(</td>
          <td class="paramtype">const int&#160;</td>
          <td class="paramname"><em>bottom_index</em></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Return whether to allow force_backward for a given bottom blob index. </p>
<p>If AllowForceBackward(i) == false, we will ignore the force_backward setting and backpropagate to blob i only if it needs gradient information (as is done when force_backward == false). </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1Layer.html#a4a2e4ca94eaa1cbc054b512c6657743e">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a class="anchor" id="ac53fa5447232a9067ef30028ad9da1cc"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int <a class="el" href="classcaffe_1_1RecurrentLayer.html">caffe::RecurrentLayer</a>&lt; Dtype &gt;::ExactNumTopBlobs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the exact number of top blobs required by the layer, or -1 if no exact number is required. </p>
<p>This method should be overridden to return a non-negative value if your layer expects some exact number of top blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1Layer.html#aa3c99ed707e8db683a3043412e151af8">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a class="anchor" id="af914a79d0cd024d6962b75325ab5ee94"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classcaffe_1_1RecurrentLayer.html">caffe::RecurrentLayer</a>&lt; Dtype &gt;::Forward_cpu </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;&#160;</td>
          <td class="paramname"><em>top</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">bottom</td><td>input <a class="el" href="classcaffe_1_1Blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 2-3)</td></tr>
  </table>
  </dd>
</dl>
<ol type="1">
<li><img class="formulaInl" alt="$ (T \times N \times ...) $" src="form_159.png"/> the time-varying input <img class="formulaInl" alt="$ x $" src="form_3.png"/>. After the first two axes, whose dimensions must correspond to the number of timesteps <img class="formulaInl" alt="$ T $" src="form_160.png"/> and the number of independent streams <img class="formulaInl" alt="$ N $" src="form_161.png"/>, respectively, its dimensions may be arbitrary. Note that the ordering of dimensions &ndash; <img class="formulaInl" alt="$ (T \times N \times ...) $" src="form_159.png"/>, rather than <img class="formulaInl" alt="$ (N \times T \times ...) $" src="form_162.png"/> &ndash; means that the <img class="formulaInl" alt="$ N $" src="form_161.png"/> independent input streams must be "interleaved".</li>
<li><img class="formulaInl" alt="$ (T \times N) $" src="form_163.png"/> the sequence continuation indicators <img class="formulaInl" alt="$ \delta $" src="form_164.png"/>. These inputs should be binary (0 or 1) indicators, where <img class="formulaInl" alt="$ \delta_{t,n} = 0 $" src="form_165.png"/> means that timestep <img class="formulaInl" alt="$ t $" src="form_37.png"/> of stream <img class="formulaInl" alt="$ n $" src="form_122.png"/> is the beginning of a new sequence, and hence the previous hidden state <img class="formulaInl" alt="$ h_{t-1} $" src="form_166.png"/> is multiplied by <img class="formulaInl" alt="$ \delta_t = 0 $" src="form_167.png"/> and has no effect on the cell's output at timestep <img class="formulaInl" alt="$ t $" src="form_37.png"/>, and a value of <img class="formulaInl" alt="$ \delta_{t,n} = 1 $" src="form_168.png"/> means that timestep <img class="formulaInl" alt="$ t $" src="form_37.png"/> of stream <img class="formulaInl" alt="$ n $" src="form_122.png"/> is a continuation from the previous timestep <img class="formulaInl" alt="$ t-1 $" src="form_169.png"/>, and the previous hidden state <img class="formulaInl" alt="$ h_{t-1} $" src="form_166.png"/> affects the updated hidden state and output.</li>
<li><img class="formulaInl" alt="$ (N \times ...) $" src="form_139.png"/> (optional) the static (non-time-varying) input <img class="formulaInl" alt="$ x_{static} $" src="form_170.png"/>. After the first axis, whose dimension must be the number of independent streams, its dimensions may be arbitrary. This is mathematically equivalent to using a time-varying input of <img class="formulaInl" alt="$ x'_t = [x_t; x_{static}] $" src="form_171.png"/> &ndash; i.e., tiling the static input across the <img class="formulaInl" alt="$ T $" src="form_160.png"/> timesteps and concatenating with the time-varying input. Note that if this input is used, all timesteps in a single batch within a particular one of the <img class="formulaInl" alt="$ N $" src="form_161.png"/> streams must share the same static input, even if the sequence continuation indicators suggest that difference sequences are ending and beginning within a single batch. This may require padding and/or truncation for uniform length.</li>
</ol>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">top</td><td>output <a class="el" href="classcaffe_1_1Blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 1)<ol type="1">
<li><img class="formulaInl" alt="$ (T \times N \times D) $" src="form_172.png"/> the time-varying output <img class="formulaInl" alt="$ y $" src="form_15.png"/>, where <img class="formulaInl" alt="$ D $" src="form_173.png"/> is <code>recurrent_param.num_output()</code>. Refer to documentation for particular <a class="el" href="classcaffe_1_1RecurrentLayer.html" title="An abstract class for implementing recurrent behavior inside of an unrolled network. This Layer type cannot be instantiated  instead, you should use one of its implementations which defines the recurrent architecture, such as RNNLayer or LSTMLayer. ">RecurrentLayer</a> implementations (such as <a class="el" href="classcaffe_1_1RNNLayer.html" title="Processes time-varying inputs using a simple recurrent neural network (RNN). Implemented as a network...">RNNLayer</a> and <a class="el" href="classcaffe_1_1LSTMLayer.html" title="Processes sequential inputs using a &quot;Long Short-Term Memory&quot; (LSTM) [1] style recurrent neural networ...">LSTMLayer</a>) for the definition of <img class="formulaInl" alt="$ y $" src="form_15.png"/>. </li>
</ol>
</td></tr>
  </table>
  </dd>
</dl>

<p>Implements <a class="el" href="classcaffe_1_1Layer.html#add965883f75bbf90c7a06f960cda7a1a">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a class="anchor" id="aafa788d0a3535c80478fee74b9dbb62b"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classcaffe_1_1RecurrentLayer.html">caffe::RecurrentLayer</a>&lt; Dtype &gt;::LayerSetUp </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;&#160;</td>
          <td class="paramname"><em>top</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Does layer-specific setup: your layer should implement this function as well as Reshape. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">bottom</td><td>the preshaped input blobs, whose data fields store the input data for this layer </td></tr>
    <tr><td class="paramname">top</td><td>the allocated but unshaped output blobs</td></tr>
  </table>
  </dd>
</dl>
<p>This method should do one-time layer specific setup. This includes reading and processing relevent parameters from the <code>layer_param_</code>. Setting up the shapes of top blobs and internal buffers should be done in <code>Reshape</code>, which will be called before the forward pass to adjust the top blob sizes. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1Layer.html#a38dc2488bf319b8de5a7ac84e0045393">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a class="anchor" id="a10254a9cb58d36d029b2da9337ad16e9"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int <a class="el" href="classcaffe_1_1RecurrentLayer.html">caffe::RecurrentLayer</a>&lt; Dtype &gt;::MaxBottomBlobs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the maximum number of bottom blobs required by the layer, or -1 if no maximum number is required. </p>
<p>This method should be overridden to return a non-negative value if your layer expects some maximum number of bottom blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1Layer.html#a6408ef3939f1abed1abcec46ff219289">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a class="anchor" id="a1deb403821dc383f13e8bf2a1eeafdf9"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int <a class="el" href="classcaffe_1_1RecurrentLayer.html">caffe::RecurrentLayer</a>&lt; Dtype &gt;::MinBottomBlobs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the minimum number of bottom blobs required by the layer, or -1 if no minimum number is required. </p>
<p>This method should be overridden to return a non-negative value if your layer expects some minimum number of bottom blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1Layer.html#ade3eee97cc743c4e68fff7eba6484916">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a class="anchor" id="a07a6d838be5330334335256811d2b6f6"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classcaffe_1_1RecurrentLayer.html">caffe::RecurrentLayer</a>&lt; Dtype &gt;::Reshape </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1Blob.html">Blob</a>&lt; Dtype &gt; * &gt; &amp;&#160;</td>
          <td class="paramname"><em>top</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Adjust the shapes of top blobs and internal buffers to accommodate the shapes of the bottom blobs. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">bottom</td><td>the input blobs, with the requested input shapes </td></tr>
    <tr><td class="paramname">top</td><td>the top blobs, which should be reshaped as needed</td></tr>
  </table>
  </dd>
</dl>
<p>This method should reshape top blobs as needed according to the shapes of the bottom (input) blobs, as well as reshaping any internal buffers and making any other necessary adjustments so that the layer can accommodate the bottom blobs. </p>

<p>Implements <a class="el" href="classcaffe_1_1Layer.html#ad9d391b972c769c0ebee34ca6d1c973e">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>include/caffe/layers/<a class="el" href="lstm__layer_8hpp_source.html">lstm_layer.hpp</a></li>
<li>include/caffe/layers/<a class="el" href="recurrent__layer_8hpp_source.html">recurrent_layer.hpp</a></li>
<li>src/caffe/layers/recurrent_layer.cpp</li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Fri Sep 16 2016 21:51:45 for Caffe by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
</body>
</html>
