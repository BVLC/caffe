{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training Networks for Segmentation\n",
    "\n",
    "This example explains how to train networks that compute image segmentations, which are given by classifying each individual pixel of an input image. Although this task is different from (whole) image classification similar architectures can be adapted to this task. \n",
    "\n",
    "This example explains how to train the FCN-32s network for the PASCAL VOC dataset. Similar to the previous example this example will make use of a pre-trained network, which gives us the following outline:\n",
    "\n",
    "1. Downloading Datasets\n",
    "2. Convolutional conversion of Pre-Trained Model\n",
    "3. Constructing and Training Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caffe_root = '../'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "\n",
    "import sys\n",
    "#sys.path.insert(0, caffe_root + 'python')\n",
    "import caffe\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Downloading Datasets\n",
    "\n",
    "Networks trained for the PASCAL VOC challenge usually also use the supplementary segmentations of the SDS Dataset. To use these we have to download both. This is done by the `get_pascal.sh` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "Downloading...\n"
     ]
    }
   ],
   "source": [
    "# run scripts from caffe root\n",
    "import os\n",
    "os.chdir(caffe_root)\n",
    "\n",
    "# Download data\n",
    "!data/pascal/get_voc2011.sh\n",
    "!data/sbdd/get_sbdd.sh\n",
    "\n",
    "# back to examples\n",
    "os.chdir('examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convolutional conversion of Pre-Trained Model\n",
    "\n",
    "When training a segmentation network we want to make use of the weight from a pre-trained VGG-16 model. To do segmentation we want to cast the (fully connected) inner product layers as convolutional layers. This conversion does not happen automatically, so lets first create a model file with only convolutional layers, which we can import easily. For this we first have to download the modlel weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\r\n"
     ]
    }
   ],
   "source": [
    "#We are currently in the examples folder, download vgg16fc.caffemodel\n",
    "import os\n",
    "os.chdir(\"segmentation\")\n",
    "!./get_vgg16.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying shared layer conv1_1\n",
      "Copying shared layer conv1_2\n",
      "Copying shared layer conv2_1\n",
      "Copying shared layer conv2_2\n",
      "Copying shared layer conv3_1\n",
      "Copying shared layer conv3_2\n",
      "Copying shared layer conv3_3\n",
      "Copying shared layer conv4_1\n",
      "Copying shared layer conv4_2\n",
      "Copying shared layer conv4_3\n",
      "Copying shared layer conv5_1\n",
      "Copying shared layer conv5_2\n",
      "Copying shared layer conv5_3\n",
      "(source) fc6 weights are (4096, 25088) dimensional and biases are (4096,) dimensional\n",
      "(destn.) fc6 weights are (4096, 512, 7, 7) dimensional and biases are (4096,) dimensional\n",
      "(source) fc7 weights are (4096, 4096) dimensional and biases are (4096,) dimensional\n",
      "(destn.) fc7 weights are (4096, 4096, 1, 1) dimensional and biases are (4096,) dimensional\n"
     ]
    }
   ],
   "source": [
    "#With the weights in place we can now perform the network surgery\n",
    "from convert_vgg import convert_net\n",
    "convert_net(\"VGG_ILSVRC_16_layers.caffemodel\",\"vgg16fc.caffemodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Constructing and Training Net\n",
    "\n",
    "With the convolutional version of VGG we can now start training. For this we first have to create the network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘snapshot’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "#create the train.prototxt and val.prototxt files, similar to previous example\n",
    "!python2 net.py\n",
    "\n",
    "#make snapshot directory\n",
    "!mkdir snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new network contains a deconvolutional layer that perform upsampling in order to counteracts the subsampling performed by pooling layers, ensuring that the output segmentation has the same size as the input image. In this example upsampling by a factor of 32 is performed, hence the name FCN-32s. This upsampling layer needs to be initialized with weights so that it performs bi-linear upsampling.\n",
    "\n",
    "Warning: When training the FCN-16s and FCN-8s networks these should be intialized with the weights of a *trained* FCN-32s net, not the VGG-16 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = 'vgg16fc.caffemodel'\n",
    "\n",
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "solver = caffe.SGDSolver('solver.prototxt')\n",
    "solver.net.copy_from(weights)\n",
    "\n",
    "# surgery\n",
    "import surgery\n",
    "interp_layers = [k for k in solver.net.params.keys() if 'up' in k]\n",
    "surgery.interp(solver.net, interp_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the correctly initialized network we can now start training. During training we want to evaluate segmentation performance, this is done using a special scoring function. Choosing a scoring function means we do not have to worry about setting the `batch_size` and `test_iter` in accordance to the test dataset size, but it requires training through python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scoring\n",
    "import score\n",
    "val = np.loadtxt('../../data/pascal/seg11valid.txt', dtype=str)\n",
    "\n",
    "for _ in range(25):\n",
    "    solver.step(4000)\n",
    "    score.seg_tests(solver, False, val, layer='score')\n",
    "\n",
    "#Save network.\n",
    "solver.net.save(\"snapshot/final.caffemodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Further Networks\n",
    "\n",
    "This example is based on the examples provided [here](fcn.berkleyvision.org). Have a look there for how to train more accurate FCN-16s and FCN-8s architectures. All python files used in this example are the same, the training code from section 3 is provided in the `solve.py` files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 2)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
