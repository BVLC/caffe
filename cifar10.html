<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Caffe</title>

    <link rel="stylesheet" href="stylesheets/reset.css">
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="javascripts/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-46255508-1', 'daggerfs.com');
    ga('send', 'pageview');
  </script>
    <div class="wrapper">
      <header>
        <h1 class="header"><a href="index.html">Caffe</a></h1>
        <p class="header">Convolutional Architecture for Fast Feature Embedding</p>

        <ul>
          <!--<li class="download"><a class="buttons" href="https://github.com/BVLC/caffe/zipball/master">Download ZIP</a></li>
          <li class="download"><a class="buttons" href="https://github.com/BVLC/caffe/tarball/master">Download TAR</a></li>-->
          <li><a class="buttons github" href="https://github.com/BVLC/caffe">View On GitHub</a></li>
        </ul>
        <p class="header">Maintained by<br><a class="header name" href="http://ucbvlc.org/">BVLC</a></p>
        <p class="header">Created by<br><a class="header name" href="http://daggerfs.com/">Yangqing Jia</a></p>

      </header>
      <section>

      <h1 id="alexs_cifar10_tutorial_caffe_style">Alex’s CIFAR-10 tutorial, Caffe style</h1>

<p>Alex Krizhevsky’s <a href="https://code.google.com/p/cuda-convnet/">cuda-convnet</a> details the model definitions, parameters, and training procedure for good performance on CIFAR-10. This example reproduces his results in Caffe.</p>

<p>We will assume that you have Caffe successfully compiled. If not, please refer to the <a href="installation.html">Installation page</a>. In this tutorial, we will assume that your caffe installation is located at <code>CAFFE_ROOT</code>.</p>

<p>We thank @chyojn for the pull request that defined the model schemas and solver configurations.</p>

<p><em>This example is a work-in-progress. It would be nice to further explain details of the network and training choices and benchmark the full training.</em></p>

<h2 id="prepare_the_dataset">Prepare the Dataset</h2>

<p>You will first need to download and convert the data format from the <a href="http://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10 website</a>. To do this, simply run the following commands:</p>

<pre><code>cd $CAFFE_ROOT/data/cifar10
./get_cifar10.sh
cd $CAFFE_ROOT/examples/cifar10
./create_cifar10.sh</code></pre>

<p>If it complains that <code>wget</code> or <code>gunzip</code> are not installed, you need to install them respectively. After running the script there should be the dataset, <code>./cifar10-leveldb</code>, and the data set image mean <code>./mean.binaryproto</code>.</p>

<h2 id="the_model">The Model</h2>

<p>The CIFAR-10 model is a CNN that composes layers of convolution, pooling, rectified linear unit (ReLU) nonlinearities, and local contrast normalization with a linear classifier on top of it all. We have defined the model in the <code>CAFFE_ROOT/examples/cifar10</code> directory’s <code>cifar10_quick_train.prototxt</code>.</p>

<h2 id="training_and_testing_the_quick_model">Training and Testing the “Quick” Model</h2>

<p>Training the model is simple after you have written the network definition protobuf and solver protobuf files. Simply run <code>train_quick.sh</code>, or the following command directly:</p>

<pre><code>cd $CAFFE_ROOT/examples/cifar10
./train_quick.sh</code></pre>

<p><code>train_quick.sh</code> is a simple script, so have a look inside. <code>GLOG_logtostderr=1</code> is the google logging flag that prints all the logging messages directly to stderr. The main tool for training is <code>train_net.bin</code>, with the solver protobuf text file as its argument.</p>

<p>When you run the code, you will see a lot of messages flying by like this:</p>

<pre><code>I0317 21:52:48.945710 2008298256 net.cpp:74] Creating Layer conv1
I0317 21:52:48.945716 2008298256 net.cpp:84] conv1 &lt;- data
I0317 21:52:48.945725 2008298256 net.cpp:110] conv1 -&gt; conv1
I0317 21:52:49.298691 2008298256 net.cpp:125] Top shape: 100 32 32 32 (3276800)
I0317 21:52:49.298719 2008298256 net.cpp:151] conv1 needs backward computation.</code></pre>

<p>These messages tell you the details about each layer, its connections and its output shape, which may be helpful in debugging. After the initialization, the training will start:</p>

<pre><code>I0317 21:52:49.309370 2008298256 net.cpp:166] Network initialization done.
I0317 21:52:49.309376 2008298256 net.cpp:167] Memory required for Data 23790808
I0317 21:52:49.309422 2008298256 solver.cpp:36] Solver scaffolding done.
I0317 21:52:49.309447 2008298256 solver.cpp:47] Solving CIFAR10_quick_train</code></pre>

<p>Based on the solver setting, we will print the training loss function every 100 iterations, and test the network every 500 iterations. You will see messages like this:</p>

<pre><code>I0317 21:53:12.179772 2008298256 solver.cpp:208] Iteration 100, lr = 0.001
I0317 21:53:12.185698 2008298256 solver.cpp:65] Iteration 100, loss = 1.73643
...
I0317 21:54:41.150030 2008298256 solver.cpp:87] Iteration 500, Testing net
I0317 21:54:47.129461 2008298256 solver.cpp:114] Test score #0: 0.5504
I0317 21:54:47.129500 2008298256 solver.cpp:114] Test score #1: 1.27805</code></pre>

<p>For each training iteration, <code>lr</code> is the learning rate of that iteration, and <code>loss</code> is the training function. For the output of the testing phase, <strong>score 0 is the accuracy</strong>, and <strong>score 1 is the testing loss function</strong>.</p>

<p>And after making yourself a cup of coffee, you are done!</p>

<pre><code>I0317 22:12:19.666914 2008298256 solver.cpp:87] Iteration 5000, Testing net
I0317 22:12:25.580330 2008298256 solver.cpp:114] Test score #0: 0.7533
I0317 22:12:25.580379 2008298256 solver.cpp:114] Test score #1: 0.739837
I0317 22:12:25.587262 2008298256 solver.cpp:130] Snapshotting to cifar10_quick_iter_5000
I0317 22:12:25.590215 2008298256 solver.cpp:137] Snapshotting solver state to cifar10_quick_iter_5000.solverstate
I0317 22:12:25.592813 2008298256 solver.cpp:81] Optimization Done.</code></pre>

<p>Our model achieved ~75% test accuracy. The model parameters are stored in binary protobuf format in</p>

<pre><code>cifar10_quick_iter_5000</code></pre>

<p>which is ready-to-deploy in CPU or GPU mode! Refer to the <code>CAFFE_ROOT/examples/cifar10/cifar10_quick.prototxt</code> for the deployment model definition that can be called on new data.</p>

<h2 id="why_train_on_a_gpu">Why train on a GPU?</h2>

<p>CIFAR-10, while still small, has enough data to make GPU training attractive.</p>

<p>To compare CPU vs. GPU training speed, simply change one line in all the <code>cifar*solver.prototxt</code>:</p>

<pre><code># solver mode: 0 for CPU and 1 for GPU
solver_mode: 0</code></pre>

<p>and you will be using CPU for training.</p>

      </section>
      <footer>
        <p><small>Hosted on <a href="http://pages.github.com">GitHub Pages</a>.</small></p>
      </footer>
    </div>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
  </body>
</html>
