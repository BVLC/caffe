{
  "_args": [
    [
      "bufferstream@>= 0.5.0",
      "/Users/stevenjames/Documents/caffe/node_modules/tarball"
    ]
  ],
  "_from": "bufferstream@>=0.5.0",
  "_id": "bufferstream@0.6.2",
  "_inCache": true,
  "_installable": true,
  "_location": "/bufferstream",
  "_npmUser": {
    "email": "dodo@blacksec.org",
    "name": "dodo"
  },
  "_npmVersion": "1.3.14",
  "_phantomChildren": {},
  "_requested": {
    "name": "bufferstream",
    "raw": "bufferstream@>= 0.5.0",
    "rawSpec": ">= 0.5.0",
    "scope": null,
    "spec": ">=0.5.0",
    "type": "range"
  },
  "_requiredBy": [
    "/tarball"
  ],
  "_resolved": "https://registry.npmjs.org/bufferstream/-/bufferstream-0.6.2.tgz",
  "_shasum": "a5f27e10d3c760084d14b35126615007319e3731",
  "_shrinkwrap": null,
  "_spec": "bufferstream@>= 0.5.0",
  "_where": "/Users/stevenjames/Documents/caffe/node_modules/tarball",
  "author": {
    "name": "dodo",
    "url": "https://github.com/dodo"
  },
  "bugs": {
    "url": "https://github.com/dodo/node-bufferstream/issues"
  },
  "dependencies": {
    "bufferjs": ">= 2.0.0",
    "buffertools": ">= 1.0.3"
  },
  "description": "painless stream buffering and cutting",
  "devDependencies": {
    "cli": ">= 0.3.7",
    "coffee-script": ">= 1.1.2",
    "express": ">= 2.4.5",
    "muffin": ">= 0.2.6",
    "nodeunit": ">= 0.5.4"
  },
  "directories": {},
  "dist": {
    "shasum": "a5f27e10d3c760084d14b35126615007319e3731",
    "tarball": "http://registry.npmjs.org/bufferstream/-/bufferstream-0.6.2.tgz"
  },
  "engines": {
    "node": ">= 0.4.x"
  },
  "homepage": "https://github.com/dodo/node-bufferstream",
  "keywords": [
    "buffer",
    "buffers",
    "stream",
    "streams"
  ],
  "licenses": [
    {
      "type": "MIT",
      "url": "http://github.com/dodo/node-bufferstream/raw/master/LICENSE"
    }
  ],
  "main": "bufferstream.js",
  "maintainers": [
    {
      "name": "dodo",
      "email": "dodo@blacksec.org"
    }
  ],
  "name": "bufferstream",
  "optionalDependencies": {
    "buffertools": ">= 1.0.3"
  },
  "readme": "# BufferStream\n\npainless stream buffering, cutting and piping.\n\n## install\n\n    npm install bufferstream\n\n## api\n\nBufferStream is a full node.js [Stream](http://nodejs.org/docs/v0.4.7/api/streams.html) so it has apis of both [Writeable Stream](http://nodejs.org/docs/v0.4.7/api/streams.html#writable_Stream) and [Readable Stream](http://nodejs.org/docs/v0.4.7/api/streams.html#readable_Stream).\n\n### BufferStream\n\n```javascript\nBufferStream = require('bufferstream')\nstream = new BufferStream([{encoding:'utf8', size:'none'}]) // default\n```\n * `encoding` default encoding for writing strings\n * `blocking` if true and the source is a child_process the stream will block the entire process (timeouts wont work anymore, but splitting and listening on data still works, because they work sync)\n * `size` defines buffer level or sets buffer to given size (see ↓`setSize` for more)\n * `disabled` immediately call disable\n * `split` short form for:\n\n```javascript\nstream.split(token, function (chunk) {stream.emit('data', chunk)})\n```\n\n### stream.setSize\n\n```javascript\nstream.setSize(size) // can be one of ['none', 'flexible', <number>]\n```\n\ndifferent buffer behaviors can be triggered by size:\n\n * `none` when output drains, bufferstream drains too\n * `flexible` buffers everthing that it gets and not piping out\n * `<number>` `TODO` buffer has given size. buffers everthing until buffer is full. when buffer is full then  the stream will drain\n\n### stream.enable\n\n```javascript\nstream.enable()\n```\n\nenables stream buffering __default__\n\n### stream.disable\n\n```javascript\nstream.disable()\n```\n\nflushes buffer and disables stream buffering.\nBufferStream now pipes all data as long as the output accepting data.\nwhen the output is draining BufferStream will buffer all input temporary.\n\n```javascript\nstream.disable(token, ...)\nstream.disable(tokens) // Array\n```\n * `token[s]` buffer splitters (should be String or Buffer)\n\ndisables given tokens. wont flush until no splitter tokens are left.\n\n### stream.split\n\n```javascript\nstream.split(token, ...)\nstream.split(tokens) // Array\n```\n * `token[s]` buffer splitters (should be String or Buffer)\n\neach time BufferStream finds a splitter token in the input data it will emit a __split__ event.\nthis also works for binary data.\n\n### Event: 'split'\n\n```javascript\nstream.on('split', function (chunk, token) {…})\nstream.split(token, function (chunk, token) {…}) // only get called for this particular token\n```\n\nwhenever the stream is enabled it will try to find all splitter token in `stream.buffer`,\ncut it off and emit the chunk (without token) as __split__ event.\nthis data will be lost when not handled.\n\nthe chunk is the cut off of `stream.buffer` without the token.\n\n__Warning:__ try to avoid calling `stream.emit('data', newchunk)` more than one time, because this will likely throw `Error: Offset is out of bounds`.\n\n### stream.getBuffer\n\n```javascript\nstream.getBuffer()\n// or just\nstream.buffer\n```\n\nreturns its [Buffer](http://nodejs.org/docs/v0.4.7/api/buffers.html).\n\n### stream.toString\n\n```javascript\nstream.toString()\n```\n\nshortcut for `stream.buffer.toString()`\n\n### stream.length\n\n```javascript\nstream.length\n```\n\nshortcut for `stream.buffer.length`\n\n### PostBuffer\n\n```javascript\nPostBuffer = require('bufferstream/postbuffer')\npost = new PostBuffer(req)\n```\n * `req` http.ServerRequest\n\nfor if you want to get all the post data from a http server request and do some db reqeust before.\n\nbuffer http client\n\n### post.onEnd\n\n```javascript\npost.onEnd(function (data) {…});\n```\n\nset a callback to get all post data from a http server request\n\n### post.pipe\n\n```javascript\npost.pipe(stream, options);\n```\n\npumps data into another stream to allow incoming streams\ngiven options will be passed to Stream.pipe\n\n## note\n\nTo improve platform independence bufferstream is using `bufferjs` instead of `buffertools` since version `0.6.0`.\nJust run `npm install buffertools` to use their implementation of `Buffer.indexOf` which is sligthly faster than `bufferjs`'s version.\nif you're forced to use the javascript-only version of `Buffer.indexOf` (like on windows) you can disable the warning by:\n```javascript\nrequire('bufferstream').fn.warn = false\n```\n\n## example\n\n```javascript\nBufferStream = require('bufferstream')\nstream = new BufferStream({encoding:'utf8', size:'flexible'})\nstream.split('//', ':')\nstream.on('split', function (chunk, token) {\n    console.log(\"got '%s' by '%s'\", chunk.toString(), token.toString())\n})\nstream.write(\"buffer:stream//23\")\nconsole.log(stream.toString())\n```\n\nresults in\n\n    got 'buffer' by ':'\n    got 'stream' by '//'\n    23\n\n* https://github.com/dodo/node-bufferstream/blob/master/example/split.js\n\n## FAQ\n\n> I'm not sure from your readme what the split event emits?\n\nyou can specify more than one split token .. so it's emitted whenever\na token is found.\n\n> does it emit the buffer up to the just before the token starts?\n\nyes.\n\n> also, does it join buffers together if they do not already end in a token?\n\nwhen size is `flexible` it joins everything together what it gets to\none buffer (accessible through `stream.buffer` or\n`stream.getBuffer()`)\nwhenever it gets data, it will try to find all tokens\n\n> in other words, can I use this to rechunk a stream so that the chunks always break on newlines, for example?\n\nyes.\n\n```javascript\nstream = new BufferStream({size:'flexible'});\nstream.split('\\n', function (line) { // line doesn't have a '\\n' anymore\n    stream.emit('data', line); // Buffer.isBuffer(line) === true\n});\n```\n\n[![Build Status](https://secure.travis-ci.org/dodo/node-bufferstream.png)](http://travis-ci.org/dodo/node-bufferstream)\n",
  "readmeFilename": "README.md",
  "repository": {
    "type": "git",
    "url": "git://github.com/dodo/node-bufferstream.git"
  },
  "scripts": {
    "prepublish": "cake build",
    "test": "cake build && nodeunit test"
  },
  "version": "0.6.2"
}
