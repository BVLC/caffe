// AUTOMATICALLY GENERATED FILE, DO NOT EDIT
#include "caffe/common.hpp"
#ifdef USE_GREENTEA
#include "caffe/greentea/cl_kernels.hpp"
#include <sstream>
#include <string>
#include <type_traits>
namespace caffe {
#ifdef USE_INDEX_64
static std::string header = "#ifndef __OPENCL_VERSION__\n#define __kernel\n#define __global\n#define __constant\n#define __local\n#define get_global_id(x) 0\n#define get_global_size(x) 0\n#define get_local_id(x) 0\n#define get_local_size(x) 0\n#define FLT_MAX 0\n#define FLT_MIN 0\n#define cl_khr_fp64\n#define cl_amd_fp64\n#define DOUBLE_SUPPORT_AVAILABLE\n#define CLK_LOCAL_MEM_FENCE\n#define CLK_GLOBAL_MEM_FENCE\n#define Dtype float\n#define barrier(x)\n#define atomic_cmpxchg(x, y, z) x\n#define signbit(x) x\n#define int_tp long\n#define uint_tp unsigned long\n#define int_tpc long\n#define uint_tpc unsigned long\n#endif\n\n#define CONCAT(A,B) A##_##B\n#define TEMPLATE(name,type) CONCAT(name,type)\n\n#define TYPE_FLOAT 1\n#define TYPE_DOUBLE 2\n\n#if defined(cl_khr_fp64)\n#pragma OPENCL EXTENSION cl_khr_fp64 : enable\n#define DOUBLE_SUPPORT_AVAILABLE\n#elif defined(cl_amd_fp64)\n#pragma OPENCL EXTENSION cl_amd_fp64 : enable\n#define DOUBLE_SUPPORT_AVAILABLE\n#endif\n\n#if defined(cl_khr_int64_base_atomics)\n#pragma OPENCL EXTENSION cl_khr_int64_base_atomics : enable\n#define ATOMICS_64_AVAILABLE\n#endif";  // NOLINT
static std::string definitions_64 = "// Types used for parameters, offset computations and so on\n#define int_tp long\n#define uint_tp unsigned long\n\n// Definitions used to cast the types above as needed\n#define int_tpc long\n#define uint_tpc unsigned long";  // NOLINT
#else
static std::string header = "#ifndef __OPENCL_VERSION__\n#define __kernel\n#define __global\n#define __constant\n#define __local\n#define get_global_id(x) 0\n#define get_global_size(x) 0\n#define get_local_id(x) 0\n#define get_local_size(x) 0\n#define FLT_MAX 0\n#define FLT_MIN 0\n#define cl_khr_fp64\n#define cl_amd_fp64\n#define DOUBLE_SUPPORT_AVAILABLE\n#define CLK_LOCAL_MEM_FENCE\n#define CLK_GLOBAL_MEM_FENCE\n#define Dtype float\n#define barrier(x)\n#define atomic_cmpxchg(x, y, z) x\n#define signbit(x) x\n#define int_tp long\n#define uint_tp unsigned long\n#define int_tpc long\n#define uint_tpc unsigned long\n#endif\n\n#define CONCAT(A,B) A##_##B\n#define TEMPLATE(name,type) CONCAT(name,type)\n\n#define TYPE_FLOAT 1\n#define TYPE_DOUBLE 2\n\n#if defined(cl_khr_fp64)\n#pragma OPENCL EXTENSION cl_khr_fp64 : enable\n#define DOUBLE_SUPPORT_AVAILABLE\n#elif defined(cl_amd_fp64)\n#pragma OPENCL EXTENSION cl_amd_fp64 : enable\n#define DOUBLE_SUPPORT_AVAILABLE\n#endif\n\n#if defined(cl_khr_int64_base_atomics)\n#pragma OPENCL EXTENSION cl_khr_int64_base_atomics : enable\n#define ATOMICS_64_AVAILABLE\n#endif";  // NOLINT
static std::string definitions_32 = "// Types used for parameters, offset computations and so on\n#define int_tp int\n#define uint_tp unsigned int\n\n// Definitions used to cast the types above as needed\n#define int_tpc int\n#define uint_tpc unsigned int";  // NOLINT
#endif
static std::string cl_kernels[] = {
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(relu_forward,Dtype)(const int_tp n,\n                                           __global const Dtype* in,\n                                           __global Dtype* out,\n                                           Dtype negative_slope) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] = in[index] > 0 ? in[index] : in[index] * negative_slope;\n  }\n}\n\n__kernel void TEMPLATE(relu_backward,Dtype)(const int_tp n,\n                                            __global const Dtype* in_diff,\n                                            __global const Dtype* in_data,\n                                            __global Dtype* out_diff,\n                                            Dtype negative_slope) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out_diff[index] = in_diff[index]\n        * ((in_data[index] > 0?1.0:0.0) + (in_data[index] <= 0?1.0:0.0) * negative_slope);\n  }\n}\n\n__kernel void TEMPLATE(tanh_forward,Dtype)(const int_tp n,\n                                           __global const Dtype* in,\n                                           __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] = tanh(in[index]);\n  }\n}\n\n__kernel void TEMPLATE(tanh_backward,Dtype)(const int_tp n,\n                                            __global const Dtype* in_diff,\n                                            __global const Dtype* out_data,\n                                            __global Dtype* out_diff) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    Dtype tanhx = out_data[index];\n    out_diff[index] = in_diff[index] * (1 - tanhx * tanhx);\n  }\n}\n\n__kernel void TEMPLATE(sigmoid_forward,Dtype)(const int_tp n,\n                                              __global const Dtype* in,\n                                              __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] = 1.0 / (1.0 + exp(-in[index]));\n  }\n}\n\n__kernel void TEMPLATE(sigmoid_backward,Dtype)(const int_tp n,\n                                               __global const Dtype* in_diff,\n                                               __global const Dtype* out_data,\n                                               __global Dtype* out_diff) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    const Dtype sigmoid_x = out_data[index];\n    out_diff[index] = in_diff[index] * sigmoid_x * (1 - sigmoid_x);\n  }\n}\n\n__kernel void TEMPLATE(threshold,Dtype)(const int_tp n, const Dtype threshold,\n                                        __global const Dtype* in,\n                                        __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] = in[index] > threshold ? 1.0 : 0.0;\n  }\n}\n\n__kernel void TEMPLATE(prelu_forward,Dtype)(const int_tp n, const int_tp channels,\n                                            const int_tp dim,\n                                            __global const Dtype* in,\n                                            __global Dtype* out,\n                                            __global const Dtype* slope_data,\n                                            const int_tp div_factor) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    int_tp c = (index / dim) % channels / div_factor;\n    out[index] = in[index] > 0 ? in[index] : in[index] * slope_data[c];\n  }\n}\n\n__kernel void TEMPLATE(prelu_backward,Dtype)(const int_tp n, const int_tp channels,\n                                             const int_tp dim,\n                                             __global const Dtype* in_diff,\n                                             __global const Dtype* in_data,\n                                             __global Dtype* out_diff,\n                                             __global const Dtype* slope_data,\n                                             const int_tp div_factor) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    int_tp c = (index / dim) % channels / div_factor;\n    out_diff[index] = in_diff[index]\n        * ((in_data[index] > 0?1.0:0.0) + (in_data[index] <= 0?1.0:0.0) * slope_data[c]);\n  }\n}\n\n__kernel void TEMPLATE(prelu_param_backward,Dtype)(const int_tp n, const int_tp rows,\n                                                   const int_tp rowPitch,\n                                                   __global const Dtype* in_diff,\n                                                   __global const Dtype* in_data,\n                                                   __global Dtype* out_diff) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out_diff[index] = in_diff[index] * in_data[index] * (in_data[index] <= 0?1.0:0.0);\n    for (int k = 1; k < rows; k++) {\n      out_diff[index] += in_diff[index + k * rowPitch]\n          * in_data[index + k * rowPitch]\n          * (in_data[index + k * rowPitch] <= 0?1.0:0.0);\n    }\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(gpu_set,Dtype)(const int_tp n, const Dtype alpha, __global Dtype* y) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[index] = alpha;\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(br_forward,Dtype)(const int_tp count, const int_tp inner_dim,\n                                         __global const Dtype* in,\n                                         __global const Dtype* permut,\n                                         __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    int_tp n = index / (inner_dim);\n    int_tp in_n = (int_tp) (permut[n]);\n    out[index] = in[in_n * (inner_dim) + index % (inner_dim)];\n  }\n}\n\n__kernel void TEMPLATE(br_backward,Dtype)(const int_tp count, const int_tp inner_dim,\n                                          __global const Dtype* in,\n                                          __global const Dtype* top_indexes,\n                                          __global const Dtype* begins,\n                                          __global const Dtype* counts,\n                                          __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    int_tp n = index / (inner_dim);\n    out[index] = 0;\n    int_tp lower = (int_tp) (begins[n]);\n    int_tp upper = lower + (int_tp) (counts[n]);\n    for (int_tp i = lower; i < upper; ++i) {\n      int_tp in_n = (int_tp) (top_indexes[i]);\n      out[index] += in[in_n * (inner_dim) + index % (inner_dim)];\n    }\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(null_kernel,Dtype)(void) {\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(bias_forward,Dtype)(const int_tp n,\n                                           __global const Dtype* in,\n                                           __global const Dtype* bias,\n                                           const int_tp bias_dim,\n                                           const int_tp inner_dim,\n                                           __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < n;\n      index += get_global_size(0)) {\n    const int_tp bias_index = (index / inner_dim) % bias_dim;\n    out[index] = in[index] + bias[bias_index];\n  }\n}\n\n__kernel void TEMPLATE(scale_forward,Dtype)(const int_tp n,\n                                            __global const Dtype* in,\n                                            __global const Dtype* scale,\n                                            const int_tp scale_dim,\n                                            const int_tp inner_dim,\n                                            __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < n;\n      index += get_global_size(0)) {\n    const int_tp scale_index = (index / inner_dim) % scale_dim;\n    out[index] = in[index] * scale[scale_index];\n  }\n}\n\n__kernel void TEMPLATE(scale_bias_forward,Dtype)(const int_tp n,\n                                                 __global const Dtype* in,\n                                                 __global const Dtype* scale,\n                                                 __global const Dtype* bias,\n                                                 const int_tp scale_dim,\n                                                 const int_tp inner_dim,\n                                                 __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < n;\n      index += get_global_size(0)) {\n    const int_tp scale_index = (index / inner_dim) % scale_dim;\n    out[index] = in[index] * scale[scale_index] + bias[scale_index];\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(bnll_forward,Dtype)(const int_tp n,\n                                           __global const Dtype* in,\n                                           __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    if (in[index] > 0.0f) {\n      out[index] = in[index] + log((Dtype) (1.0 + exp(-in[index])));\n    } else {\n      out[index] = log((Dtype) (1.0 + exp(in[index])));\n    }\n  }\n}\n\n__kernel void TEMPLATE(bnll_backward,Dtype)(const int_tp n,\n                                            __global const Dtype* in_diff,\n                                            __global const Dtype* in_data,\n                                            __global Dtype* out_diff) {\n  Dtype kBNLL_THRESHOLD = 50.;\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    Dtype expval = exp(min(in_data[index], kBNLL_THRESHOLD));\n    out_diff[index] = in_diff[index] * expval / (expval + 1.);\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(kernel_channel_max,Dtype)(const int_tp num, const int_tp channels,\n                                   const int_tp spatial_dim,\n                                   __global const Dtype* data,\n                                   __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < num * spatial_dim; index +=\n      get_global_size(0)) {\n    int_tp n = index / spatial_dim;\n    int_tp s = index % spatial_dim;\n    float maxval = -FLT_MAX;\n    for (int_tp c = 0; c < channels; ++c) {\n      maxval = max((Dtype)(data[(n * channels + c) * spatial_dim + s]), (Dtype)maxval);\n    }\n    out[index] = maxval;\n  }\n}\n\n__kernel void TEMPLATE(kernel_channel_subtract,Dtype)(const int_tp count, const int_tp num,\n                                        const int_tp channels,\n                                        const int_tp spatial_dim,\n                                        __global const Dtype* channel_max,\n                                        __global Dtype* data) {\n  for (int_tp index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    int_tp n = index / channels / spatial_dim;\n    int_tp s = index % spatial_dim;\n    data[index] -= channel_max[n * spatial_dim + s];\n  }\n}\n\n__kernel void TEMPLATE(kernel_exp,Dtype)(const int_tp count, __global const Dtype* data,\n                           __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    out[index] = exp(data[index]);\n  }\n}\n\n__kernel void TEMPLATE(kernel_channel_sum,Dtype)(const int_tp num, const int_tp channels,\n                                   const int_tp spatial_dim,\n                                   __global const Dtype* data,\n                                   __global Dtype* channel_sum) {\n  for (int_tp index = get_global_id(0); index < num * spatial_dim; index +=\n      get_global_size(0)) {\n    int_tp n = index / spatial_dim;\n    int_tp s = index % spatial_dim;\n    Dtype sum = 0;\n    for (int_tp c = 0; c < channels; ++c) {\n      sum += data[(n * channels + c) * spatial_dim + s];\n    }\n    channel_sum[index] = sum;\n  }\n}\n\n__kernel void TEMPLATE(kernel_channel_div,Dtype)(const int_tp count, const int_tp num,\n                                   const int_tp channels, const int_tp spatial_dim,\n                                   __global const Dtype* channel_sum,\n                                   __global Dtype* data) {\n  for (int_tp index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    int_tp n = index / channels / spatial_dim;\n    int_tp s = index % spatial_dim;\n    data[index] /= channel_sum[n * spatial_dim + s];\n  }\n}\n\n__kernel void TEMPLATE(kernel_channel_dot,Dtype)(const int_tp num, const int_tp channels,\n                                   const int_tp spatial_dim,\n                                   __global const Dtype* data_1,\n                                   __global const Dtype* data_2,\n                                   __global Dtype* channel_dot) {\n  for (int_tp index = get_global_id(0); index < num * spatial_dim; index +=\n      get_global_size(0)) {\n    int_tp n = index / spatial_dim;\n    int_tp s = index % spatial_dim;\n    Dtype dot = 0;\n    for (int_tp c = 0; c < channels; ++c) {\n      dot += (data_1[(n * channels + c) * spatial_dim + s]\n          * data_2[(n * channels + c) * spatial_dim + s]);\n    }\n    channel_dot[index] = dot;\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(concat,Dtype)(const int_tp nthreads, __global const Dtype* in_data,\n                                     const int forward, const int_tp num_concats,\n                                     const int_tp concat_size,\n                                     const int_tp top_concat_axis,\n                                     const int_tp bottom_concat_axis,\n                                     const int_tp offset_concat_axis,\n                                     __global Dtype* out_data) {\n\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp total_concat_size = concat_size * bottom_concat_axis;\n    const int_tp concat_num = index / total_concat_size;\n    const int_tp concat_index = index % total_concat_size;\n    const int_tp top_index = concat_index\n        + (concat_num * top_concat_axis + offset_concat_axis) * concat_size;\n    if (forward == 1) {\n      out_data[top_index] = in_data[index];\n    } else {\n      out_data[index] = in_data[top_index];\n    }\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(cll_backward,Dtype)(const int_tp count, const int_tp channels,\n                            const Dtype margin, const Dtype alpha, __global const Dtype* y,\n                            __global const Dtype* diff, __global const Dtype* dist_sq,\n                            __global Dtype *bottom_diff) {\n  for (int_tp i = get_global_id(0); i < count;\n      i += get_global_size(0)) {\n    int_tp n = i / channels;  // the num index, to access y and dist_sq\n    if (trunc(y[n]) != 0.) {  // similar pairs\n      bottom_diff[i] = alpha * diff[i];\n    } else {  // dissimilar pairs\n      Dtype mdist = 0.;\n      Dtype beta = 0.;\n      Dtype dist = sqrt(dist_sq[n]);\n      mdist = (margin - dist);\n      beta = -alpha * mdist / (dist + 1e-4) * diff[i];\n      if (mdist > 0.) {\n        bottom_diff[i] = beta;\n      } else {\n        bottom_diff[i] = 0;\n      }\n    }\n  }\n}\n\n__kernel void TEMPLATE(cll_backward_legacy,Dtype)(const int count, const int channels,\n    const Dtype margin, const Dtype alpha, __global Dtype* y,\n    __global Dtype* diff, __global Dtype* dist_sq,\n    __global Dtype* bottom_diff) {\n    for (int_tp i = get_global_id(0); i < count;\n      i += get_global_size(0)) {\n    int n = i / channels;  // the num index, to access y and dist_sq\n    if (trunc(y[n]) != 0.) {  // similar pairs\n      bottom_diff[i] = alpha * diff[i];\n    } else {  // dissimilar pairs\n      Dtype mdist = 0.;\n      Dtype beta = 0.;\n      mdist = (margin - dist_sq[n]);\n      beta = -alpha;\n      if (mdist > 0.) {\n        bottom_diff[i] = beta;\n      } else {\n        bottom_diff[i] = 0;\n      }\n    }\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(conv_layer_spatial_phony,Dtype)(void) {\n\n}\n\n#define __CAT(x, y) x##y\n#define CAT(x, y) __CAT(x, y)\n#define LOOP1(VAR, STMT) (STMT); (VAR)++;\n#define LOOP2(VAR, STMT) LOOP1(VAR, STMT); (STMT); (VAR)++;\n#define LOOP3(VAR, STMT) LOOP2(VAR, STMT); (STMT); (VAR)++;\n#define LOOP4(VAR, STMT) LOOP3(VAR, STMT); (STMT); (VAR)++;\n#define LOOP5(VAR, STMT) LOOP4(VAR, STMT); (STMT); (VAR)++;\n#define LOOP6(VAR, STMT) LOOP5(VAR, STMT); (STMT); (VAR)++;\n#define LOOP7(VAR, STMT) LOOP6(VAR, STMT); (STMT); (VAR)++;\n#define LOOP8(VAR, STMT) LOOP7(VAR, STMT); (STMT); (VAR)++;\n#define LOOP9(VAR, STMT) LOOP8(VAR, STMT); (STMT); (VAR)++;\n#define LOOP10(VAR, STMT) LOOP9(VAR, STMT); (STMT); (VAR)++;\n#define LOOP11(VAR, STMT) LOOP10(VAR, STMT); (STMT); (VAR)++;\n#define LOOP12(VAR, STMT) LOOP11(VAR, STMT); (STMT); (VAR)++;\n#define LOOP13(VAR, STMT) LOOP12(VAR, STMT); (STMT); (VAR)++;\n#define LOOP14(VAR, STMT) LOOP13(VAR, STMT); (STMT); (VAR)++;\n#define LOOP15(VAR, STMT) LOOP14(VAR, STMT); (STMT); (VAR)++;\n#define LOOP16(VAR, STMT) LOOP15(VAR, STMT); (STMT); (VAR)++;\n#define LOOP(N, VAR, STMT) CAT(LOOP, N)((VAR), (STMT))\n\n#ifdef MULTI\n__kernel void CFMulti(__global Dtype* image_data, int_tp image_offset,\n    __global Dtype* kernel_data, int_tp kernel_offset,\n    __global Dtype* bias,const int_tp bias_offset,\n    __global Dtype* convolved_image,const int_tp convolved_image_offset,\n    const ushort WIDTH,\n    const ushort HEIGHT,\n    const ushort OUTPUT_W,\n    const ushort OUTPUT_H) {\n\n  const int_tp outputX = get_global_id(0);\n  const int_tp outputY = get_global_id(1);\n  const int_tp kernelNum = get_global_id(2)*ZPAR;\n  if(outputX < OUTPUT_W && outputY < OUTPUT_H)\n  {\n    Dtype sum[ZPAR];\n    Dtype4 vectorSum[ZPAR];\n    for(int_tp kern =0; kern < ZPAR; kern++)\n    {\n      sum[kern] = 0.0f;\n      vectorSum[kern] = (0.0f,0.0f,0.0f,0.0f);\n    }\n\n    const int_tp currentKernelOffset = kernel_offset + kernelNum*KERNEL_H*KERNEL_W*CHANNELS;\n    const int_tp biasIndex=bias_offset + kernelNum;\n    const int_tp local_image_offset = outputY*STRIDE_H*WIDTH + outputX*STRIDE_W;\n    const int_tp imageSize = WIDTH*HEIGHT;\n    const int_tp float4Reads = KERNEL_W / 4;\n    const int_tp floatReads = KERNEL_W % 4;\n    Dtype4 imageCache;\n\n    __global Dtype* image_dataPtrFloat = (image_data + (image_offset + local_image_offset));\n    __global Dtype* kernel_dataPtrFloat = (kernel_data + (currentKernelOffset));\n\n    for(int_tp c = 0; c < CHANNELS; c++)\n    {\n      for(int_tp y = 0; y < KERNEL_H; y++)\n      {\n\n        for(int_tp x=0; x< float4Reads; x++)\n        {\n          imageCache = ((__global Dtype4*)image_dataPtrFloat)[x];\n          for(int_tp kern =0; kern < ZPAR; kern++)\n          {\n            vectorSum[kern] += imageCache*((__global Dtype4*)&(kernel_dataPtrFloat[kern*KERNEL_H*KERNEL_W*CHANNELS]))[x];\n          }\n        }\n\n        if(floatReads == 1)\n        {\n          imageCache = ((__global Dtype4*)image_dataPtrFloat)[float4Reads];\n          for(int_tp kern =0; kern < ZPAR; kern++)\n          vectorSum[kern].s0 += ( imageCache * ( (__global Dtype4*) &(kernel_dataPtrFloat[kern*KERNEL_H*KERNEL_W*CHANNELS]) )[float4Reads] ).s0;\n        }\n        else if(floatReads == 2)\n        {\n          imageCache = ((__global Dtype4*)image_dataPtrFloat)[float4Reads];\n          for(int_tp kern =0; kern < ZPAR; kern++)\n          vectorSum[kern].s01 += (imageCache*((__global Dtype4*)&(kernel_dataPtrFloat[kern*KERNEL_H*KERNEL_W*CHANNELS]))[float4Reads]).s01;\n        }\n        else if(floatReads == 3)\n        {\n          imageCache = ((__global Dtype4*)image_dataPtrFloat)[float4Reads];\n          for(int_tp kern =0; kern < ZPAR; kern++)\n          vectorSum[kern].s012 += (imageCache*((__global Dtype4*)&(kernel_dataPtrFloat[kern*KERNEL_H*KERNEL_W*CHANNELS]))[float4Reads]).s012;\n        }\n\n        image_dataPtrFloat += WIDTH;\n        kernel_dataPtrFloat += KERNEL_W;\n      }\n      image_dataPtrFloat += imageSize - WIDTH*KERNEL_H;\n    }\n    for(int_tp kern =0; kern < ZPAR; kern++)\n    sum[kern] = vectorSum[kern].x + vectorSum[kern].y + vectorSum[kern].z + vectorSum[kern].w;\n\n    if(APPLY_BIAS == 1)\n    {\n      for(int_tp kern = 0; kern < ZPAR; kern++)\n      if(kernelNum+kern < OUTPUT_Z)\n      convolved_image[convolved_image_offset + (kernelNum+kern)*OUTPUT_H*OUTPUT_W + outputY*OUTPUT_W + outputX] =\n      sum[kern] + bias[biasIndex +kern];\n    }\n    else\n    for(int_tp kern = 0; kern < ZPAR; kern++)\n    if(kernelNum+kern < OUTPUT_Z)\n    convolved_image[convolved_image_offset + (kernelNum+kern)*OUTPUT_H*OUTPUT_W + outputY*OUTPUT_W + outputX] = sum[kern];\n  }\n}\n\n#endif\n\n\n#ifdef MULTI_11\n__kernel void CFMulti_11_11_4(__global Dtype* image_data, int_tp image_offset,\n    __global Dtype* kernel_data, int_tp kernel_offset,\n    __global Dtype* bias,const int_tp bias_offset,\n    __global Dtype* convolved_image,const int_tp convolved_image_offset,\n    const ushort WIDTH,\n    const ushort HEIGHT,\n    const ushort OUTPUT_W,\n    const ushort OUTPUT_H) {\n\n  int_tp outputX = get_global_id(0)*XPAR;\n  int_tp outputY = get_global_id(1)*YPAR;\n  int_tp kernelNum = get_global_id(2)*ZPAR;\n  if(outputX < OUTPUT_W && outputY < OUTPUT_H)\n  {\n    Dtype sum[XPAR*YPAR*ZPAR];\n    for(int_tp kern =0; kern < XPAR*YPAR*ZPAR; kern++)\n    {\n      sum[kern] = 0.0f;\n    }\n\n    int_tp currentKernelOffset = kernel_offset + kernelNum*KERNELSIZE*CHANNELS;\n    int_tp biasIndex=bias_offset + kernelNum;\n    int_tp local_image_offset = outputY*STRIDE_H*WIDTH + outputX*STRIDE_W;\n    int_tp imageSize = WIDTH*HEIGHT;\n    int_tp index;\n\n    __global Dtype* image_dataPtrFloat = (image_data + (image_offset + local_image_offset));\n    __global Dtype* kernel_dataPtrFloat = (kernel_data + (currentKernelOffset));\n\n    Dtype16 imageCache;\n    Dtype8 imageCacheR;\n    Dtype8 kernelCache;\n    Dtype4 kernelCacheR;\n\n    for(int_tp c = 0; c < CHANNELS; c++)\n    {\n      for(int_tp y = 0; y < 11; y++)\n      {\n        imageCache = ((__global Dtype16*)image_dataPtrFloat)[0];\n        imageCacheR =((__global Dtype8*)image_dataPtrFloat)[2];\n\n        for(int_tp kern =0; kern < ZPAR; kern++)\n        {\n          kernelCache = ((__global Dtype8*)&(kernel_dataPtrFloat[kern*KERNELSIZE*CHANNELS]))[0];\n          kernelCacheR = ((__global Dtype4*)&(kernel_dataPtrFloat[kern*KERNELSIZE*CHANNELS]))[2];\n\n          index = kern*XPAR;\n          sum[index + 0] += dot(imageCache.S0123,kernelCache.S0123);\n          sum[index + 1] += dot(imageCache.S4567,kernelCache.S0123);\n          sum[index + 2] += dot(imageCache.S89AB,kernelCache.S0123);\n          sum[index + 3] += dot(imageCache.SCDEF,kernelCache.S0123);\n\n          sum[index + 0] += dot(imageCache.S4567,kernelCache.S4567);\n          sum[index + 1] += dot(imageCache.S89AB,kernelCache.S4567);\n          sum[index + 2] += dot(imageCache.SCDEF,kernelCache.S4567);\n          sum[index + 3] += dot(imageCacheR.S0123,kernelCache.S4567);\n\n          sum[index + 0] += dot(imageCache.S89A,kernelCacheR.S012);\n          sum[index + 1] += dot(imageCache.SCDE,kernelCacheR.S012);\n          sum[index + 2] += dot(imageCacheR.S012,kernelCacheR.S012);\n          sum[index + 3] += dot(imageCacheR.S456,kernelCacheR.S012);\n        }\n\n        image_dataPtrFloat += WIDTH;\n        kernel_dataPtrFloat += KERNEL_W;\n      }\n      image_dataPtrFloat += imageSize - WIDTH*KERNEL_H;\n    }\n\n    if(APPLY_BIAS == 1)\n    {\n      for(int_tp kern = 0; kern < ZPAR; kern++)\n      {\n        for(int_tp wi =0; wi < XPAR; wi++)\n        if(kernelNum+kern < OUTPUT_Z && outputX + wi < OUTPUT_W)\n        convolved_image[convolved_image_offset + (kernelNum+kern)*OUTPUT_H*OUTPUT_W + outputY*OUTPUT_W + outputX + wi] =\n        sum[kern*XPAR + wi] + bias[biasIndex +kern];\n      }\n    }\n    else\n    for(int_tp kern = 0; kern < ZPAR; kern++)\n    for(int_tp wi =0; wi < XPAR; wi++)\n    if(kernelNum+kern < OUTPUT_Z && outputX + wi < OUTPUT_W)\n    convolved_image[convolved_image_offset + (kernelNum+kern)*OUTPUT_H*OUTPUT_W + outputY*OUTPUT_W + outputX + wi] = sum[kern*XPAR + wi];\n  }\n}\n\n#endif\n\n#ifdef MULTI_GEN\n__kernel void CFMulti_6(__global const Dtype* restrict image_data, const int_tp image_offset,\n    __global const Dtype* restrict kernel_data, const int_tp kernel_offset,\n    __global const Dtype* restrict bias,const int_tp bias_offset,\n    __global Dtype* restrict convolved_image,const int_tp convolved_image_offset,\n    const ushort WIDTH,\n    const ushort HEIGHT,\n    const ushort OUTPUT_W,\n    const ushort OUTPUT_H) {\n\n  const int_tp outputX = get_global_id(0)*XPAR;\n  const int_tp outputY = get_global_id(1)*YPAR;\n  const int_tp kernelNum = get_global_id(2)*ZPAR;\n\n  if(outputX < OUTPUT_W && outputY < OUTPUT_H)\n  {\n    Dtype sum[XPAR*YPAR*ZPAR];\n    for(uint_tp kern = 0; kern < XPAR*YPAR*ZPAR; kern++)\n    sum[kern] = 0.0f;\n\n    const int_tp currentKernelOffset = kernel_offset + kernelNum*KERNELSIZE*CHANNELS;\n    const int_tp biasIndex=bias_offset + kernelNum;\n    const int_tp local_image_offset = outputY*STRIDE_H*WIDTH + outputX*STRIDE_W;\n    const int_tp imageSize = WIDTH*HEIGHT;\n    int_tp index;\n\n    __global const Dtype* image_dataPtrFloat[2];\n    image_dataPtrFloat[0] = (image_data + (image_offset + local_image_offset));\n    image_dataPtrFloat[1] = image_dataPtrFloat[0];\n    __global const Dtype* kernel_dataPtrFloat = (kernel_data + (currentKernelOffset));\n\n    DTImage imageCache[YPAR];\n    DTKernel kernelCache;\n    Dtype4 temp;\n\n    for(uint_tp c = 0; c < CHANNELS; c++)\n    {\n      imageCache[0] = ((__global DTImage*)image_dataPtrFloat[1])[0];\n      for(uint_tp preload = 1; preload < YPAR; preload++)\n      {\n        image_dataPtrFloat[1] += WIDTH;\n        imageCache[preload] = ((__global DTImage*)image_dataPtrFloat[1])[0];\n      }\n\n      int_tp y =0;\n      LOOP(KERNEL_H, y,\n          {\n            int_tp kern=0;\n            LOOP(ZPAR, kern,\n                {\n                  kernelCache = ((__global DTKernel*)&(kernel_dataPtrFloat[kern*KERNELSIZE*CHANNELS]))[0];\n                  index = kern*XPAR*YPAR;\n\n                  for(uint_tp y_par = 0; y_par < YPAR; y_par++)\n                  {\n                    temp = floatDotV4(imageCache[y_par],kernelCache);\n                    sum[index + y_par*XPAR + 0] += temp.s0;\n                    sum[index + y_par*XPAR + 1] += temp.s1;\n                    sum[index + y_par*XPAR + 2] += temp.s2;\n                    sum[index + y_par*XPAR + 3] += temp.s3;\n                  }\n                });\n\n            kernel_dataPtrFloat += KERNEL_W;\n\n            for(uint_tp rotateData = 0; rotateData < YPAR - 1; rotateData++)\n            imageCache[rotateData] = imageCache[rotateData + 1];\n\n            image_dataPtrFloat[1] += WIDTH;\n            imageCache[YPAR - 1] = ((__global DTImage*)image_dataPtrFloat[1])[0];\n          });\n\n      image_dataPtrFloat[0] += imageSize;\n      image_dataPtrFloat[1] = image_dataPtrFloat[0];\n    }\n\n    if(APPLY_BIAS == 1)\n    {\n      for(uint_tp kern = 0; kern < ZPAR; kern++)\n      {\n        for(uint_tp hi =0; hi < YPAR; hi++)\n        for(uint_tp wi =0; wi < XPAR; wi++)\n        if(kernelNum+kern < OUTPUT_Z && outputX + wi < OUTPUT_W && outputY + hi < OUTPUT_H)\n        convolved_image[convolved_image_offset + (kernelNum+kern)*OUTPUT_H*OUTPUT_W + (outputY +hi)*OUTPUT_W + outputX + wi] =\n        sum[kern*XPAR*YPAR + XPAR*hi + wi] + bias[biasIndex +kern];\n      }\n    }\n    else\n    for(uint_tp kern = 0; kern < ZPAR; kern++)\n    for(uint_tp hi =0; hi < YPAR; hi++)\n    for(uint_tp wi =0; wi < XPAR; wi++)\n    if(kernelNum+kern < OUTPUT_Z && outputX + wi < OUTPUT_W && outputY + hi < OUTPUT_H)\n    convolved_image[convolved_image_offset + (kernelNum+kern)*OUTPUT_H*OUTPUT_W + (outputY + hi)*OUTPUT_W + outputX + wi] = sum[kern*XPAR*YPAR +XPAR*hi +wi];\n  }\n}\n#endif\n\n//Begin IDLF kernels below here\n#ifdef IDLF\n\n#define activation_function(x) (x)\n\n#if 0\n#define _IW INPUT_WIDTH\n#define _IH INPUT_HEIGHT\n#define _OW OUTPUT_WIDTH\n#define _OH OUTPUT_HEIGHT\n#endif\n\n#define _ID INPUT_DEPTH\n#define _OD NUM_FILTERS\n\n#define FILTER_DEPTH INPUT_DEPTH\n#define NUM_INPUT INPUT_DEPTH\n#define NUM_OUTPUT NUM_FILTERS\n\n#define KERNEL FILTER_WIDTH\n// convolution stride, same for x and y\n#define K_STRIDE STRIDEX\n\n#ifndef IWPAD\n#define IWPAD 0\n#endif\n\n#ifndef IHPAD\n#define IHPAD 0\n#endif\n\n#define OUT_BLOCK_SIZE (OUT_BLOCK_WIDTH*OUT_BLOCK_HEIGHT)\n\n#ifndef MASTER_OUT_BLOCK_WIDTH\n#define MASTER_OUT_BLOCK_WIDTH OUT_BLOCK_WIDTH\n#endif\n#ifndef MASTER_OUT_BLOCK_HEIGHT\n#define MASTER_OUT_BLOCK_HEIGHT OUT_BLOCK_HEIGHT\n#endif\n\n// Each work-item computes a 4x6 region of one output map.\n// Each work-group (which will be mapped to 1 SIMD16 EU thread) will compute 16 different feature maps, but each feature map is for the same 4x6 region of the imput image.\n// NDRange:  (_OW+pad)/ OUT_BLOCK_WIDTH, (_OH+pad)/OUT_BLOCK_HEIGHT, _OD/OUT_BLOCK_DEPTH\n\n//#define SIMD_SIZE 16\n// NOTE: this reqd_work_group_size does not guarantee that SIMD16 mode will be used, the compiler could choose to use two SIMD8 threads, and if that happens the code will break.\n#ifdef SIMD16\n\n\n#define TILE_X ((OUT_BLOCK_WIDTH - 1) * STRIDEX + KERNEL)\n#define TILE_Y ((OUT_BLOCK_HEIGHT - 1) * STRIDEY + KERNEL)\n\n#if (TILE_X % 4) != 0\n__attribute__((reqd_work_group_size(1, 1, SIMD_SIZE)))\nkernel void\nconvolve_simd16(  // __global float *inputs, __global float* weights, __global float* outputs\n    __global float* inputs_base,\n    filter_qualifier float* weights_base,\n    __global float* biases_base,\n    __global float* outputs_base,\n    const ushort _IW,\n    const ushort _IH,\n    const ushort _OW,\n    const ushort _OH)\n{\n  __global float* outputs = outputs_base;\n  __global float* inputs = inputs_base;\n  filter_qualifier float* weights = weights_base;\n  __global float* biases = biases_base;\n\n  uint_tp oc = get_global_id(0) * MASTER_OUT_BLOCK_WIDTH;  // oc = Output Column\n  uint_tp or = get_global_id(1) * MASTER_OUT_BLOCK_HEIGHT;// or = Output Row\n  uint_tp fm = get_global_id(2);// fm = Feature Map = od = Output Depth\n  uint_tp fmg = get_group_id(2);\n  uint_tp lid = get_local_id(2);\n\n  float in[IN_BUFFER_SIZE];// load 11x16 block of input data, really only need 11x15 for 4x6 outputs, but keep it simple.\n  //float out[24]; // 4x6 block of outputs that is SIMD_SIZE deep (along the Feature Map dimension).\n  float out[OUT_BLOCK_SIZE];\n\n  uint_tp in_addr;\n\n  // find weights adress of given neuron (lid is index)\n  uint_tp weight_addr = (fmg % (_OD/SIMD_SIZE)) * INPUT_DEPTH * KERNEL * KERNEL * SIMD_SIZE + lid;\n\n  for(int_tp i=0;i<OUT_BLOCK_SIZE;i++) {\n    out[i]=0.0f;\n  }\n\n  uint_tp num_in_batch = fm / _OD;\n\n  uint_tp input_batch_offset = num_in_batch * (_IH + IHPAD) * (_IW + IWPAD) * TOTAL_INPUT_DEPTH_SIZE;\n  for(int_tp kd = 0; kd < _ID; kd++)\n  {\n    in_addr = input_batch_offset + (kd + INPUT_START_Z) * (_IH + IHPAD) * (_IW + IWPAD) + (or*K_STRIDE + INPUT_START_Y) * (_IW + IWPAD) + (oc*K_STRIDE + INPUT_START_X) + lid;\n\n    // read 11x16 input block into registers.\n    for(uint_tp reg = 0; reg < IN_BUFFER_SIZE; reg++) {\n      in[reg] = inputs[in_addr];    // read 16 elements\n      in_addr += (_IW + IWPAD);// move to next row down\n    }\n\n// PREF could be 4 or 8, could not be other values.\n#define WEIGHT_PREF 8\n    union {\n      float w[WEIGHT_PREF];\n      uint8 ui8;\n    } weight_buf;\n    int_tp w_idx=0;\n\n    weight_buf.ui8 = intel_sub_group_block_read8((__global uint *)&weights[weight_addr]);\n    uint_tp orig_weight_addr = weight_addr;\n    weight_addr += SIMD_SIZE * WEIGHT_PREF;\n\n    int_tp kr = 0;  // kr = Kernel Row\n    LOOP(KERNEL, kr,// LOOP is a macro that unrolls the loop.\n        {\n          int_tp kc = 0;  // kc = Kernel Column\n          LOOP(KERNEL, kc,\n              {\n                for(int_tp br=0; br < OUT_BLOCK_HEIGHT; br++) {\n                  for(int_tp bc=0; bc < OUT_BLOCK_WIDTH; bc++) {\n                    float input = intel_sub_group_shuffle( in[br * K_STRIDE + kr], bc * K_STRIDE + kc);\n                    out[br * OUT_BLOCK_WIDTH + bc] = mad(weight_buf.w[w_idx % WEIGHT_PREF], input, out[br * OUT_BLOCK_WIDTH + bc]);\n                  }\n                }\n                // We assume KERNEL_W is equal to KERNEL_H here.\n                if ((w_idx + 1) % WEIGHT_PREF == 0\n                    #if KERNEL*KERNEL % 8 != 0\n                    && ((w_idx + 1) <= (KERNEL * KERNEL - WEIGHT_PREF))\n                    #endif\n                    ) {\n                  weight_buf.ui8 = intel_sub_group_block_read8((__global uint *)&weights[weight_addr]);\n                  weight_addr += SIMD_SIZE * WEIGHT_PREF;  // weights must be stored in just the right SIMD swizzled format for this to work, see host code for details.\n                }\n              #if KERNEL*KERNEL % 8 == 0\n                // need to do nothing\n              #else\n                else if ((w_idx + 1) %  WEIGHT_PREF == 0 && ((w_idx + 1) > (KERNEL * KERNEL - WEIGHT_PREF)))\n                #if KERNEL*KERNEL % 8 == 1\n                  weight_buf.w[0] = weights[weight_addr];\n                #elif KERNEL*KERNEL % 4 == 0\n                  weight_buf.ui8.s0123 = intel_sub_group_block_read4((__global uint *)&weights[weight_addr]);\n                #else\n                // should never be here if kernel_w equal to kernel_h. just in case.\n                #error unsupported kernel size.\n                #endif\n              #endif\n                ++w_idx;\n              });\n        });\n    weight_addr = orig_weight_addr + KERNEL * KERNEL * SIMD_SIZE;\n\n  }\n\n#ifdef IMAGE_AS_OUTPUT\n  // TODO: no ULT for that one yet!\n  uint_tp out_addr = ( num_in_batch * TOTAL_OUTPUT_DEPTH + (fm % _OD)) * (_OW + OWPAD) * (_OH + OHPAD);// out_addr indexes into start of 16 feature maps.\n#else\n  // we need this address calculation for outputs because we support views and batching\n  uint_tp out_addr = OUT_BUFF_OFFSET + ( num_in_batch * TOTAL_OUTPUT_DEPTH + (fm % _OD) ) * (_OW + OWPAD) * (_OH + OHPAD);\n#endif\n\n  out_addr += or * (_OW + OWPAD) + oc;  // offset for the 4x3 block that this workitem is working on;\n\n  // we need this address calculation for biases because we support views and batching\n  float bias = biases[(fm) % _OD ];\n#ifndef WRITE_PADDED_VALUES\n  if(get_global_id(0) != (get_global_size(0)-1) &&\n      get_global_id(1) != (get_global_size(1)-1) )\n  {\n#endif\n    for(uint_tp r = 0; r < OUT_BLOCK_HEIGHT; r++) {\n      for(uint_tp c = 0; c < OUT_BLOCK_WIDTH; c++) {\n        // this does a scattered write to 16 different feature maps, so that data within one map is contiguous, thus ready for input to next layer.\n#ifdef IMAGE_AS_OUTPUT\n        write_imagef(outputs,(int2)(out_addr + r * (_OW + OWPAD) + c,num_in_batch),activation_function(bias + out[r * OUT_BLOCK_WIDTH + c]));\n#else\n        outputs[out_addr + r * (_OW + OWPAD) + c] = activation_function(bias + out[r * OUT_BLOCK_WIDTH + c]);\n#endif\n      }\n    }\n#ifndef WRITE_PADDED_VALUES\n  } else if ( get_global_id(1) != (get_global_size(1)-1) )\n  {\n    for(uint_tp r = 0; r < OUT_BLOCK_HEIGHT; r++) {\n      for(uint_tp c = 0; c < LAST_BLOCK_WIDTH; c++) {\n#ifdef IMAGE_AS_OUTPUT\n        write_imagef(outputs,(int2)(out_addr + r * (_OW + OWPAD) + c,num_in_batch),activation_function(bias + out[r * OUT_BLOCK_WIDTH + c]));\n#else\n        outputs[out_addr + r * (_OW + OWPAD) + c] = activation_function(bias + out[r * OUT_BLOCK_WIDTH + c]);\n#endif\n      }\n    }\n  }\n  else if ( get_global_id(0) != (get_global_size(0)-1) )\n  {\n    for(uint_tp r = 0; r < LAST_BLOCK_HEIGHT; r++) {\n      for(uint_tp c = 0; c < OUT_BLOCK_WIDTH; c++) {\n#ifdef IMAGE_AS_OUTPUT\n        write_imagef(outputs,(int2)(out_addr + r * (_OW + OWPAD) + c,num_in_batch),activation_function(bias + out[r * OUT_BLOCK_WIDTH + c]));\n#else\n        outputs[out_addr + r * (_OW + OWPAD) + c] = activation_function(bias + out[r * OUT_BLOCK_WIDTH + c]);\n#endif\n      }\n    }\n  }\n  else\n  {\n    for(uint_tp r = 0; r < LAST_BLOCK_HEIGHT; r++) {\n      for(uint_tp c = 0; c < LAST_BLOCK_WIDTH; c++) {\n#ifdef IMAGE_AS_OUTPUT\n        write_imagef(outputs,(int2)(c,r*(_OW + OWPAD)),activation_function(bias + out[r * OUT_BLOCK_WIDTH + c]));\n#else\n        outputs[out_addr + r * (_OW + OWPAD) + c] = activation_function(bias + out[r * OUT_BLOCK_WIDTH + c]);\n#endif\n      }\n    }\n  }\n#endif //#ifndef WRITE_PADDED_VALUES\n}\n#endif\n\n#if TILE_X % 4 == 0\n#define TILE_Y_STRIDE (64 / TILE_X)\n#define INVEC_NUM ((TILE_Y + TILE_Y_STRIDE - 1) / TILE_Y_STRIDE)\n__attribute__((reqd_work_group_size(1, 1, SIMD_SIZE)))\nkernel void\nconvolve_simd16(  // __global float *inputs, __global float* weights, __global float* outputs\n    __global float* inputs_base,\n    filter_qualifier float* weights_base,\n    __global float* biases_base,\n    __global float* outputs_base,\n    const ushort _IW,\n    const ushort _IH,\n    const ushort _OW,\n    const ushort _OH)\n{\n  __global float* outputs = outputs_base;\n  __global float* inputs = inputs_base;\n  filter_qualifier float* weights = weights_base;\n  __global float* biases = biases_base;\n\n  uint_tp oc = get_global_id(0) * MASTER_OUT_BLOCK_WIDTH;  // oc = Output Column\n  uint_tp or = get_global_id(1) * MASTER_OUT_BLOCK_HEIGHT;// or = Output Row\n  uint_tp fm = get_global_id(2);// fm = Feature Map = od = Output Depth\n  uint_tp fmg = get_group_id(2);\n  uint_tp lid = get_local_id(2);\n\n  float out[OUT_BLOCK_SIZE];\n\n  uint_tp in_addr;\n\n  // find weights adress of given neuron (lid is index)\n  uint_tp weight_addr = (fmg % (_OD/SIMD_SIZE)) * INPUT_DEPTH * KERNEL * KERNEL * SIMD_SIZE + lid;\n\n  for(int_tp i=0;i<OUT_BLOCK_SIZE;i++) {\n    out[i]=0.0f;\n  }\n\n  uint_tp num_in_batch = ( fm ) / _OD;\n\n  uint_tp input_batch_offset = num_in_batch * (_IH + IHPAD) * (_IW + IWPAD) * TOTAL_INPUT_DEPTH_SIZE;\n\n  in_addr = input_batch_offset + INPUT_START_Z * (_IH + IHPAD) * (_IW + IWPAD) + (or*STRIDEY + INPUT_START_Y) * (_IW + IWPAD) + (oc*STRIDEX + INPUT_START_X)\n            + ( lid / ( TILE_X / 4 ) ) * (_IW + IWPAD) * STRIDEY             // y tile offset\n            + ( lid % ( TILE_X / 4 ) ) * 4 * STRIDEX;                        // x tile offset\n\n  for(int_tp kd = 0; kd < _ID; kd++)\n  {\n    union {\n      float4 in_vec[INVEC_NUM];\n      float in_array[INVEC_NUM * 4];\n    } in_buf;\n    uint_tp in_offset = in_addr;\n    int_tp reg = 0;\n#if INVEC_NUM == 1\n    LOOP(1, reg,\n#elif INVEC_NUM == 2\n    LOOP(2, reg,\n#elif INVEC_NUM == 3\n    LOOP(3, reg,\n#elif INVEC_NUM == 4\n    LOOP(4, reg,\n#else\n    #error too large invec_num.\n#endif\n      {\n        in_buf.in_vec[reg] = *(global float4*)(inputs + in_offset);    // read 16 elements\n        in_offset += (_IW + IWPAD) * TILE_Y_STRIDE;\n      });\n    in_addr += (_IH + IHPAD) * (_IW + IWPAD);\n    \n// PREF could be 4 or 8, could not be other values.\n#define WEIGHT_PREF 8\n    union {\n      float w[WEIGHT_PREF];\n      uint8 ui8;\n    } weight_buf;\n    int_tp w_idx=0;\n\n    weight_buf.ui8 = intel_sub_group_block_read8((__global uint *)&weights[weight_addr]);\n    uint_tp orig_weight_addr = weight_addr;\n    weight_addr += SIMD_SIZE * WEIGHT_PREF;\n\n#define BLOCK_IN(n) sub_group_broadcast( in_buf.in_array[((n)%4) + ((n) / (TILE_Y_STRIDE * TILE_X)) * 4], (((n) % (TILE_Y_STRIDE * TILE_X))/4))\n\n    int_tp kr = 0;  // kr = Kernel Row\n    LOOP(KERNEL, kr,// LOOP is a macro that unrolls the loop.\n        {\n          int_tp kc = 0;  // kc = Kernel Column\n          LOOP(KERNEL, kc,\n              {\n                for(int_tp br=0; br < OUT_BLOCK_HEIGHT; br++) {\n                  for(int_tp bc=0; bc < OUT_BLOCK_WIDTH; bc++) {\n                    float input = BLOCK_IN((br * STRIDEY + kr) * TILE_X + bc * STRIDEX + kc);//intel_sub_group_shuffle( in[br * K_STRIDE + kr], bc * K_STRIDE + kc);\n                    out[br * OUT_BLOCK_WIDTH + bc] = mad(weight_buf.w[w_idx % WEIGHT_PREF], input, out[br * OUT_BLOCK_WIDTH + bc]);\n                  }\n                }\n                // We assume KERNEL_W is equal to KERNEL_H here.\n                if ((w_idx + 1) % WEIGHT_PREF == 0\n                #if KERNEL*KERNEL % 8 != 0\n                && ((w_idx + 1) <= (KERNEL * KERNEL - WEIGHT_PREF))\n                #endif\n                    ) {\n                  weight_buf.ui8 = intel_sub_group_block_read8((__global uint *)&weights[weight_addr]);\n                  weight_addr += SIMD_SIZE * WEIGHT_PREF;  // weights must be stored in just the right SIMD swizzled format for this to work, see host code for details.\n                }\n              #if KERNEL*KERNEL % 8 == 0\n                // need to do nothing\n              #else\n                else if ((w_idx + 1) %  WEIGHT_PREF == 0 && ((w_idx + 1) > (KERNEL * KERNEL - WEIGHT_PREF)))\n                #if KERNEL*KERNEL % 8 == 1\n                  weight_buf.w[0] = weights[weight_addr];\n                #elif KERNEL*KERNEL % 4 == 0\n                  weight_buf.ui8.s0123 = intel_sub_group_block_read4((__global uint *)&weights[weight_addr]);\n                #else\n                // should never be here if kernel_w equal to kernel_h. just in case.\n                #error unsupported kernel size.\n                #endif\n              #endif\n                ++w_idx;\n              });\n        });\n    weight_addr = orig_weight_addr + KERNEL * KERNEL * SIMD_SIZE;\n\n  }\n\n  // we need this address calculation for outputs because we support views and batching\n  uint_tp out_addr = OUT_BUFF_OFFSET + ( num_in_batch * TOTAL_OUTPUT_DEPTH + (fm % _OD) ) * (_OW + OWPAD) * (_OH + OHPAD);\n\n  out_addr += or * (_OW + OWPAD) + oc;  // offset for the 4x3 block that this workitem is working on;\n\n  // we need this address calculation for biases because we support views and batching\n  float bias = biases[(fm) % _OD ];\n#ifndef WRITE_PADDED_VALUES\n  if(get_global_id(0) != (get_global_size(0)-1) &&\n      get_global_id(1) != (get_global_size(1)-1) )\n  {\n#endif\n    for(uint_tp r = 0; r < OUT_BLOCK_HEIGHT; r++) {\n      for(uint_tp c = 0; c < OUT_BLOCK_WIDTH; c++) {\n        // this does a scattered write to 16 different feature maps, so that data within one map is contiguous, thus ready for input to next layer.\n        outputs[out_addr + r * (_OW + OWPAD) + c] = activation_function(bias + out[r * OUT_BLOCK_WIDTH + c]);\n      }\n    }\n#ifndef WRITE_PADDED_VALUES\n  } else if ( get_global_id(1) != (get_global_size(1)-1) )\n  {\n    for(uint_tp r = 0; r < OUT_BLOCK_HEIGHT; r++) {\n      for(uint_tp c = 0; c < LAST_BLOCK_WIDTH; c++) {\n        outputs[out_addr + r * (_OW + OWPAD) + c] = activation_function(bias + out[r * OUT_BLOCK_WIDTH + c]);\n      }\n    }\n  }\n  else if ( get_global_id(0) != (get_global_size(0)-1) )\n  {\n    for(uint_tp r = 0; r < LAST_BLOCK_HEIGHT; r++) {\n      for(uint_tp c = 0; c < OUT_BLOCK_WIDTH; c++) {\n        outputs[out_addr + r * (_OW + OWPAD) + c] = activation_function(bias + out[r * OUT_BLOCK_WIDTH + c]);\n      }\n    }\n  }\n  else\n  {\n    for(uint_tp r = 0; r < LAST_BLOCK_HEIGHT; r++) {\n      for(uint_tp c = 0; c < LAST_BLOCK_WIDTH; c++) {\n        outputs[out_addr + r * (_OW + OWPAD) + c] = activation_function(bias + out[r * OUT_BLOCK_WIDTH + c]);\n      }\n    }\n  }\n#endif //#ifndef WRITE_PADDED_VALUES\n}\n#endif // Stride > 2\n#endif\n\n#endif",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(copyImage, Dtype)\n    (__global Dtype* image_data,\n     int_tp image_offset,\n     const int_tp channels, const int_tp height, const int_tp width,\n     const int_tp adjustedHeight, const int_tp adjustedWidth,\n     const int_tp pad_h, const int_tp pad_w,\n     __global Dtype* output_image,\n     const int_tp output_offset,\n     const int_tp batch_size) {\n\n  uint_tp sX = get_global_id(0);\n  uint_tp sY = get_global_id(1);\n  uint_tp sZ = get_global_id(2);\n\n  int_tp in_y = sY - pad_h;\n  int_tp in_x = sX - pad_w;\n\n  int_tp batch_offset = 0;\n  int_tp adjusted_batch_offset = 0;\n  for(uint_tp batch_idx = 0; batch_idx < batch_size; batch_idx++) {\n    int_tp dst_offset = adjusted_batch_offset + output_offset + sZ*adjustedHeight*adjustedWidth + sY*adjustedWidth +sX;\n    int_tp src_offset = batch_offset + image_offset + sZ*height*width + in_y*width + in_x;\n    if((in_y >= 0 && in_y < height && in_x >= 0 && in_x < width))\n      output_image[dst_offset] = image_data[src_offset];\n    else\n      output_image[dst_offset] = 0;\n    batch_offset += height * width * channels;\n    adjusted_batch_offset += adjustedHeight * adjustedWidth * channels;\n  }\n}\n\n__kernel void TEMPLATE(copyWeightsSwizzled, Dtype)\n    (__global Dtype* weightIn,\n     __global Dtype* weightOut,\n     const int_tp kernel_w,\n     const int_tp kernel_h,\n     const int_tp channels,\n     const int_tp outputs,\n     const int_tp swizzleFactor) {\n\n  uint_tp sX = get_global_id(0);\n\n  //Original location\n\n  //Output location\n  int_tp outputSublayer = channels / swizzleFactor;\n  int_tp outputSublayerIndex = channels % swizzleFactor;\n\n  int_tp filter = sX / (kernel_w*kernel_h*channels);\n  int_tp kernel_X = sX % kernel_w;\n  int_tp kernel_Y = (sX / kernel_w) % kernel_h;\n  int_tp kernel_C = (sX / (kernel_w * kernel_h)) % channels;\n\n  int_tp FP = filter / swizzleFactor;\n  int_tp F1 = filter % swizzleFactor;\n\n  weightOut[FP*(kernel_w*kernel_h*channels*swizzleFactor) + kernel_C*(kernel_w*kernel_h*swizzleFactor) + kernel_Y*(kernel_w*swizzleFactor) + kernel_X*swizzleFactor + F1]\n  = weightIn[filter*(kernel_w*kernel_h*channels) + kernel_C*(kernel_w*kernel_h) + kernel_Y*kernel_w + kernel_X];\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(crop_copy, Dtype)(const int_tp n, const int_tp height,\n                                         const int_tp width,\n                                         const int_tp src_outer_stride,\n                                         const int_tp src_inner_stride,\n                                         const int_tp dest_outer_stride,\n                                         const int_tp dest_inner_stride,\n                                         __global const Dtype* src,\n                                         const int_tp src_off,\n                                         __global Dtype* dest,\n                                         const int_tp dest_off) {\n  for (int_tp index = get_global_id(0); index < n;\n      index += get_global_size(0)) {\n    int_tp src_start = index / height * src_outer_stride\n        + index % height * src_inner_stride;\n    int_tp dest_start = index / height * dest_outer_stride\n        + index % height * dest_inner_stride;\n    for (int_tp i = 0; i < width; ++i) {\n      dest[dest_off + dest_start + i] = src[src_off + src_start + i];\n    }\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(dropout_forward,Dtype)(const int_tp n,\n                                              __global const Dtype* in,\n                                              __global const uint_tp* mask,\n                                              const uint_tp threshold,\n                                              const Dtype scale,\n                                              __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] = in[index] * ((mask[index] > threshold)?1.0:0.0) * scale;\n  }\n}\n\n__kernel void TEMPLATE(dropout_backward,Dtype)(\n    const int_tp n, __global const Dtype* in_diff,\n    __global const uint_tp* mask, const uint_tp threshold,\n    const Dtype scale,\n    __global Dtype* out_diff) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out_diff[index] = in_diff[index] * ((mask[index] > threshold)?1.0:0.0) * scale;\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(eltwise_max_forward,Dtype)(\n    const int_tp nthreads, __global const Dtype* bottom_data_a,\n    __global const Dtype* bottom_data_b, const int_tp blob_idx,\n    __global Dtype* top_data,\n    __global int_tp* mask) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    Dtype maxval = -FLT_MAX;\n    int_tp maxidx = -1;\n    if (bottom_data_a[index] > bottom_data_b[index]) {\n      // only update for very first bottom_data blob (blob_idx == 0)\n      if (blob_idx == 0) {\n        maxval = bottom_data_a[index];\n        top_data[index] = maxval;\n        maxidx = blob_idx;\n        mask[index] = maxidx;\n      }\n    } else {\n      maxval = bottom_data_b[index];\n      top_data[index] = maxval;\n      maxidx = blob_idx + 1;\n      mask[index] = maxidx;\n    }\n  }\n}\n\n__kernel void TEMPLATE(eltwise_max_backward,Dtype)(const int_tp nthreads,\n                                                   __global const Dtype* top_diff,\n                                                   const int_tp blob_idx,\n                                                   __global const int_tp* mask,\n                                                   __global Dtype* bottom_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    Dtype gradient = 0;\n    if (mask[index] == blob_idx) {\n      gradient += top_diff[index];\n    }\n    bottom_diff[index] = gradient;\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(elu_forward,Dtype)(const int n, __global const Dtype* in,\n                                          __global Dtype* out,\n                                          Dtype alpha) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] = in[index] > 0 ? in[index] : alpha * (exp(in[index]) - 1.0);\n  }\n}\n\n__kernel void TEMPLATE(elu_backward,Dtype)(const int n, __global const Dtype* in_diff,\n                                           __global const Dtype* out_data,\n                                           __global const Dtype* in_data,\n                                           __global Dtype* out_diff,\n                                           Dtype alpha) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out_diff[index] =\n        in_data[index] > 0 ?\n            in_diff[index] : in_diff[index] * (out_data[index] + alpha);\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(embed_forward,Dtype)(const int_tp nthreads,\n                                            __global const Dtype* bottom_data,\n                                            __global const Dtype* weight,\n                                            const int_tp M, const int_tp N,\n                                            const int_tp K,\n                                            __global Dtype* top_data) {\n  for (int_tp top_index = get_global_id(0); top_index < nthreads;\n      top_index += get_global_size(0)) {\n      const int_tp n = top_index / N;\n      const int_tp d = top_index % N;\n      const int_tp index = (int_tp)(bottom_data[n]);\n      const int_tp weight_index = index * N + d;\n      top_data[top_index] = weight[weight_index];\n    }\n  }\n\n// atomic_add from: http://suhorukov.blogspot.com/2011/12/opencl-11-atomic-operations-on-floating.html\n#if (TYPE == TYPE_FLOAT)\ninline void TEMPLATE(atomic_add,Dtype)(volatile __global Dtype *source, const Dtype operand) {\n    union {\n        uint_tp intVal;\n        Dtype floatVal;\n    } newVal;\n    union {\n        uint_tp intVal;\n        Dtype floatVal;\n    } prevVal;\n    do {\n        prevVal.floatVal = *source;\n        newVal.floatVal = prevVal.floatVal + operand;\n    } while (atomic_cmpxchg((volatile __global unsigned int *)source, prevVal.intVal, newVal.intVal) != prevVal.intVal);\n}\n\n__kernel void TEMPLATE(embed_backward,Dtype)(const int_tp nthreads, __global const Dtype* bottom_data,\n    __global const Dtype* top_diff, const int_tp M, const int_tp N, const int_tp K,\n    __global Dtype* weight_diff) {\n  for (int_tp top_index = get_global_id(0); top_index < nthreads;\n      top_index += get_global_size(0)) {\n    const int_tp n = top_index / N;\n    const int_tp d = top_index % N;\n    const int_tp index = (int_tp)(bottom_data[n]);\n    const int_tp weight_index = index * N + d;\n\n    TEMPLATE(atomic_add,Dtype)((weight_diff + weight_index), *(top_diff + top_index));\n  }\n}\n#endif\n\n#if (TYPE == TYPE_DOUBLE)\n#ifdef ATOMICS_64_AVAILABLE\ninline void TEMPLATE(atomic_add,Dtype)(volatile __global Dtype *source, const Dtype operand) {\n    union {\n        unsigned long intVal;\n        Dtype floatVal;\n    } newVal;\n    union {\n        unsigned long intVal;\n        Dtype floatVal;\n    } prevVal;\n    do {\n        prevVal.floatVal = *source;\n        newVal.floatVal = prevVal.floatVal + operand;\n    } while (atom_cmpxchg((volatile __global unsigned long *)source, prevVal.intVal, newVal.intVal) != prevVal.intVal);\n}\n\n__kernel void TEMPLATE(embed_backward,Dtype)(const int_tp nthreads, __global const Dtype* bottom_data,\n    __global const Dtype* top_diff, const int_tp M, const int_tp N, const int_tp K,\n    __global Dtype* weight_diff) {\n  for (int_tp top_index = get_global_id(0); top_index < nthreads;\n      top_index += get_global_size(0)) {\n    const int_tp n = top_index / N;\n    const int_tp d = top_index % N;\n    const int_tp index = (int_tp)(bottom_data[n]);\n    const int_tp weight_index = index * N + d;\n\n    TEMPLATE(atomic_add,Dtype)((weight_diff + weight_index), *(top_diff + top_index));\n  }\n}\n#endif\n#endif",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(fft_phony,Dtype)(void) {\n\n}\n\n#ifdef FFT\n#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n#define DtypeComplex Dtype2\n\n__kernel void TEMPLATE(copy2buffer_cyclic_shift_in,Dtype)(\n    __global Dtype* fft_gpu_weights_real, const int_tp offset_fft_gpu_weights_real,\n    __global Dtype* weight, const int_tp offset_weight,\n    const int_tp ker_size, const int_tp ch_gr, const int_tp ker_size_ch_gr,\n    const int_tp ker_w, const int_tp ker_c_h, const int_tp ker_c_w, \n    const int_tp fft_height, const int_tp fft_width, const int_tp complex_w_len) {\n  fft_gpu_weights_real += offset_fft_gpu_weights_real;\n  weight += offset_weight;\n  int_tp gId = get_global_id(0);\n  int_tp out = gId / ker_size_ch_gr;\n  int_tp c = (gId - out * ker_size_ch_gr) / ker_size;\n  int_tp map_offset = out * ch_gr + c;\n  int_tp map_offset_ker_size = map_offset * ker_size;\n  int_tp pos_in_map = gId - map_offset_ker_size;\n  int_tp h = pos_in_map / ker_w;\n  int_tp h_ker_w = h * ker_w;\n  int_tp w = pos_in_map - h_ker_w;\n  int_tp src_idx = map_offset_ker_size + h_ker_w + w;\n  int_tp ky = h - ker_c_h;\n  if (ky < 0) ky += fft_height;\n  int_tp kx = w - ker_c_w;\n  if (kx < 0) kx += fft_width;\n  int_tp dst_idx = (map_offset * fft_height + ky) * complex_w_len + kx;\n  fft_gpu_weights_real[dst_idx] = weight[src_idx];\n}\n\n/* Use when width < 4 */\n__kernel void TEMPLATE(copy2buffer_left_top_in_naive,Dtype)(__global Dtype* map_out,\n    const int_tp offset_map_out, \n    const __global Dtype* map_in, const int_tp offset_map_in, \n    const int_tp size, \n    const int_tp height_out, const int_tp width_out, \n    const int_tp height, const int_tp width, const int_tp stride_h, const int_tp stride_w,\n    const int_tp pad_h, const int_tp pad_w) {\n  map_out += offset_map_out;\n  map_in  += offset_map_in;\n  int_tp gId = get_global_id(0);\n  int_tp h = gId / width;\n  int_tp w = gId - (h * width);\n  int_tp dst_idx = (h*stride_h + pad_h)*width_out + (w*stride_w + pad_w);\n  map_out[dst_idx] = map_in[gId];\n}\n\n/* Use when width < 4 */\n__kernel void TEMPLATE(copy2buffer_left_top_in_naive_2d,Dtype)(__global Dtype* map_out,\n    const int_tp offset_map_out, \n    const __global Dtype* map_in, const int_tp offset_map_in, \n    const int_tp map_out_size, const int_tp size, const int_tp count,\n    const int_tp height_out, const int_tp width_out, \n    const int_tp height, const int_tp width, const int_tp stride_h, const int_tp stride_w,\n    const int_tp pad_h, const int_tp pad_w) {\n  map_out += offset_map_out;\n  map_in  += offset_map_in;\n  int_tp gId_x = get_global_id(0);\n  int_tp gId_y = get_global_id(1); \n  int_tp h = gId_x / width;\n  int_tp w = gId_x - (h * width);\n  int_tp src_idx = gId_y * size + gId_x;\n  int_tp dst_idx = gId_y * map_out_size + \n      (h * stride_h + pad_h) * width_out + (w * stride_w + pad_w);\n  map_out[dst_idx] = map_in[src_idx];\n}\n\n/* Use when width >= 4 */\n__kernel void TEMPLATE(copy2buffer_left_top_in,Dtype)(__global Dtype* map_out,\n    const int_tp offset_map_out,\n    const __global Dtype* map_in, const int_tp offset_map_in,\n    const int_tp size,\n    const int_tp height_out, const int_tp width_out, \n    const int_tp height, const int_tp width, const int_tp stride_h, const int_tp stride_w,\n    const int_tp pad_h, const int_tp pad_w) {\n  map_out += offset_map_out;\n  map_in  += offset_map_in;\n  int_tp gId = get_global_id(0);\n  int_tp count = size >> 2;\n  int_tp gId4 = gId << 2;\n  int_tp h = gId4 / width;\n  int_tp w = gId4 - (h * width);\n  int_tp dst_h = h*stride_h + pad_h;\n  int_tp dst_w = w*stride_w + pad_w;\n  int_tp dst_idx = dst_h*width_out + dst_w;\n  if (gId < count) {\n    Dtype4 map_in_cache4 = vload4(gId, map_in);\n    int_tp has_pad = width - dst_w; \n    if (has_pad >= 4) {\n      vstore4(map_in_cache4, dst_idx >> 2, map_out);\n    } else { \n      if (0 == has_pad) {\n        dst_idx += width_out + pad_w - dst_w;\n      }\n      map_out[dst_idx] = map_in_cache4.x;\n      if (1 == has_pad) {\n        dst_idx += width_out + pad_w - dst_w - 1;\n      }\n      map_out[dst_idx+1] = map_in_cache4.y;\n      if (2 == has_pad) {\n        dst_idx += width_out + pad_w - dst_w - 2;\n      }\n      map_out[dst_idx+2] = map_in_cache4.z;\n      if (3 == has_pad) {\n        dst_idx += width_out + pad_w - dst_w - 3;\n      }\n      map_out[dst_idx+3] = map_in_cache4.w;\n      dst_h += 1;\n      dst_w = pad_w;\n    }\n  } else if (gId == count) {\n    int_tp res = size - (count << 2); /* size % 4 */\n    if (res > 0) {\n      Dtype4 map_in_cache4 = 0.f;\n      if (res >= 1) \n        map_in_cache4.x = map_in[gId4];\n      if (res >= 2)\n        map_in_cache4.y = map_in[gId4+1];\n      if (res == 3)\n        map_in_cache4.z = map_in[gId4+2];\n      int_tp has_pad = width - dst_w; \n      if (has_pad >= 4) {\n        vstore4(map_in_cache4, dst_idx >> 2, map_out);\n      } else { \n        if (0 == has_pad) {\n          dst_idx += width_out + pad_w - dst_w;\n        }\n        map_out[dst_idx] = map_in_cache4.x;\n        if (1 == has_pad) {\n          dst_idx += width_out + pad_w - dst_w - 1;\n        }\n        map_out[dst_idx+1] = map_in_cache4.y;\n        if (2 == has_pad) {\n          dst_idx += width_out + pad_w - dst_w - 2;\n        }\n        map_out[dst_idx+2] = map_in_cache4.z;\n        if (3 == has_pad) {\n          dst_idx += width_out + pad_w - dst_w - 3;\n        }\n        map_out[dst_idx+3] = map_in_cache4.w;\n        dst_h += 1;\n        dst_w = pad_w;\n      }\n    }\n  }\n}\n\n/* Use when width >= 4 */\n__kernel void TEMPLATE(copy2buffer_left_top_in_2d,Dtype)(__global Dtype* map_out,\n    const int_tp offset_map_out,\n    const __global Dtype* map_in, const int_tp offset_map_in,\n    const int_tp map_out_size, const int_tp size, const int_tp count,\n    const int_tp height_out, const int_tp width_out, \n    const int_tp height, const int_tp width, const int_tp stride_h, const int_tp stride_w,\n    const int_tp pad_h, const int_tp pad_w) {\n  map_out += offset_map_out;\n  map_in  += offset_map_in;\n  int_tp gId = get_global_id(0);\n  int_tp gId_y = get_global_id(1);\n  int_tp gId4 = gId << 2;\n  int_tp h = gId4 / width;\n  int_tp w = gId4 - (h * width);\n  int_tp dst_h = h*stride_h + pad_h;\n  int_tp dst_w = w*stride_w + pad_w;\n  int_tp dst_idx = dst_h*width_out + dst_w;\n  const __global Dtype* map_in_2d = map_in + gId_y * size;\n  __global Dtype* map_out_2d = map_out + gId_y * map_out_size;\n  if (gId < count) {\n    Dtype4 map_in_cache4 = vload4(gId, map_in_2d);\n    int_tp has_pad = width - dst_w; \n    if (has_pad >= 4) {\n      vstore4(map_in_cache4, dst_idx >> 2, map_out_2d);\n    } else { \n      if (0 == has_pad) {\n        dst_idx += width_out + pad_w - dst_w;\n      }\n      map_out_2d[dst_idx] = map_in_cache4.x;\n      if (1 == has_pad) {\n        dst_idx += width_out + pad_w - dst_w - 1;\n      }\n      map_out_2d[dst_idx+1] = map_in_cache4.y;\n      if (2 == has_pad) {\n        dst_idx += width_out + pad_w - dst_w - 2;\n      }\n      map_out_2d[dst_idx+2] = map_in_cache4.z;\n      if (3 == has_pad) {\n        dst_idx += width_out + pad_w - dst_w - 3;\n      }\n      map_out_2d[dst_idx+3] = map_in_cache4.w;\n      dst_h += 1;\n      dst_w = pad_w;\n    }\n  } else if (gId == count) {\n    int_tp res = size - (count << 2); /* size % 4 */\n    if (res > 0) {\n      Dtype4 map_in_cache4 = 0.f;\n      if (res >= 1) \n        map_in_cache4.x = map_in_2d[gId4];\n      if (res >= 2)\n        map_in_cache4.y = map_in_2d[gId4+1];\n      if (res == 3)\n        map_in_cache4.z = map_in_2d[gId4+2];\n      int_tp has_pad = width - dst_w; \n      if (has_pad >= 4) {\n        vstore4(map_in_cache4, dst_idx >> 2, map_out_2d);\n      } else { \n        if (0 == has_pad) {\n          dst_idx += width_out + pad_w - dst_w;\n        }\n        map_out_2d[dst_idx] = map_in_cache4.x;\n        if (1 == has_pad) {\n          dst_idx += width_out + pad_w - dst_w - 1;\n        }\n        map_out_2d[dst_idx+1] = map_in_cache4.y;\n        if (2 == has_pad) {\n          dst_idx += width_out + pad_w - dst_w - 2;\n        }\n        map_out_2d[dst_idx+2] = map_in_cache4.z;\n        if (3 == has_pad) {\n          dst_idx += width_out + pad_w - dst_w - 3;\n        }\n        map_out_2d[dst_idx+3] = map_in_cache4.w;\n        dst_h += 1;\n        dst_w = pad_w;\n      }\n    }\n  }\n}\n\n/* Use when width_out < 4 */\n__kernel void TEMPLATE(copy2buffer_left_top_out_naive,Dtype)(__global Dtype* map_out,\n    const int_tp offset_map_out, \n    const __global Dtype* map_in, const int_tp offset_map_in, \n    const int_tp size,\n    const int_tp height_out, const int_tp width_out, \n    const int_tp fft_height, const int_tp fft_width, \n    const int_tp ker_center_h, const int_tp ker_center_w,\n    const int_tp stride_h, const int_tp stride_w, \n    const int_tp pad_h, const int_tp pad_w) {\n  map_out += offset_map_out;\n  map_in += offset_map_in;\n  int_tp gId = get_global_id(0);\n  int_tp h_out = gId / width_out;\n  int_tp w_out = gId - (h_out * width_out);\n  int_tp h = h_out * stride_h + ker_center_h;\n  int_tp w = w_out * stride_w + ker_center_w;\n  int_tp src_idx = h*fft_width + w;\n  map_out[gId] = map_in[src_idx];\n}\n\n/* Use when width_out < 4 */\n__kernel void TEMPLATE(copy2buffer_left_top_out_naive_2d,Dtype)(__global Dtype* map_out,\n    const int_tp offset_map_out, \n    const __global Dtype* map_in, const int_tp offset_map_in,\n    const int_tp size, const int_tp count, const int_tp map_in_size,\n    const int_tp height_out, const int_tp width_out, \n    const int_tp fft_height, const int_tp fft_width, \n    const int_tp ker_center_h, const int_tp ker_center_w,\n    const int_tp stride_h, const int_tp stride_w, \n    const int_tp pad_h, const int_tp pad_w) {\n  map_out += offset_map_out;\n  map_in += offset_map_in;\n  int_tp gId = get_global_id(0);\n  int_tp out = get_global_id(1);\n  int_tp h_out = gId / width_out;\n  int_tp w_out = gId - (h_out * width_out);\n  int_tp h = h_out * stride_h + ker_center_h;\n  int_tp w = w_out * stride_w + ker_center_w;\n  int_tp src_idx = out * map_in_size + h*fft_width + w;\n  int_tp dst_idx = out * size + gId;\n  map_out[dst_idx] = map_in[src_idx];\n}\n\n/* Use when width_out >= 4 */\n__kernel void TEMPLATE(copy2buffer_left_top_out,Dtype)(__global Dtype* map_out,\n    const int_tp offset_map_out,\n    const __global Dtype* map_in, const int_tp offset_map_in, \n    const int_tp size,\n    const int_tp height_out, const int_tp width_out, \n    const int_tp fft_height, const int_tp fft_width, \n    const int_tp ker_c_h, const int_tp ker_c_w,\n    const int_tp stride_h, const int_tp stride_w, const int_tp pad_h, const int_tp pad_w) {\n  map_out += offset_map_out;\n  map_in  += offset_map_in;\n  int_tp gId = get_global_id(0);\n  int_tp count = size >> 2;\n  int_tp gId4 = gId << 2;\n  int_tp h_out = gId4 / width_out;\n  int_tp w_out = gId4 - (h_out * width_out);\n  int_tp h = h_out * stride_h + ker_c_h;\n  int_tp w = w_out * stride_w + ker_c_w;\n  int_tp src_idx = h*fft_width + w;\n  if (gId < count) {\n    Dtype4 map_in_cache4;\n    int_tp has_pad = width_out - (w - pad_w); \n    if (has_pad >= 4) {\n      map_in_cache4 = vload4(src_idx >> 2, map_in);\n    } else {\n      int_tp right_elements = fft_width - width_out;\n      if (0 == has_pad) {\n        src_idx += right_elements;\n      }\n      map_in_cache4.x = map_in[src_idx];\n      if (1 == has_pad) {\n        src_idx += right_elements;\n      }\n      map_in_cache4.y = map_in[src_idx+1];\n      if (2 == has_pad) {\n        src_idx += right_elements;\n      }\n      map_in_cache4.z = map_in[src_idx+2];\n      if (3 == has_pad) {\n        src_idx += right_elements;\n      }\n      map_in_cache4.w = map_in[src_idx+3];\n    }\n    vstore4(map_in_cache4, gId, map_out);\n  } else if (gId == count) {\n    int_tp res = size - (count << 2); /* size % 4 */\n    if (res > 0) {\n      for (int_tp i = gId4; i < size; ++i) {\n        map_out[i] = map_in[src_idx];\n        src_idx++;\n      }\n    }\n  }\n}\n\n/* Use when width_out >= 4 */\n__kernel void TEMPLATE(copy2buffer_left_top_out_2d,Dtype)(__global Dtype* map_out,\n    const int_tp offset_map_out,\n    const __global Dtype* map_in, const int_tp offset_map_in, \n    const int_tp size, const int_tp count, const int_tp map_in_size,\n    const int_tp height_out, const int_tp width_out, \n    const int_tp fft_height, const int_tp fft_width, \n    const int_tp ker_c_h, const int_tp ker_c_w,\n    const int_tp stride_h, const int_tp stride_w, const int_tp pad_h, const int_tp pad_w) {\n  map_out += offset_map_out;\n  map_in  += offset_map_in;\n  int_tp gId = get_global_id(0);\n  int_tp out = get_global_id(1);\n  int_tp gId4 = gId << 2;\n  int_tp h_out = gId4 / width_out;\n  int_tp w_out = gId4 - (h_out * width_out);\n  int_tp h = h_out * stride_h + ker_c_h;\n  int_tp w = w_out * stride_w + ker_c_w;\n  int_tp src_idx = h*fft_width + w;\n  const __global Dtype* map_in_2d = map_in + out * map_in_size;\n  __global Dtype* map_out_2d = map_out + out * size;\n  if (gId < count) {\n    Dtype4 map_in_cache4;\n    int_tp has_pad = width_out - (w - pad_w); \n    if (has_pad >= 4) {\n      map_in_cache4 = vload4(src_idx >> 2, map_in_2d);\n    } else {\n      int_tp right_elements = fft_width - width_out;\n      if (0 == has_pad) {\n        src_idx += right_elements;\n      }\n      map_in_cache4.x = map_in_2d[src_idx];\n      if (1 == has_pad) {\n        src_idx += right_elements;\n      }\n      map_in_cache4.y = map_in_2d[src_idx+1];\n      if (2 == has_pad) {\n        src_idx += right_elements;\n      }\n      map_in_cache4.z = map_in_2d[src_idx+2];\n      if (3 == has_pad) {\n        src_idx += right_elements;\n      }\n      map_in_cache4.w = map_in_2d[src_idx+3];\n    }\n    vstore4(map_in_cache4, gId, map_out_2d);\n  } else if (gId == count) {\n    int_tp res = size - (count << 2); /* size % 4 */\n    if (res > 0) {\n      const __global Dtype4* map_in_2d_4 =\n            (const __global Dtype4*)(map_in_2d + src_idx);\n      __global Dtype4* map_out_2d_4 = (__global Dtype4*)(map_out_2d + gId4);\n      if (res == 3) {\n        map_out_2d_4[0].xyz = map_in_2d_4[0].xyz;\n      } else if (res == 2) {\n        map_out_2d_4[0].xy = map_in_2d_4[0].xy;\n      } else if (res == 1) {\n        map_out_2d_4[0].x = map_in_2d_4[0].x;\n      }\n    }\n  }\n}\n\n__kernel void TEMPLATE(copy2buffer_cyclic_shift_out,Dtype)(__global Dtype* map_out,\n    const int_tp offset_map_out, \n    const __global Dtype* map_in, const int_tp offset_map_in, \n    const int_tp width_out, \n    const int_tp fft_height, const int_tp fft_width, \n    const int_tp ker_center_h, const int_tp ker_center_w,\n    const int_tp stride_h, const int_tp stride_w, \n    const int_tp pad_h, const int_tp pad_w) {\n  map_out += offset_map_out;\n  map_in  += offset_map_in;\n  int_tp gId = get_global_id(0);\n  int_tp h_out = gId / width_out;\n  int_tp w_out = gId - (h_out * width_out);\n  int_tp h = h_out * stride_h + pad_h;\n  int_tp w = w_out * stride_w + pad_w;\n  int_tp ky = h - ker_center_h;\n  if (ky < 0) ky += fft_height;\n  int_tp kx = w - ker_center_w;\n  if (kx < 0) kx += fft_width;\n  int_tp src_idx = ky*fft_width + kx;\n  map_out[gId] = map_in[src_idx];\n}\n\n__kernel void TEMPLATE(copy2buffer_cyclic_shift_out_2d,Dtype)(__global Dtype* map_out,\n    const int_tp offset_map_out, \n    const __global Dtype* map_in, const int_tp offset_map_in,\n    const int_tp map_out_size, const int_tp map_in_size, \n    const int_tp width_out, \n    const int_tp fft_height, const int_tp fft_width, \n    const int_tp ker_center_h, const int_tp ker_center_w,\n    const int_tp stride_h, const int_tp stride_w, \n    const int_tp pad_h, const int_tp pad_w) {\n  map_out += offset_map_out;\n  map_in  += offset_map_in;\n  int_tp gId = get_global_id(0);\n  int_tp gId_y = get_global_id(1);\n  int_tp h_out = gId / width_out;\n  int_tp w_out = gId - (h_out * width_out);\n  int_tp h = h_out * stride_h + pad_h;\n  int_tp w = w_out * stride_w + pad_w;\n  int_tp ky = h - ker_center_h;\n  if (ky < 0) ky += fft_height;\n  int_tp kx = w - ker_center_w;\n  if (kx < 0) kx += fft_width;\n  int_tp src_idx = gId_y * map_in_size + ky*fft_width + kx;\n  int_tp dst_idx = gId_y * map_out_size + gId;\n  map_out[dst_idx] = map_in[src_idx];\n}\n\n__kernel void TEMPLATE(complex_conjugate_multiplication_1d,Dtype)(__global Dtype* dst,\n    const int_tp offset_dst, \n    const __global Dtype* src1, const int_tp offset_src1,\n    const __global Dtype* src2, const int_tp offset_src2, \n    const int_tp ch_gr) {\n  dst += offset_dst;\n  src1 += offset_src1;\n  src2 += offset_src2;\n  int_tp gId = get_global_id(0); \n  int_tp size = get_global_size(0);\n  Dtype4 dst_cache = 0.f;\n  int_tp src_idx;\n  Dtype4 s1_cache;\n  Dtype4 s2_cache;\n  for (int_tp c = 0; c < ch_gr; ++c) {\n    src_idx = size * c + gId;\n    s1_cache = vload4(src_idx, src1);\n    s2_cache = vload4(src_idx, src2);\n    dst_cache.x +=  s1_cache.x * s2_cache.x + s1_cache.y * s2_cache.y;\n    dst_cache.y += -s1_cache.x * s2_cache.y + s1_cache.y * s2_cache.x;\n    dst_cache.z +=  s1_cache.z * s2_cache.z + s1_cache.w * s2_cache.w;\n    dst_cache.w += -s1_cache.z * s2_cache.w + s1_cache.w * s2_cache.z;\n  }\n  ((__global Dtype4*)(&dst[gId<<2]))[0] += dst_cache; \n}\n\n__kernel void TEMPLATE(complex_conjugate_multiplication_2d,Dtype)(__global Dtype* dst,\n    const int_tp offset_dst, \n    const __global Dtype* src1, const int_tp offset_src1, \n    const __global Dtype* src2, const int_tp offset_src2,\n    const int_tp out_gr, const int_tp map_size, const int_tp ch_gr) {\n  dst += offset_dst;\n  src1 += offset_src1;\n  src2 += offset_src2;\n  int_tp gId = get_global_id(0);\n  int_tp out = get_global_id(1);\n  int_tp src1_idx, src2_idx;\n  int_tp dst_map_offset = map_size * out;\n  int_tp dst_idx = dst_map_offset + gId;\n  Dtype4 s1_cache, s2_cache;\n  Dtype4 dst_cache = 0.f;\n  int_tp map_offset = dst_map_offset * ch_gr;\n  for (int_tp i = 0; i < ch_gr; ++i) {\n    src1_idx = map_size * i + gId;\n    src2_idx = map_offset + src1_idx;\n    s1_cache = vload4(src1_idx, src1);\n    s2_cache = vload4(src2_idx, src2);\n    dst_cache.xz += mad( s1_cache.xz, s2_cache.xz, s1_cache.yw * s2_cache.yw);\n    dst_cache.yw += mad(-s1_cache.xz, s2_cache.yw, s1_cache.yw * s2_cache.xz);\n  }\n  vstore4(dst_cache, dst_idx, dst);\n}\n\n__kernel void TEMPLATE(complex_conjugate_multiplication_2d_SLM,Dtype)(\n    __global Dtype* restrict dst, const int_tp offset_dst,\n    const __global Dtype* restrict src1, const int_tp offset_src1, \n    __local Dtype* local_src1, \n    const __global Dtype* restrict src2, const int_tp offset_src2, \n    const int_tp out_gr, const int_tp map_size, const int_tp ch_gr) {\n  int_tp gId = get_global_id(0);\n  if (gId >= map_size) return; /* Do not remove this */\n  int_tp out = get_global_id(1);\n  if (out >= out_gr) return;   /* Do not remove this */\n  dst += offset_dst;\n  src1 += offset_src1;\n  src2 += offset_src2;\n  int_tp tId = get_local_id(0);\n  int_tp local_out = get_local_id(1);\n  int_tp tile_size = get_local_size(0);\n  Dtype4 s1_cache;\n  if (local_out == 0) {\n    for (int_tp c = 0; c < ch_gr; ++c) {\n      s1_cache = vload4(map_size * c + gId, src1);\n      vstore4(s1_cache, tile_size * c + tId, local_src1); \n    }\n  }\n  barrier(CLK_LOCAL_MEM_FENCE);\n  int_tp dst_map_offset = map_size * out;\n  int_tp dst_idx = (dst_map_offset + gId) << 2;\n  Dtype4 dst_cache = 0.f;\n  Dtype4 s2_cache;\n  int_tp ch_offset = 0; \n  int_tp map_offset = dst_map_offset * ch_gr; \n  for (int_tp c = 0; c < ch_gr; ++c) {\n    ch_offset = map_size * c;\n    s1_cache = vload4(tile_size * c + tId, local_src1);\n    s2_cache = vload4(map_offset + ch_offset + gId, src2);\n    dst_cache.xz += mad( s1_cache.xz, s2_cache.xz, s1_cache.yw * s2_cache.yw);\n    dst_cache.yw += mad(-s1_cache.xz, s2_cache.yw, s1_cache.yw * s2_cache.xz);\n  }\n  ((__global Dtype4*)(&dst[dst_idx]))[0] += dst_cache; \n}\n\n__kernel void TEMPLATE(complex_conjugate_multiplication_3d,Dtype)(__global Dtype* dst,\n    const int_tp offset_dst, \n    const __global Dtype* src1, const int_tp offset_src1,\n    const __global Dtype* src2, const int_tp offset_src2, \n    const int_tp out_gr, const int_tp size, const int_tp ch_gr) {\n  dst  += offset_dst;\n  src1 += offset_src1;\n  src2 += offset_src2;\n  int_tp gId = get_global_id(0);\n  int_tp out = get_global_id(1);\n  int_tp ch  = get_global_id(2);\n  Dtype4 dst_cache = 0.f;\n  Dtype4 s1_cache  = ((__global Dtype4*)(&(src1[(size*ch+gId)<<2])))[0];\n  Dtype4 s2_cache  = ((__global Dtype4*)(&(src2[(size*(out*ch_gr+ch)+gId)<<2])))[0];\n  dst_cache.x =  s1_cache.x * s2_cache.x + s1_cache.y * s2_cache.y;\n  dst_cache.y = -s1_cache.x * s2_cache.y + s1_cache.y * s2_cache.x;\n  dst_cache.z =  s1_cache.z * s2_cache.z + s1_cache.w * s2_cache.w;\n  dst_cache.w = -s1_cache.z * s2_cache.w + s1_cache.w * s2_cache.z;\n  ((__global Dtype4*)(&dst[(size*out+gId)<<2]))[0] += dst_cache;\n}\n\n__kernel void TEMPLATE(complex_conjugate_multiplication_3d_SLM,Dtype)(__global Dtype* dst,\n    const int_tp offset_dst, __local Dtype* local_dst,  \n    const __global Dtype* src1, const int_tp offset_src1, \n    __local Dtype* local_src1, const __global Dtype* src2, \n    const int_tp offset_src2, const int_tp out_gr, const int_tp map_size, \n    const int_tp ch_gr) {\n  int_tp gId = get_global_id(0);\n  if (gId >= map_size) return; /* Do not remove this */\n  int_tp out = get_global_id(1);\n  if (out >= out_gr) return;   /* Do not remove this */\n  int_tp ch = get_global_id(2);\n  if (ch >= ch_gr) return;     /* Do not remove this */\n  dst += offset_dst;\n  src1 += offset_src1;\n  src2 += offset_src2;\n  int_tp tId = get_local_id(0);\n  int_tp local_out = get_local_id(1);\n  int_tp tile_size = get_local_size(0);\n  Dtype4 s1_cache;\n  if (local_out == 0) {\n    s1_cache = vload4(map_size * ch + gId, src1);\n    vstore4(s1_cache, tile_size * ch + tId, local_src1);\n  }\n  barrier(CLK_LOCAL_MEM_FENCE);\n  int_tp dst_map_offset = map_size * out;\n  int_tp dst_idx = (dst_map_offset + gId) << 2;\n  Dtype4 dst_cache = 0.f;\n  Dtype4 s2_cache;\n  s1_cache = vload4(tile_size * ch + tId, local_src1);\n  s2_cache = vload4((dst_map_offset * ch_gr) + (map_size * ch) + gId, src2);\n  dst_cache.x +=  s1_cache.x * s2_cache.x + s1_cache.y * s2_cache.y;\n  dst_cache.y += -s1_cache.x * s2_cache.y + s1_cache.y * s2_cache.x;\n  dst_cache.z +=  s1_cache.z * s2_cache.z + s1_cache.w * s2_cache.w;\n  dst_cache.w += -s1_cache.z * s2_cache.w + s1_cache.w * s2_cache.z;\n  ((__global Dtype4*)(&dst[dst_idx]))[0] += dst_cache;\n}\n\n__kernel void TEMPLATE(complex_multiplication_1d,Dtype)(__global Dtype* dst,\n    const int_tp offset_dst, \n    const __global Dtype* src1, const int_tp offset_src1, \n    const __global Dtype* src2, const int_tp offset_src2,\n    const int_tp size, const int_tp ch_gr) {\n  dst += offset_dst;\n  src1 += offset_src1;\n  src2 += offset_src2;\n  int_tp gId = get_global_id(0);\n  Dtype4 s2_cache;\n  Dtype4 dst_cache = 0.f;\n  int_tp idx_with_ch;\n  Dtype4 s1_cache = vload4(gId, src1);\n  for (int_tp ch = 0; ch < ch_gr; ++ch) {\n    idx_with_ch = size * ch + gId;\n    s2_cache = vload4(idx_with_ch, src2);\n    dst_cache.xz = s1_cache.xz * s2_cache.xz - s1_cache.yw * s2_cache.yw;\n    dst_cache.yw = s1_cache.xz * s2_cache.yw + s1_cache.yw * s2_cache.xz;\n    ((__global Dtype4*)(&dst[idx_with_ch<<2]))[0] += dst_cache;\n  }\n}\n\n__kernel void TEMPLATE(complex_multiplication_2d_SLM,Dtype)(__global Dtype* restrict dst,\n    const int_tp offset_dst, __local Dtype* local_dst,\n    const __global Dtype* restrict src1, const int_tp offset_src1, \n    const __global Dtype* restrict src2, const int_tp offset_src2,\n    const int_tp num_output, const int_tp size, const int_tp ch_gr) {\n  int_tp gId = get_global_id(0);\n  if (gId >= size) return;\n  int_tp out = get_global_id(1);\n  if (out >= num_output) return;\n  dst += offset_dst;\n  src1 += offset_src1;\n  src2 += offset_src2;\n  int_tp tId = get_local_id(0);\n  int_tp tOut = get_local_id(1);\n  int_tp tile_size = get_local_size(0);\n  int_tp local_out_size = get_local_size(1);\n  int_tp out_offset = out * size;\n  int_tp out_ch_offset = out_offset * ch_gr;\n  int_tp tile_size_in_all_ch = tile_size * ch_gr;\n  int_tp local_out_ch_offset = tOut * tile_size_in_all_ch;\n  int_tp src2_idx, local_dst_idx;\n  Dtype4 s2_cache, dst_cache;\n  int_tp src1_idx = out_offset + gId;\n  Dtype4 s1_cache = vload4(src1_idx, src1);\n  for (int_tp ch = 0; ch < ch_gr; ++ch) {\n    src2_idx = out_ch_offset + ch * size + gId;\n    s2_cache = vload4(src2_idx, src2);\n    dst_cache.xz = s1_cache.xz * s2_cache.xz - s1_cache.yw * s2_cache.yw;\n    dst_cache.yw = s1_cache.xz * s2_cache.yw + s1_cache.yw * s2_cache.xz;\n    local_dst_idx = local_out_ch_offset + ch * tile_size + tId;\n    vstore4(dst_cache, local_dst_idx, local_dst);\n  }\n  barrier(CLK_LOCAL_MEM_FENCE);\n  int_tp start_idx, half_start_idx;\n  int_tp ch_offset;\n  int_tp this_idx, that_idx;\n  for (int_tp offset = local_out_size >>= 1; offset > 0; offset >>=1) {\n    if (tOut < offset) {\n      start_idx = tOut * tile_size_in_all_ch + tId;\n      half_start_idx = (tOut + offset) * tile_size_in_all_ch + tId;\n      for (int_tp ch = 0; ch < ch_gr; ++ch) {\n        ch_offset = ch * tile_size;\n        this_idx = (start_idx + ch_offset) << 2;\n        that_idx = (half_start_idx + ch_offset) << 2;\n        ((__local Dtype4*)(&local_dst[this_idx]))[0] += \n            ((__local Dtype4*)(&local_dst[that_idx]))[0];\n      }\n    }\n    barrier(CLK_LOCAL_MEM_FENCE);\n  }\n  if (tOut == 0) {\n    for (int_tp ch = 0; ch < ch_gr; ++ch) {\n      dst_cache = vload4(tile_size * ch + tId, local_dst);\n      ((__global Dtype4*)(&dst[(size * ch + gId)<<2]))[0] += dst_cache;\n    }\n  }\n}\n\n__kernel void TEMPLATE(complex_multiplication_3d,Dtype)(__global Dtype* dst,\n    const int_tp offset_dst, \n    const __global Dtype* src1, const int_tp offset_src1, \n    const __global Dtype* src2, const int_tp offset_src2,\n    const int_tp size, const int_tp ch_gr, const int_tp out_gr, const int_tp num_output) {\n  dst  += offset_dst;\n  src1 += offset_src1;\n  src2 += offset_src2;\n  int_tp gId = get_global_id(0);\n  int_tp ch  = get_global_id(1);\n  int_tp out = get_global_id(2);\n  int_tp g = out / out_gr;\n  ch += (g * ch_gr);\n  int_tp c_offset = ch - ((ch / ch_gr) * ch_gr); \n  __global Dtype2* dst_ch = ((__global Dtype2*)(dst)) + (size * ch);\n  __global Dtype2* src1_out = ((__global Dtype2*)(src1)) + (size * out);\n  __global Dtype2* src2_out_ch = ((__global Dtype2*)(src2)) + (size * (out * ch_gr + c_offset));\n  Dtype2 s1_cache  = src1_out[gId];\n  Dtype2 s2_cache  = src2_out_ch[gId];\n  Dtype2 dst_cache = 0.f;\n  dst_cache.x = s1_cache.x * s2_cache.x - s1_cache.y * s2_cache.y;\n  dst_cache.y = s1_cache.x * s2_cache.y + s1_cache.y * s2_cache.x;\n  dst_ch[gId] += dst_cache;\n}\n\n/* Convert [RRRR...GGGG...BBBB...] to [RGBRGBRGBRGB...] */\n/* Reshape 2 */\n__kernel void TEMPLATE(convert_data_to_channel_major,Dtype)(__global Dtype2* dst, \n    const __global Dtype2* src, const int_tp size, const int_tp ch_gr) {\n  int_tp gId = get_global_id(0);\n  __global Dtype* dst_ptr = (__global Dtype*)(dst + (gId * ch_gr));\n  const __global Dtype* src_ptr = (const __global Dtype*)(src + gId);\n  Dtype2 s;\n  int_tp src_idx = 0;\n  for (int_tp i = 0; i < ch_gr; ++i) {\n    s = vload2(src_idx, src_ptr);\n    vstore2(s, i, dst_ptr);\n    src_idx += size;\n  }\n}\n/* Reshape 1 */\n/*__kernel void TEMPLATE(convert_data_to_channel_major(__global Dtype4* dst,\n    const __global Dtype4* src, const int_tp size, const int_tp ch_gr) {\n  int_tp gId = get_global_id(0);\n  const __global Dtype4* src_ptr4 = src + gId; \n  __global Dtype4* dst_ptr4 = dst + (gId * ch_gr);\n  for (int_tp i = 0; i < ch_gr; ++i) {\n      dst_ptr4[i] = src_ptr4[i*size];\n  }\n}\n*/\n\n/* Convert multiple [RRRR...GGGG...BBBB...] to multiple [RGBRGBRGBRGB...] */\n/* Reshape 2 */\n__kernel void TEMPLATE(convert_weight_to_channel_major,Dtype)(__global Dtype2* dst, \n    const __global Dtype2* src, const int_tp size, const int_tp ch_gr,\n    const int_tp num_output) {\n  int_tp gId = get_global_id(0);\n  int_tp out = get_global_id(1);\n  int_tp out_offset = out * (size * ch_gr);\n  __global Dtype* dst_ptr = (__global Dtype*)(dst + out_offset + (gId * ch_gr));\n  const __global Dtype* src_ptr = \n      (const __global Dtype*)(src + out_offset + gId);\n  Dtype2 s;\n  int_tp src_idx = 0;\n  for (int_tp i = 0; i < ch_gr; ++i) {\n    s = vload2(src_idx, src_ptr);\n    vstore2(s, i, dst_ptr);\n    src_idx += size;\n  }\n}\n/* Reshape 1 */\n/*\n__kernel void TEMPLATE(convert_weight_to_channel_major(__global Dtype4* dst,\n    const __global Dtype4* src, const int_tp size, const int_tp ch_gr,\n    const int_tp out_gr) {\n  int_tp gId = get_global_id(0);\n  int_tp out = get_global_id(1);\n  int_tp out_offset = out * (size * ch_gr);\n  __global Dtype4* dst_ptr4 = dst + out_offset + (gId * ch_gr);\n  const __global Dtype4* src_ptr4 = src + out_offset + gId;\n  for (int_tp i = 0; i < ch_gr; ++i) {\n    dst_ptr4[i] = src_ptr4[size * i];\n  }\n}\n*/\n\n/* Cdotc per element */\n/* Reshape 1 */\n/*\n__kernel void TEMPLATE(batchedCdotc(__global Dtype4* dst, \n    const __global Dtype4* src1, const __global Dtype4* src2,  \n    const int_tp size, const int_tp ch_gr, const int_tp out_gr) {  \n  int_tp gId = get_global_id(0); \n  int_tp out = get_global_id(1); \n  int_tp ch_offset = gId * ch_gr; \n  int_tp out_offset = out * size; \n  const __global Dtype* src1_ptr = (const __global Dtype*)(src1 + ch_offset);  \n  const __global Dtype* src2_ptr = (const __global Dtype*)(src2 + (out_offset * ch_gr) + ch_offset); \n  Dtype4 cdotc = 0.f; \n  Dtype4 s1, s2; \n  for (int_tp c = 0; c < ch_gr; ++c) { \n    s1 = vload4(c, src1_ptr); \n    s2 = vload4(c, src2_ptr); \n    cdotc.xz += mad( s1.xz, s2.xz, s1.yw * s2.yw); \n    cdotc.yw += mad(-s1.xz, s2.yw, s1.yw * s2.xz); \n  } \n  __global Dtype4* dst_ptr4 = dst + out_offset + gId; \n  dst_ptr4[0] += cdotc; \n}\n*/\n\n/* Cdotc per two elements */\n/* Reshape 2 */\n__kernel void TEMPLATE(batchedCdotc,Dtype)(__global Dtype2* dst,\n    const __global Dtype2* src1, const __global Dtype2* src2, \n    const int_tp size, const int_tp ch_gr, const int_tp out_gr) {\n  int_tp gId = get_global_id(0);\n  int_tp out = get_global_id(1);\n  int_tp ch_offset = gId * ch_gr;\n  const __global Dtype* src1_ptr = (const __global Dtype*)(src1 + ch_offset); \n  const __global Dtype* src2_ptr = \n      (const __global Dtype*)(src2 + (out * size * ch_gr) + ch_offset);\n  Dtype4 cdotc4 = 0.f;\n  Dtype2 cdotc = 0.f;\n  Dtype4 s1, s2;\n  int_tp n = ch_gr >> 1;\n  int_tp r = ch_gr - (n << 1);\n  for (int_tp i = 0; i < n; ++i) {\n    s1 = vload4(i, src1_ptr);\n    s2 = vload4(i, src2_ptr);\n    cdotc4.xz += mad( s1.xz, s2.xz, s1.yw * s2.yw);\n    cdotc4.yw += mad(-s1.xz, s2.yw, s1.yw * s2.xz);\n  }\n  cdotc.x += dot(cdotc4.xz, (float2)(1));\n  cdotc.y += dot(cdotc4.yw, (float2)(1));\n  if (r == 1) {\n    const __global Dtype* src1_ptr2 = \n        (const __global Dtype*)(((const __global Dtype4*)(src1_ptr)) + n);\n    const __global Dtype* src2_ptr2 = \n        (const __global Dtype*)(((const __global Dtype4*)(src2_ptr)) + n);\n    Dtype2 t1 = vload2(0, src1_ptr2); \n    Dtype2 t2 = vload2(0, src2_ptr2);\n    cdotc.x += mad( t1.x, t2.x, t1.y * t2.y);\n    cdotc.y += mad(-t1.x, t2.y, t1.y * t2.x);\n  }\n  __global Dtype* dst_ptr = (__global Dtype*)(dst + (out * size) + gId);\n  vstore2(cdotc, 0, dst_ptr);\n}\n#endif",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(fillbuffer,Dtype)(const int_tp n, const char alpha, __global char* x,\n                                   const int_tp offx) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    x[index + offx] = alpha;\n  }\n}\n\n__kernel void TEMPLATE(fill,Dtype)(const int_tp n, const Dtype alpha, __global Dtype* x,\n                                   const int_tp offx) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    x[index + offx] = alpha;\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(im2col,Dtype)(const int_tp n,\n                                     __global const Dtype* data_im,\n                                     const int_tp data_im_off,\n                                     const int_tp height, const int_tp width,\n                                     const int_tp kernel_h,\n                                     const int_tp kernel_w, const int_tp pad_h,\n                                     const int_tp pad_w, const int_tp stride_h,\n                                     const int_tp stride_w,\n                                     const int_tp dilation_h,\n                                     const int_tp dilation_w,\n                                     const int_tp height_col,\n                                     const int_tp width_col,\n                                     __global Dtype* data_col,\n                                     const int_tp data_col_off) {\n\n  for (int_tp index = get_global_id(0); index < n;\n      index += get_global_size(0)) {\n    const int_tp h_index = index / width_col;\n    const int_tp h_col = h_index % height_col;\n    const int_tp w_col = index % width_col;\n    const int_tp c_im = h_index / height_col;\n    const int_tp c_col = c_im * kernel_h * kernel_w;\n    const int_tp h_offset = h_col * stride_h - pad_h;\n    const int_tp w_offset = w_col * stride_w - pad_w;\n    __global Dtype* data_col_ptr = data_col + data_col_off;\n    data_col_ptr += (c_col * height_col + h_col) * width_col + w_col;\n    __global const Dtype* data_im_ptr = data_im + data_im_off;\n    data_im_ptr += (c_im * height + h_offset) * width + w_offset;\n    for (int_tp i = 0; i < kernel_h; ++i) {\n      for (int_tp j = 0; j < kernel_w; ++j) {\n        int_tp h_im = h_offset + i * dilation_h;\n        int_tp w_im = w_offset + j * dilation_w;\n        *data_col_ptr =\n            (h_im >= 0 && w_im >= 0 && h_im < height && w_im < width) ?\n                data_im_ptr[i * dilation_h * width + j * dilation_w] : 0;\n        data_col_ptr += height_col * width_col;\n      }\n    }\n  }\n}\n\n__kernel void TEMPLATE(col2im,Dtype)(const int_tp n,\n                                     __global const Dtype* data_col,\n                                     const int_tp data_col_off,\n                                     const int_tp height, const int_tp width,\n                                     const int_tp channels,\n                                     const int_tp kernel_h,\n                                     const int_tp kernel_w, const int_tp pad_h,\n                                     const int_tp pad_w, const int_tp stride_h,\n                                     const int_tp stride_w,\n                                     const int_tp dilation_h,\n                                     const int_tp dilation_w,\n                                     const int_tp height_col,\n                                     const int_tp width_col,\n                                     __global Dtype* data_im,\n                                     const int_tp data_im_off) {\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    Dtype val = 0;\n    const int_tp w_im = index % width + pad_w;\n    const int_tp h_im = (index / width) % height + pad_h;\n    const int_tp c_im = index / (width * height);\n    int_tp kernel_extent_w = (kernel_w - 1) * dilation_w + 1;\n    int_tp kernel_extent_h = (kernel_h - 1) * dilation_h + 1;\n    // compute the start and end of the output\n    const int_tp w_col_start =\n        (w_im < kernel_extent_w) ? 0 : (w_im - kernel_extent_w) / stride_w + 1;\n    const int_tp w_col_end = min(w_im / stride_w + 1, width_col);\n    const int_tp h_col_start =\n        (h_im < kernel_extent_h) ? 0 : (h_im - kernel_extent_h) / stride_h + 1;\n    const int_tp h_col_end = min(h_im / stride_h + 1, height_col);\n    // TODO: use LCM of stride and dilation to avoid unnecessary loops\n    for (int_tp h_col = h_col_start; h_col < h_col_end; h_col += 1) {\n      for (int_tp w_col = w_col_start; w_col < w_col_end; w_col += 1) {\n        int_tp h_k = (h_im - h_col * stride_h);\n        int_tp w_k = (w_im - w_col * stride_w);\n        if (h_k % dilation_h == 0 && w_k % dilation_w == 0) {\n          h_k /= dilation_h;\n          w_k /= dilation_w;\n          int_tp data_col_index = (((c_im * kernel_h + h_k) * kernel_w + w_k) *\n                                height_col + h_col) * width_col + w_col;\n          val += data_col[data_col_off + data_col_index];\n        }\n      }\n    }\n    data_im[data_im_off + index] = val;\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(im2col_nd, Dtype)(const int_tp n, const int_tp num_axes,\n                                         const int_tp channel_axis,\n                                         __global const Dtype* data_im,\n                                         const int_tp data_im_off,\n                                         __global const int_tp* im_shape,\n                                         __global const int_tp* col_shape,\n                                         __global const int_tp* kernel_shape,\n                                         __global const int_tp* pad,\n                                         __global const int_tp* stride,\n                                         __global const int_tp* dilation,\n                                         __global Dtype* data_col,\n                                         const int_tp data_col_off) {\n  int_tp d_temp[6];\n  int_tp d_iter[6];\n  int_tp i;\n\n  __global const int_tp* im_shape_ptr = im_shape + channel_axis;\n  __global const int_tp* col_shape_ptr = col_shape + channel_axis;\n\n  __local int_tp shared_dilation[6];\n  __local int_tp shared_kernel_shape[6];\n  __local int_tp shared_pad[6];\n  __local int_tp shared_stride[6];\n  __local int_tp shared_col_shape[6 + 1];\n  __local int_tp shared_im_shape[6 + 1];\n\n  for (int li = get_local_id(0); li < num_axes; li += get_local_size(0)) {\n    shared_dilation[li] = dilation[li];\n    shared_kernel_shape[li] = kernel_shape[li];\n    shared_pad[li] = pad[li];\n    shared_stride[li] = stride[li];\n  }\n\n  for (int li = get_local_id(0); li < num_axes + 1; li += get_local_size(0)) {\n    shared_col_shape[li] = col_shape_ptr[li];\n    shared_im_shape[li] = im_shape_ptr[li];\n  }\n\n  barrier(CLK_LOCAL_MEM_FENCE);\n\n  for (int_tp index = get_global_id(0); index < n;\n      index += get_global_size(0)) {\n    // Initialize channel_in, computed in the loop below, with intermediate\n    // computations used to compute the spatial indices.\n    int_tp channel_in = index;\n    int_tp channel_out = 1;\n    for (i = num_axes - 1; i >= 0; --i) {\n      d_temp[i] = channel_in % shared_col_shape[i + 1];\n      channel_in /= shared_col_shape[i + 1];\n      channel_out *= shared_kernel_shape[i];\n    }\n    channel_out *= channel_in;\n    int_tp data_col_inc = 1;\n    for (i = 0; i < num_axes; ++i) {\n      channel_out *= shared_col_shape[i + 1];\n      channel_out += d_temp[i];\n      d_temp[i] = d_temp[i] * shared_stride[i] - shared_pad[i];\n      channel_in *= shared_im_shape[i + 1];\n      channel_in += d_temp[i];\n      data_col_inc *= shared_col_shape[i + 1];\n      d_iter[i] = 0;\n    }\n    __global Dtype* data_col_ptr = data_col + data_col_off + channel_out;\n    __global const Dtype* data_im_ptr = data_im + data_im_off + channel_in;\n    bool incremented;\n    do {\n      bool in_range = true;\n      for (i = 0; i < num_axes; ++i) {\n        const int_tp d_iter_im = d_iter[i] * shared_dilation[i] + d_temp[i];\n        in_range &= d_iter_im >= 0 && d_iter_im < shared_im_shape[i + 1];\n        if (!in_range) {\n          break;\n        }\n      }\n      if (in_range) {\n        int_tp data_im_offset = d_iter[0] * shared_dilation[0];\n        for (i = 1; i < num_axes; ++i) {\n          data_im_offset *= shared_im_shape[i + 1];\n          data_im_offset += d_iter[i] * shared_dilation[i];\n        }\n        *data_col_ptr = data_im_ptr[data_im_offset];\n      } else {\n        *data_col_ptr = 0;\n      }\n      data_col_ptr += data_col_inc;\n      incremented = false;\n      for (i = num_axes - 1; i >= 0; --i) {\n        const int_tp d_max = shared_kernel_shape[i];\n        if (d_iter[i] == d_max - 1) {\n          d_iter[i] = 0;\n        } else {  // d_iter[i] < d_max - 1\n          ++d_iter[i];\n          incremented = true;\n          break;\n        }\n      }  // for (int_tp i = num_axes - 1; i >= 0; --i)\n    } while (incremented);  // do\n  }\n}\n\n__kernel void TEMPLATE(col2im_nd, Dtype)(const int_tp n, const int_tp num_axes,\n                                         const int_tp channel_axis,\n                                         __global const Dtype* data_col,\n                                         const int_tp data_col_off,\n                                         __global const int_tp* im_shape,\n                                         __global const int_tp* col_shape,\n                                         __global const int_tp* kernel_shape,\n                                         __global const int_tp* pad,\n                                         __global const int_tp* stride,\n                                         __global const int_tp* dilation,\n                                         __global Dtype* data_im,\n                                         const int_tp data_im_off) {\n  int_tp d_im[6];\n  int_tp d_col_iter[6];\n  int_tp d_col_start[6];\n  int_tp d_col_end[6];\n\n  __global const int_tp* im_shape_ptr = im_shape + channel_axis;\n  __global const int_tp* col_shape_ptr = col_shape + channel_axis;\n\n  __local int_tp shared_dilation[6];\n  __local int_tp shared_kernel_shape[6];\n  __local int_tp shared_pad[6];\n  __local int_tp shared_stride[6];\n  __local int_tp shared_col_shape[6 + 1];\n  __local int_tp shared_im_shape[6 + 1];\n\n  for (int li = get_local_id(0); li < num_axes; li += get_local_size(0)) {\n    shared_dilation[li] = dilation[li];\n    shared_kernel_shape[li] = kernel_shape[li];\n    shared_pad[li] = pad[li];\n    shared_stride[li] = stride[li];\n  }\n  for (int li = get_local_id(0); li < num_axes + 1; li += get_local_size(0)) {\n    shared_col_shape[li] = col_shape_ptr[li];\n    shared_im_shape[li] = im_shape_ptr[li];\n  }\n\n  barrier(CLK_LOCAL_MEM_FENCE);\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    // Initialize channel_in, computed in the loop below, with intermediate\n    // computations used to compute the spatial indices.\n    int_tp c_im = index;\n    // Calculate d_im (image dimensions).\n    for (int_tp i = num_axes - 1; i >= 0; --i) {\n      d_im[i] = c_im % shared_im_shape[i + 1] + shared_pad[i];\n      c_im /= shared_im_shape[i + 1];\n    }\n    // Calculate col start/end indices.\n    bool done = false;\n    for (int_tp i = 0; i < num_axes; ++i) {\n      const int_tp kernel_extent = shared_dilation[i]\n          * (shared_kernel_shape[i] - 1) + 1;\n      d_col_start[i] = d_col_iter[i] =\n          (d_im[i] < kernel_extent) ?\n              0 : (d_im[i] - kernel_extent) / shared_stride[i] + 1;\n      d_col_end[i] = min(d_im[i] / shared_stride[i] + 1,\n                         shared_col_shape[i + 1]);\n      if (d_col_start[i] >= d_col_end[i]) {\n        // Skip computation if the dimension is 0 at any spatial axis --\n        // final val will be 0.\n        data_im[index] = 0;\n        done = true;\n        break;  // for (int_tp i = 0; i < num_axes; ++i)\n      }\n    }\n    if (!done) {\n      // Loop over the col to compute the output val.\n      Dtype val = 0;\n      bool incremented = true;\n      bool skip = false;\n      do {\n        // Compute the final offset.\n        int_tp final_offset = 0;\n        int_tp kernel_shape_prod = 1;\n        int_tp kernel_index;\n        for (int_tp i = num_axes - 1; i >= 0; --i) {\n          kernel_index = d_im[i] - d_col_iter[i] * shared_stride[i];\n          if (kernel_index % shared_dilation[i]) {\n            skip = true;\n            break;\n          } else {\n            kernel_index /= shared_dilation[i];\n            final_offset += kernel_index * kernel_shape_prod;\n            kernel_shape_prod *= shared_kernel_shape[i];\n          }\n        }\n        if (!skip) {\n          final_offset += kernel_shape_prod * c_im;\n          for (int_tp i = 0; i < num_axes; ++i) {\n            final_offset *= shared_col_shape[i + 1];\n            final_offset += d_col_iter[i];\n          }\n          val += data_col[data_col_off + final_offset];\n        }\n        skip = false;\n        incremented = false;\n        for (int_tp i = num_axes - 1; i >= 0; --i) {\n          const int_tp d_max = d_col_end[i];\n          if (d_col_iter[i] == d_max - 1) {\n            d_col_iter[i] = d_col_start[i];\n          } else {  // d_col_iter[i] < d_max - 1\n            ++d_col_iter[i];\n            incremented = true;\n            break;  // for (int_tp i = num_axes - 1; i >= 0; --i)\n          }\n        }  // for (int_tp i = num_axes - 1; i >= 0; --i)\n      } while (incremented);\n      data_im[data_im_off + index] = val;\n    }\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(lrn_compute_output,Dtype)(const int_tp nthreads,\n                                                 __global const Dtype* in,\n                                                 __global const Dtype* scale,\n                                                 const Dtype negative_beta,\n                                                 __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    out[index] = in[index] * pow(scale[index], negative_beta);\n  }\n}\n\n__kernel void TEMPLATE(lrn_fill_scale,Dtype)(const int_tp nthreads, __global const Dtype* in,\n                             const int_tp num, const int_tp channels,\n                             const int_tp height, const int_tp width, const int_tp size,\n                             const Dtype alpha_over_size, const Dtype k,\n                             __global Dtype* const scale) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    // find out the local offset\n    const int_tp w = index % width;\n    const int_tp h = (index / width) % height;\n    const int_tp n = index / width / height;\n    const int_tp offset = (n * channels * height + h) * width + w;\n    const int_tp step = height * width;\n    __global const Dtype* in_off = in + offset;\n    __global Dtype* scale_off = scale + offset;\n    int_tp head = 0;\n    const int_tp pre_pad = (size - 1) / 2;\n    const int_tp post_pad = size - pre_pad - 1;\n    Dtype accum_scale = 0;\n    // fill the scale at [n, :, h, w]\n    // accumulate values\n    while (head < post_pad && head < channels) {\n      accum_scale += in_off[head * step] * in_off[head * step];\n      ++head;\n    }\n    // both add and subtract\n    while (head < channels) {\n      accum_scale += in_off[head * step] * in_off[head * step];\n      if (head - size >= 0) {\n        accum_scale -= in_off[(head - size) * step]\n            * in_off[(head - size) * step];\n      }\n      scale_off[(head - post_pad) * step] = k + accum_scale * alpha_over_size;\n      ++head;\n    }\n    // subtract only\n    while (head < channels + post_pad) {\n      if (head - size >= 0) {\n        accum_scale -= in_off[(head - size) * step]\n            * in_off[(head - size) * step];\n      }\n      scale_off[(head - post_pad) * step] = k + accum_scale * alpha_over_size;\n      ++head;\n    }\n  }\n}\n\n__kernel void TEMPLATE(lrn_compute_diff,Dtype)(const int_tp nthreads,\n                               __global const Dtype* bottom_data,\n                               __global const Dtype* top_data,\n                               __global const Dtype* scale,\n                               __global const Dtype* top_diff, const int_tp num,\n                               const int_tp channels, const int_tp height,\n                               const int_tp width, const int_tp size,\n                               const Dtype negative_beta,\n                               const Dtype cache_ratio,\n                               __global Dtype* bottom_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    // find out the local offset\n    const int_tp w = index % width;\n    const int_tp h = (index / width) % height;\n    const int_tp n = index / width / height;\n    const int_tp offset = (n * channels * height + h) * width + w;\n    const int_tp step = height * width;\n    __global const Dtype* bottom_off = bottom_data + offset;\n    __global const Dtype* top_off = top_data + offset;\n    __global const Dtype* scale_off = scale + offset;\n    __global const Dtype* top_diff_off = top_diff + offset;\n    __global Dtype* bottom_diff_off = bottom_diff + offset;\n    int_tp head = 0;\n    const int_tp pre_pad = size - (size + 1) / 2;\n    const int_tp post_pad = size - pre_pad - 1;\n    Dtype accum_ratio = 0;\n    // accumulate values\n    while (head < post_pad && head < channels) {\n      accum_ratio += top_diff_off[head * step] * top_off[head * step]\n          / scale_off[head * step];\n      ++head;\n    }\n    // both add and subtract\n    while (head < channels) {\n      accum_ratio += top_diff_off[head * step] * top_off[head * step]\n          / scale_off[head * step];\n      if (head - size >= 0) {\n        accum_ratio -= top_diff_off[(head - size) * step]\n            * top_off[(head - size) * step] / scale_off[(head - size) * step];\n      }\n      bottom_diff_off[(head - post_pad) * step] = top_diff_off[(head - post_pad)\n          * step] * pow(scale_off[(head - post_pad) * step], negative_beta)\n          - cache_ratio * bottom_off[(head - post_pad) * step] * accum_ratio;\n      ++head;\n    }\n    // subtract only\n    while (head < channels + post_pad) {\n      if (head - size >= 0) {\n        accum_ratio -= top_diff_off[(head - size) * step]\n            * top_off[(head - size) * step] / scale_off[(head - size) * step];\n      }\n      bottom_diff_off[(head - post_pad) * step] = top_diff_off[(head - post_pad)\n          * step] * pow(scale_off[(head - post_pad) * step], negative_beta)\n          - cache_ratio * bottom_off[(head - post_pad) * step] * accum_ratio;\n      ++head;\n    }\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\ninline Dtype TEMPLATE(lstm_sigmoid,Dtype)(const Dtype x) {\n  return (Dtype)1 / ((Dtype)1 + exp(-x));\n}\n\ninline Dtype TEMPLATE(lstm_tanh,Dtype)(const Dtype x) {\n  return (Dtype)2 * TEMPLATE(lstm_sigmoid,Dtype)((Dtype)2 * x) - (Dtype)1;\n}\n\n__kernel void TEMPLATE(lstm_acts_forward,Dtype)(const int_tp nthreads, const int_tp dim,\n                                __global const Dtype* X, __global Dtype* X_acts) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp x_dim = 4 * dim;\n    const int_tp d = index % x_dim;\n    if (d < 3 * dim) {\n      X_acts[index] = TEMPLATE(lstm_sigmoid,Dtype)(X[index]);\n    } else {\n      X_acts[index] = TEMPLATE(lstm_tanh,Dtype)(X[index]);\n    }\n  }\n}\n\n__kernel void TEMPLATE(lstm_unit_forward,Dtype)(const int_tp nthreads, const int_tp dim,\n    __global const Dtype* C_prev, __global const Dtype* X, __global const Dtype* cont,\n    __global Dtype* C, __global Dtype* H) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp n = index / dim;\n    const int_tp d = index % dim;\n    __global const Dtype* X_offset = X + 4 * dim * n;\n    const Dtype i = X_offset[d];\n    const Dtype f = X_offset[1 * dim + d];\n    const Dtype o = X_offset[2 * dim + d];\n    const Dtype g = X_offset[3 * dim + d];\n    const Dtype c_prev = C_prev[index];\n    const Dtype c = cont[n] * f * c_prev + i * g;\n    C[index] = c;\n    const Dtype tanh_c = TEMPLATE(lstm_tanh,Dtype)(c);\n    H[index] = o * tanh_c;\n  }\n}\n\n__kernel void TEMPLATE(lstm_unit_backward,Dtype)(const int_tp nthreads, const int_tp dim,\n    __global const Dtype* C_prev, __global const Dtype* X, __global const Dtype* C, __global const Dtype* H,\n    __global const Dtype* cont, __global const Dtype* C_diff, __global const Dtype* H_diff,\n    __global Dtype* C_prev_diff, __global Dtype* X_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp n = index / dim;\n    const int_tp d = index % dim;\n    __global const Dtype* X_offset = X + 4 * dim * n;\n    const Dtype i = X_offset[d];\n    const Dtype f = X_offset[1 * dim + d];\n    const Dtype o = X_offset[2 * dim + d];\n    const Dtype g = X_offset[3 * dim + d];\n    const Dtype c_prev = C_prev[index];\n    const Dtype c = C[index];\n    const Dtype tanh_c = TEMPLATE(lstm_tanh,Dtype)(c);\n    __global Dtype* c_prev_diff = C_prev_diff + index;\n    __global Dtype* X_diff_offset = X_diff + 4 * dim * n;\n    __global Dtype* i_diff = X_diff_offset + d;\n    __global Dtype* f_diff = X_diff_offset + 1 * dim + d;\n    __global Dtype* o_diff = X_diff_offset + 2 * dim + d;\n    __global Dtype* g_diff = X_diff_offset + 3 * dim + d;\n    const Dtype c_term_diff =\n        C_diff[index] + H_diff[index] * o * (1 - tanh_c * tanh_c);\n    const Dtype cont_n = cont[n];\n    *c_prev_diff = cont_n * c_term_diff * f;\n    *i_diff = c_term_diff * g;\n    *f_diff = cont_n * c_term_diff * c_prev;\n    *o_diff = H_diff[index] * tanh_c;\n    *g_diff = c_term_diff * i;\n  }\n}\n\n__kernel void TEMPLATE(lstm_acts_backward,Dtype)(const int_tp nthreads, const int_tp dim,\n          __global const Dtype* X_acts, __global const Dtype* X_acts_diff, __global Dtype* X_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp x_dim = 4 * dim;\n    const int_tp d = index % x_dim;\n    const Dtype X_act = X_acts[index];\n    if (d < 3 * dim) {\n      X_diff[index] = X_acts_diff[index] * X_act * ((Dtype)1 - X_act);\n    } else {\n      X_diff[index] = X_acts_diff[index] * ((Dtype)1 - X_act * X_act);\n    }\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(mul,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa,\n                                  __global Dtype* b,\n                                  const int_tp offb, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[index + offy] = a[index + offa] * b[index + offb];\n  }\n}\n\n__kernel void TEMPLATE(div,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa,\n                                  __global Dtype* b,\n                                  const int_tp offb, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[index + offy] = a[index + offa] / b[index + offb];\n  }\n}\n\n__kernel void TEMPLATE(add_scalar,Dtype)(const int_tp N, const Dtype alpha,\n__global Dtype* Y,\n                                         const int_tp offY) {\n  for (int_tp index = get_global_id(0); index < N; index += get_global_size(0)) {\n    Y[offY + index] += alpha;\n  }\n}\n\n__kernel void TEMPLATE(add,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa, __global const Dtype* b,\n                                  const int_tp offb, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[offy + index] = a[offa + index] + b[offb + index];\n  }\n}\n\n__kernel void TEMPLATE(sub,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa, __global const Dtype* b,\n                                  const int_tp offb, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[offy + index] = a[offa + index] - b[offb + index];\n  }\n}\n\n__kernel void TEMPLATE(abs,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[offy + index] = fabs((Dtype)(a[offa + index]));\n  }\n}\n\n__kernel void TEMPLATE(exp,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[offy + index] = exp(a[offa + index]);\n  }\n}\n\n__kernel void TEMPLATE(log,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[offy + index] = log((Dtype)(a[offa + index]));\n  }\n}\n\n__kernel void TEMPLATE(powx,Dtype)(const int_tp n, __global const Dtype* a,\n                                   const int_tp offa, Dtype alpha,\n                                   __global Dtype* y,\n                                   const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    if(alpha == 2.0) {\n      y[offy + index] = pow((Dtype)fabs(a[offa + index]), (Dtype)alpha);\n    } else {\n      y[offy + index] = pow((Dtype)a[offa + index], (Dtype)alpha);\n    }\n  }\n}\n\n__kernel void TEMPLATE(sign,Dtype)(const int_tp n, __global const Dtype* x,\n                                   const int_tp offx, __global Dtype* y,\n                                   const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[index + offy] = (0.0 < x[index + offx])\n        - (x[index + offx] < 0.0);\n  }\n}\n\n__kernel void TEMPLATE(sgnbit,Dtype)(const int_tp n, __global const Dtype* x,\n                                     const int_tp offx, __global Dtype* y,\n                                     const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[index + offy] = signbit(x[index + offx]);\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(merge_copy_forward_stack, Dtype)(const int_tp nthreads,\n                                                  const int_tp dims,\n                                                  __global const Dtype* bottom_a,\n                                                  const int_tp forward_a,\n                                                  __global const Dtype* bottom_b,\n                                                  const int_tp forward_b,\n                                                  __global Dtype* top,\n                                                  const int_tp num,\n                                                  const int_tp channels_a,\n                                                  const int_tp channels_b,\n                                                  __global const int_tp* shape_a,\n                                                  __global const int_tp* shape_b) {\n  int_tp pad[6];\n  int_tp tmp_idx[6];\n  int_tp size_a = 1;\n  int_tp size_b = 1;\n\n  for (int_tp i = 0; i < dims; ++i) {\n    pad[i] = (shape_b[i] - shape_a[i]) / 2;\n    size_a *= shape_a[i];\n    size_b *= shape_b[i];\n  }\n\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    int_tp batch_id = index / ((channels_a + channels_b) * size_a);\n    int_tp bottom_id = ((index - batch_id * (channels_a + channels_b) * size_a)\n        / (channels_a * size_a)) % 2;\n    int_tp counter = index;\n    for (int_tp i = dims - 1; i >= 0; --i) {\n      tmp_idx[i] = counter % shape_a[i];\n      counter /= shape_a[i];\n    }\n\n    if (bottom_id == 0) {\n      int_tp channel_id = (index / size_a) % channels_a;\n      int_tp aidx = batch_id * channels_a + channel_id;\n      for (int_tp i = 0; i < dims; ++i) {\n        aidx *= shape_a[i];\n        aidx += tmp_idx[i];\n      }\n      top[index] = (forward_a == 1) ? bottom_a[aidx] : 0;\n    } else {\n      int_tp channel_id = (index / size_a) % channels_b;\n      int_tp bidx = (batch_id * channels_b + channel_id) * size_b;\n      int_tp btemp = 1;\n      for (int_tp i = dims - 1; i >= 0; --i) {\n        bidx += btemp * (tmp_idx[i] + pad[i]);\n        btemp *= shape_b[i];\n      }\n      top[index] = (forward_b == 1) ? bottom_b[bidx] : 0;\n    }\n  }\n}\n\n__kernel void TEMPLATE(merge_copy_backward_stack,Dtype)(const int_tp nthreads,\n                                                  const int_tp dims,\n                                                  __global Dtype* bottom_a,\n                                                  const int_tp backward_a,\n                                                  __global Dtype* bottom_b,\n                                                  const int_tp backward_b,\n                                                  __global const Dtype* top,\n                                                  const int_tp num,\n                                                  const int_tp channels_a,\n                                                  const int_tp channels_b,\n                                                  __global const int_tp* shape_a,\n                                                  __global const int_tp* shape_b) {\n  int_tp pad[6];\n  int_tp tmp_idx[6];\n  int_tp size_a = 1;\n  int_tp size_b = 1;\n\n  for (int_tp i = 0; i < dims; ++i) {\n    pad[i] = (shape_b[i] - shape_a[i]) / 2;\n    size_a *= shape_a[i];\n    size_b *= shape_b[i];\n  }\n\n  for (int_tp index = get_global_id(0); index < nthreads; index +=\n      get_global_size(0)) {\n    int_tp batch_id = index / ((channels_a + channels_b) * size_a);\n    int_tp bottom_id = ((index - batch_id * (channels_a + channels_b) * size_a)\n        / (channels_a * size_a)) % 2;\n    int_tp counter = index;\n    for (int_tp i = dims - 1; i >= 0; --i) {\n      tmp_idx[i] = counter % shape_a[i];\n      counter /= shape_a[i];\n    }\n\n    if (bottom_id == 0) {\n      int_tp channel_id = (index / size_a) % channels_a;\n      int_tp aidx = batch_id * channels_a + channel_id;\n      for (int_tp i = 0; i < dims; ++i) {\n        aidx *= shape_a[i];\n        aidx += tmp_idx[i];\n      }\n      bottom_a[aidx] = (backward_a == 1) ? top[index] : 0;\n    } else {\n      int_tp channel_id = (index / size_a) % channels_b;\n      int_tp bidx = (batch_id * channels_b + channel_id) * size_b;\n      int_tp btemp = 1;\n      for (int_tp i = dims - 1; i >= 0; --i) {\n        bidx += btemp * (tmp_idx[i] + pad[i]);\n        btemp *= shape_b[i];\n      }\n      bottom_b[bidx] = (backward_b == 1) ? top[index] : 0;\n    }\n  }\n}\n\n\n__kernel void TEMPLATE(merge_copy_forward_add, Dtype)(const int_tp nthreads,\n                                                  const int_tp dims,\n                                                  __global const Dtype* bottom_a,\n                                                  const int_tp forward_a,\n                                                  __global const Dtype* bottom_b,\n                                                  const int_tp forward_b,\n                                                  __global Dtype* top,\n                                                  const int_tp num,\n                                                  const int_tp channels,\n                                                  __global const int_tp* shape_a,\n                                                  __global const int_tp* shape_b) {\n  int_tp pad[6];\n  int_tp tmp_idx[6];\n  int_tp size_a = 1;\n  int_tp size_b = 1;\n\n  for (int_tp i = 0; i < dims; ++i) {\n    pad[i] = (shape_b[i] - shape_a[i]) / 2;\n    size_a *= shape_a[i];\n    size_b *= shape_b[i];\n  }\n\n  for (int_tp index = get_global_id(0); index < nthreads; index +=\n      get_global_size(0)) {\n    int_tp batch_id = index / (channels * size_a);\n    int_tp counter = index;\n    for (int_tp i = dims - 1; i >= 0; --i) {\n      tmp_idx[i] = counter % shape_a[i];\n      counter /= shape_a[i];\n    }\n\n    top[index] = 0;\n    int_tp channel_id = (index / size_a) % channels;\n    int_tp aidx = batch_id * channels + channel_id;\n    for (int_tp i = 0; i < dims; ++i) {\n      aidx *= shape_a[i];\n      aidx += tmp_idx[i];\n    }\n    top[index] = forward_a ? top[index] + bottom_a[aidx] : top[index];\n    int_tp bidx = (batch_id * channels + channel_id) * size_b;\n    int_tp btemp = 1;\n    for (int_tp i = dims - 1; i >= 0; --i) {\n      bidx += btemp * (tmp_idx[i] + pad[i]);\n      btemp *= shape_b[i];\n    }\n    top[index] = forward_b ? top[index] + bottom_b[bidx] : top[index];\n  }\n}\n\n__kernel void TEMPLATE(merge_copy_backward_add,Dtype)(const int_tp nthreads,\n                                                  const int_tp dims,\n                                                  __global Dtype* bottom_a,\n                                                  const int_tp backward_a,\n                                                  __global Dtype* bottom_b,\n                                                  const int_tp backward_b,\n                                                  __global const Dtype* top,\n                                                  const int_tp num,\n                                                  const int_tp channels,\n                                                  __global const int_tp* shape_a,\n                                                  __global const int_tp* shape_b) {\n  int_tp pad[6];\n  int_tp tmp_idx[6];\n  int_tp size_a = 1;\n  int_tp size_b = 1;\n\n  for (int_tp i = 0; i < dims; ++i) {\n    pad[i] = (shape_b[i] - shape_a[i]) / 2;\n    size_a *= shape_a[i];\n    size_b *= shape_b[i];\n  }\n\n  for (int_tp index = get_global_id(0); index < nthreads; index +=\n      get_global_size(0)) {\n    int_tp batch_id = index / (channels * size_a);\n    int_tp counter = index;\n    for (int_tp i = dims - 1; i >= 0; --i) {\n      tmp_idx[i] = counter % shape_a[i];\n      counter /= shape_a[i];\n    }\n\n    int_tp channel_id = (index / size_a) % channels;\n    int_tp aidx = batch_id * channels + channel_id;\n    for (int_tp i = 0; i < dims; ++i) {\n      aidx *= shape_a[i];\n      aidx += tmp_idx[i];\n    }\n    bottom_a[aidx] = backward_a ? top[index] : 0;\n    int_tp bidx = (batch_id * channels + channel_id) * size_b;\n    int_tp btemp = 1;\n    for (int_tp i = dims - 1; i >= 0; --i) {\n      bidx += btemp * (tmp_idx[i] + pad[i]);\n      btemp *= shape_b[i];\n    }\n    bottom_b[bidx] = backward_b ? top[index] : 0;\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(max_pool_forward,Dtype)(\n    const int_tp nthreads, __global const Dtype* bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp stride_h, const int_tp stride_w, const int_tp pad_h,\n    const int_tp pad_w,\n    __global Dtype* top_data,\n    const int use_mask, __global int_tp* mask, __global Dtype* top_mask) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp pw = index % pooled_width;\n    const int_tp ph = (index / pooled_width) % pooled_height;\n    const int_tp c = (index / pooled_width / pooled_height) % channels;\n    const int_tp n = index / pooled_width / pooled_height / channels;\n    int_tp hstart = ph * stride_h - pad_h;\n    int_tp wstart = pw * stride_w - pad_w;\n    const int_tp hend = min(hstart + kernel_h, height);\n    const int_tp wend = min(wstart + kernel_w, width);\n    hstart = max(hstart, (int_tp)0);\n    wstart = max(wstart, (int_tp)0);\n    Dtype maxval = -FLT_MAX;\n    int_tp maxidx = -1;\n    __global const Dtype* bottom_slice = bottom_data\n        + (n * channels + c) * height * width;\n    for (int_tp h = hstart; h < hend; ++h) {\n      for (int_tp w = wstart; w < wend; ++w) {\n        if (bottom_slice[h * width + w] > maxval) {\n          maxidx = h * width + w;\n          maxval = bottom_slice[maxidx];\n        }\n      }\n    }\n    top_data[index] = maxval;\n    if (use_mask == 1) {\n      mask[index] = maxidx;\n    } else {\n      top_mask[index] = maxidx;\n    }\n  }\n}\n\n__kernel void TEMPLATE(ave_pool_forward,Dtype)(\n    const int_tp nthreads, __global const Dtype* const bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp stride_h, const int_tp stride_w, const int_tp pad_h,\n    const int_tp pad_w, __global Dtype* top_data) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    {\n      const int_tp pw = index % pooled_width;\n      const int_tp ph = (index / pooled_width) % pooled_height;\n      const int_tp c = (index / pooled_width / pooled_height) % channels;\n      const int_tp n = index / pooled_width / pooled_height / channels;\n      int_tp hstart = ph * stride_h - pad_h;\n      int_tp wstart = pw * stride_w - pad_w;\n      int_tp hend = min(hstart + kernel_h, height + pad_h);\n      int_tp wend = min(wstart + kernel_w, width + pad_w);\n      const int_tp pool_size = (hend - hstart) * (wend - wstart);\n      hstart = max(hstart, (int_tp)0);\n      wstart = max(wstart, (int_tp)0);\n      hend = min(hend, height);\n      wend = min(wend, width);\n      Dtype aveval = 0;\n      __global const Dtype* bottom_slice = bottom_data\n          + (n * channels + c) * height * width;\n      for (int_tp h = hstart; h < hend; ++h) {\n        for (int_tp w = wstart; w < wend; ++w) {\n          aveval += bottom_slice[h * width + w];\n        }\n      }\n      top_data[index] = aveval / pool_size;\n    }\n  }\n}\n\n__kernel void TEMPLATE(sto_pool_forward_train,Dtype)(\n    const int_tp nthreads, __global const Dtype* bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp stride_h, const int_tp stride_w,\n    __global Dtype* rand_idx,\n    __global Dtype* top_data) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp pw = index % pooled_width;\n    const int_tp ph = (index / pooled_width) % pooled_height;\n    const int_tp c = (index / pooled_width / pooled_height) % channels;\n    const int_tp n = index / pooled_width / pooled_height / channels;\n    const int_tp hstart = ph * stride_h;\n    const int_tp hend = min(hstart + kernel_h, height);\n    const int_tp wstart = pw * stride_w;\n    const int_tp wend = min(wstart + kernel_w, width);\n    Dtype cumsum = 0.;\n    __global const Dtype* bottom_slice = bottom_data\n        + (n * channels + c) * height * width;\n    // First pass: get sum\n    for (int_tp h = hstart; h < hend; ++h) {\n      for (int_tp w = wstart; w < wend; ++w) {\n        cumsum += bottom_slice[h * width + w];\n      }\n    }\n    const float thres = rand_idx[index] * cumsum;\n    // Second pass: get value, and set index.\n    cumsum = 0;\n    for (int_tp h = hstart; h < hend; ++h) {\n      for (int_tp w = wstart; w < wend; ++w) {\n        cumsum += bottom_slice[h * width + w];\n        if (cumsum >= thres) {\n          rand_idx[index] = ((n * channels + c) * height + h) * width + w;\n          top_data[index] = bottom_slice[h * width + w];\n          h = hend;\n          w = wend;\n        }\n      }\n    }\n  }\n}\n\n__kernel void TEMPLATE(sto_pool_forward_test,Dtype)(\n    const int_tp nthreads, __global const Dtype* const bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp stride_h, const int_tp stride_w,\n    __global Dtype* top_data) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp pw = index % pooled_width;\n    const int_tp ph = (index / pooled_width) % pooled_height;\n    const int_tp c = (index / pooled_width / pooled_height) % channels;\n    const int_tp n = index / pooled_width / pooled_height / channels;\n    const int_tp hstart = ph * stride_h;\n    const int_tp hend = min(hstart + kernel_h, height);\n    const int_tp wstart = pw * stride_w;\n    const int_tp wend = min(wstart + kernel_w, width);\n    // We set cumsum to be 0 to avoid divide-by-zero problems\n    Dtype cumsum = FLT_MIN;\n    Dtype cumvalues = 0.;\n    __global const Dtype* bottom_slice = bottom_data\n        + (n * channels + c) * height * width;\n    // First pass: get sum\n    for (int_tp h = hstart; h < hend; ++h) {\n      for (int_tp w = wstart; w < wend; ++w) {\n        cumsum += bottom_slice[h * width + w];\n        cumvalues += bottom_slice[h * width + w] * bottom_slice[h * width + w];\n      }\n    }\n    top_data[index] = cumvalues / cumsum;\n  }\n}\n\n__kernel void TEMPLATE(max_pool_backward,Dtype)(const int_tp nthreads,\n                                                __global const Dtype* top_diff,\n                                                const int use_mask,\n                                                __global const int_tp* mask,\n                                                __global const Dtype* top_mask,\n                                                const int_tp num,\n                                                const int_tp channels,\n                                                const int_tp height,\n                                                const int_tp width,\n                                                const int_tp pooled_height,\n                                                const int_tp pooled_width,\n                                                const int_tp kernel_h,\n                                                const int_tp kernel_w,\n                                                const int_tp stride_h,\n                                                const int_tp stride_w,\n                                                const int_tp pad_h,\n                                                const int_tp pad_w,\n                                                __global Dtype* bottom_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    // find out the local index\n    // find out the local offset\n    const int_tp w = index % width;\n    const int_tp h = (index / width) % height;\n    const int_tp c = (index / width / height) % channels;\n    const int_tp n = index / width / height / channels;\n    const int_tp phstart =\n        (h + pad_h < kernel_h) ? 0 : (h + pad_h - kernel_h) / stride_h + 1;\n    const int_tp phend = min((h + pad_h) / stride_h + 1, pooled_height);\n    const int_tp pwstart =\n        (w + pad_w < kernel_w) ? 0 : (w + pad_w - kernel_w) / stride_w + 1;\n    const int_tp pwend = min((w + pad_w) / stride_w + 1, pooled_width);\n    Dtype gradient = 0;\n    const int_tp offset = (n * channels + c) * pooled_height * pooled_width;\n    __global const Dtype* top_diff_slice = top_diff + offset;\n    if (use_mask == 1) {\n      __global const int_tp* mask_slice = mask + offset;\n      for (int_tp ph = phstart; ph < phend; ++ph) {\n        for (int_tp pw = pwstart; pw < pwend; ++pw) {\n          if (mask_slice[ph * pooled_width + pw] == h * width + w) {\n            gradient += top_diff_slice[ph * pooled_width + pw];\n          }\n        }\n      }\n    } else {\n      __global const Dtype* top_mask_slice = top_mask + offset;\n      for (int_tp ph = phstart; ph < phend; ++ph) {\n        for (int_tp pw = pwstart; pw < pwend; ++pw) {\n          if (top_mask_slice[ph * pooled_width + pw] == h * width + w) {\n            gradient += top_diff_slice[ph * pooled_width + pw];\n          }\n        }\n      }\n    }\n    bottom_diff[index] = gradient;\n  }\n}\n\n__kernel void TEMPLATE(ave_pool_backward,Dtype)(const int_tp nthreads,\n                                                __global const Dtype* top_diff,\n                                                const int_tp num,\n                                                const int_tp channels,\n                                                const int_tp height,\n                                                const int_tp width,\n                                                const int_tp pooled_height,\n                                                const int_tp pooled_width,\n                                                const int_tp kernel_h,\n                                                const int_tp kernel_w,\n                                                const int_tp stride_h,\n                                                const int_tp stride_w,\n                                                const int_tp pad_h,\n                                                const int_tp pad_w,\n                                                __global Dtype* bottom_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    // find out the local index\n    // find out the local offset\n    const int_tp w = index % width + pad_w;\n    const int_tp h = (index / width) % height + pad_h;\n    const int_tp c = (index / width / height) % channels;\n    const int_tp n = index / width / height / channels;\n    const int_tp phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;\n    const int_tp phend = min(h / stride_h + 1, pooled_height);\n    const int_tp pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;\n    const int_tp pwend = min(w / stride_w + 1, pooled_width);\n    Dtype gradient = 0.0;\n    __global const Dtype* const top_diff_slice = top_diff\n        + (n * channels + c) * pooled_height * pooled_width;\n    for (int_tp ph = phstart; ph < phend; ++ph) {\n      for (int_tp pw = pwstart; pw < pwend; ++pw) {\n        // figure out the pooling size\n        int_tp hstart = ph * stride_h - pad_h;\n        int_tp wstart = pw * stride_w - pad_w;\n        int_tp hend = min(hstart + kernel_h, height + pad_h);\n        int_tp wend = min(wstart + kernel_w, width + pad_w);\n        int_tp pool_size = (hend - hstart) * (wend - wstart);\n        gradient += top_diff_slice[ph * pooled_width + pw] / pool_size;\n      }\n    }\n    bottom_diff[index] = gradient;\n  }\n}\n\n__kernel void TEMPLATE(sto_pool_backward,Dtype)(\n    const int_tp nthreads, __global const Dtype* rand_idx,\n    __global const Dtype* const top_diff, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width,\n    const int_tp kernel_h, const int_tp kernel_w, const int_tp stride_h,\n    const int_tp stride_w, __global Dtype* bottom_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads; index +=\n      get_global_size(0)) {\n    // find out the local index\n    // find out the local offset\n    const int_tp w = index % width;\n    const int_tp h = (index / width) % height;\n    const int_tp c = (index / width / height) % channels;\n    const int_tp n = index / width / height / channels;\n    const int_tp phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;\n    const int_tp phend = min(h / stride_h + 1, pooled_height);\n    const int_tp pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;\n    const int_tp pwend = min(w / stride_w + 1, pooled_width);\n    Dtype gradient = 0.0;\n    __global const Dtype* rand_idx_slice = rand_idx\n        + (n * channels + c) * pooled_height * pooled_width;\n    __global const Dtype* top_diff_slice = top_diff\n        + (n * channels + c) * pooled_height * pooled_width;\n    for (int_tp ph = phstart; ph < phend; ++ph) {\n      for (int_tp pw = pwstart; pw < pwend; ++pw) {\n        gradient += top_diff_slice[ph * pooled_width + pw]\n            * (index == (int_tp) (rand_idx_slice[ph * pooled_width + pw])?1.0:0.0);\n      }\n    }\n    bottom_diff[index] = gradient;\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(max_pool_forward_nd, Dtype)(const int_tp n,\n                                                   const int_tp num_axes,\n                                                   __global const Dtype* bottom_data,\n                                                   const int_tp channels,\n                                                   __global const int_tp* size,\n                                                   __global const int_tp* pooled_size,\n                                                   __global const int_tp* kernel_size,\n                                                   __global const int_tp* ext_kernel_size,\n                                                   __global const int_tp* stride,\n                                                   __global const int_tp* dilation,\n                                                   __global const int_tp* pad,\n                                                   __global Dtype* top_data,\n                                                   const int use_mask,\n                                                   __global int_tp* mask, __global Dtype* top_mask) {\n  int_tp d_idx[6];\n  int_tp d_start[6];\n  int_tp d_end[6];\n  int_tp d_iter[6];\n  int_tp i;\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    int_tp offset = 1;\n    int_tp num = index;\n\n    bool do_continue = false;\n\n    for (i = num_axes - 1; i >= 0; --i) {\n      d_idx[i] = num % pooled_size[i];\n      d_start[i] = d_idx[i] * stride[i] - pad[i];\n      d_end[i] = min(d_start[i] + ext_kernel_size[i], size[i]);\n      d_start[i] = max(d_start[i], (int_tp)0);\n      num /= pooled_size[i];\n      offset *= size[i];\n      d_iter[i] = d_start[i];\n\n      if (d_start[i] >= d_end[i]) {\n        top_data[index] = -FLT_MAX;\n        if (use_mask) {\n          mask[index] = -1;\n        } else {\n          top_mask[index] = -1;\n        }\n        do_continue = true;\n      }\n    }\n\n    if(do_continue) {\n      continue;\n    }\n\n    int_tp chan = num % channels;\n    num /= channels;\n    offset *= (num * channels + chan);\n\n    Dtype maxval = -FLT_MAX;\n    int_tp maxidx = -1;\n    int_tp final_offset = 0;\n\n    bool incremented;\n    do {\n      final_offset = offset;\n      int_tp size_prod = 1;\n      for (i = num_axes - 1; i >= 0; --i) {\n        final_offset += d_iter[i] * size_prod;\n        size_prod *= size[i];\n      }\n\n      if (bottom_data[final_offset] > maxval) {\n        maxidx = final_offset;\n        maxval = bottom_data[maxidx];\n      }\n\n      incremented = false;\n      for (i = num_axes - 1; i >= 0; --i) {\n        if (d_iter[i] >= d_end[i] - dilation[i]) {\n          d_iter[i] = d_start[i];\n        } else {\n          d_iter[i] += dilation[i];\n          incremented = true;\n          break;\n        }\n      }\n    } while (incremented);\n\n    top_data[index] = maxval;\n    if (use_mask == 1) {\n      mask[index] = maxidx;\n    } else {\n      top_mask[index] = maxidx;\n    }\n  }\n}\n\n\n__kernel void TEMPLATE(max_pool_backward_nd, Dtype)(const int_tp n,\n                                                    const int_tp num_axes,\n                                                    __global const Dtype* top_diff,\n                                                    const int use_mask,\n                                                    __global const int_tp* mask,\n                                                    __global const Dtype* top_mask,\n                                                    const int_tp channels,\n                                                    __global const int_tp* size,\n                                                    __global const int_tp* pooled_size,\n                                                    __global const int_tp* kernel_size,\n                                                    __global const int_tp* ext_kernel_size,\n                                                    __global const int_tp* stride,\n                                                    __global const int_tp* dilation,\n                                                    __global const int_tp* pad,\n                                                    __global Dtype* bottom_diff) {\n  int_tp d_idx[6];\n  int_tp d_start[6];\n  int_tp d_end[6];\n  int_tp d_iter[6];\n  int_tp i;\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    // find out the local index\n    // find out the local offset\n    int_tp offset = 1;\n    int_tp num = index;\n    for (i = num_axes - 1; i >= 0; --i) {\n      d_idx[i] = num % size[i];\n      if (dilation[i] > 1) {\n        d_start[i] =\n            (d_idx[i] < ext_kernel_size[i]) ?\n                d_idx[i] % dilation[i] : (d_idx[i] - ext_kernel_size[i]) + 1;\n        d_end[i] =\n            (d_idx[i] >= pooled_size[i]) ?\n                (pooled_size[i] - 1)\n                    - (pooled_size[i] - 1 - d_start[i]) % dilation[i] :\n                d_idx[i];\n      } else {\n        d_start[i] =\n            (d_idx[i] + pad[i] < kernel_size[i]) ?\n                0 : (d_idx[i] + pad[i] - kernel_size[i]) / stride[i] + 1;\n        d_end[i] = min((int_tp) ((d_idx[i] + pad[i]) / stride[i] + 1),\n                       (int_tp) (pooled_size[i]));\n      }\n      num /= size[i];\n      offset *= pooled_size[i];\n      d_iter[i] = d_start[i];\n\n      if (d_start[i] > d_end[i]) {\n        bottom_diff[index] = 0;\n        return;\n      }\n    }\n    int_tp chan = num % channels;\n    num /= channels;\n    offset *= (num * channels + chan);\n\n    Dtype gradient = 0;\n    int_tp final_offset = 0;\n    int_tp im_offset = 0;\n\n    bool incremented;\n    do {\n      final_offset = offset;\n      im_offset = 0;\n      int_tp size_prod = 1;\n      int_tp pooled_size_prod = 1;\n      for (i = num_axes - 1; i >= 0; --i) {\n        final_offset += d_iter[i] * pooled_size_prod;\n        im_offset += d_idx[i] * size_prod;\n        size_prod *= size[i];\n        pooled_size_prod *= pooled_size[i];\n      }\n\n      if (use_mask) {\n        if (mask[final_offset] == im_offset) {\n          gradient += top_diff[final_offset];\n        }\n      } else {\n        if (top_mask[final_offset] == im_offset) {\n          gradient += top_diff[final_offset];\n        }\n      }\n\n      incremented = false;\n      for (i = num_axes - 1; i >= 0; --i) {\n        if (d_iter[i] > d_end[i] - dilation[i]) {\n          d_iter[i] = d_start[i];\n        } else {\n          d_iter[i] += dilation[i];\n          incremented = true;\n          break;\n        }\n      }\n    } while (incremented);\n    bottom_diff[index] = gradient;\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(max_pool_forward_sk,Dtype)(const int_tp nthreads,\n__global Dtype* bottom_data,\n                                                  const int_tp num,\n                                                  const int_tp channels,\n                                                  const int_tp height,\n                                                  const int_tp width,\n                                                  const int_tp pooled_height,\n                                                  const int_tp pooled_width,\n                                                  const int_tp kernel_h,\n                                                  const int_tp kernel_w,\n                                                  const int_tp ext_kernel_h,\n                                                  const int_tp ext_kernel_w,\n                                                  const int_tp stride_h,\n                                                  const int_tp stride_w,\n                                                  const int_tp dilation_h,\n                                                  const int_tp dilation_w,\n                                                  const int_tp pad_h,\n                                                  const int_tp pad_w,\n                                                  __global Dtype* top_data,\n                                                  const int use_mask,\n                                                  __global int_tp* mask,\n                                                  __global Dtype* top_mask) {\n  for (int_tp index = get_global_id(0); index < nthreads; index +=\n      get_global_size(0)) {\n    int_tp pw = index % pooled_width;\n    int_tp ph = (index / pooled_width) % pooled_height;\n    int_tp c = (index / pooled_width / pooled_height) % channels;\n    int_tp n = index / pooled_width / pooled_height / channels;\n    int_tp hstart = ph * stride_h - pad_h;\n    int_tp wstart = pw * stride_w - pad_w;\n    int_tp hend = min(hstart + ext_kernel_h, height);\n    int_tp wend = min(wstart + ext_kernel_w, width);\n    hstart = max(hstart, (int_tp) 0);\n    wstart = max(wstart, (int_tp) 0);\n    Dtype maxval = -FLT_MAX;\n    int_tp maxidx = -1;\n    __global Dtype* bottom_data_ptr = bottom_data\n        + (n * channels + c) * height * width;\n    for (int_tp h = hstart; h < hend; h += dilation_h) {\n      for (int_tp w = wstart; w < wend; w += dilation_w) {\n        if (bottom_data_ptr[h * width + w] > maxval) {\n          maxidx = h * width + w;\n          maxval = bottom_data_ptr[maxidx];\n        }\n      }\n    }\n    top_data[index] = maxval;\n    if (use_mask == 1) {\n      mask[index] = maxidx;\n    } else {\n      top_mask[index] = maxidx;\n    }\n  }\n}\n\n__kernel void TEMPLATE(max_pool_backward_sk,Dtype)(\n    const int_tp nthreads, __global const Dtype* top_diff, const int use_mask,\n    __global const int_tp* mask, __global const Dtype* top_mask,\n    const int_tp num, const int_tp channels, const int_tp height,\n    const int_tp width, const int_tp pooled_height, const int_tp pooled_width,\n    const int_tp kernel_h, const int_tp kernel_w, const int_tp ext_kernel_h,\n    const int_tp ext_kernel_w, const int_tp stride_h, const int_tp stride_w,\n    const int_tp dilation_h, const int_tp dilation_w, const int_tp pad_h,\n    const int_tp pad_w,\n    __global Dtype* bottom_diff) {\n\n  for (int_tp index = get_global_id(0); index < nthreads; index +=\n      get_global_size(0)) {\n\n    __global const int_tp* mask_ptr = mask;\n    __global const Dtype* top_diff_ptr = top_diff;\n\n// find out the local index\n// find out the local offset\n    int_tp w = index % width;\n    int_tp h = (index / width) % height;\n    int_tp c = (index / width / height) % channels;\n    int_tp n = index / width / height / channels;\n\n    int_tp pooled_height_1 = pooled_height - 1;\n    int_tp pooled_width_1 = pooled_width - 1;\n    int_tp phstart =\n        (h < ext_kernel_h) ? h % dilation_h : (h - ext_kernel_h) + 1;\n    int_tp phend =\n        (h >= pooled_height) ?\n            pooled_height_1 - (pooled_height_1 - phstart) % dilation_h : h;\n    int_tp pwstart =\n        (w < ext_kernel_w) ? w % dilation_w : (w - ext_kernel_w) + 1;\n    int_tp pwend =\n        (w >= pooled_width) ?\n            pooled_width_1 - (pooled_width_1 - pwstart) % dilation_w : w;\n\n    Dtype gradient = 0;\n    int_tp offset = (n * channels + c) * pooled_height * pooled_width;\n    top_diff_ptr += offset;\n    if (use_mask == 1) {\n      mask_ptr += offset;\n      for (int_tp ph = phstart; ph <= phend; ph += dilation_h) {\n        for (int_tp pw = pwstart; pw <= pwend; pw += dilation_w) {\n          if (mask_ptr[ph * pooled_width + pw] == h * width + w) {\n            gradient += top_diff_ptr[ph * pooled_width + pw];\n          }\n        }\n      }\n    } else {\n      for (int_tp ph = phstart; ph <= phend; ph += dilation_h) {\n        for (int_tp pw = pwstart; pw <= pwend; pw += dilation_w) {\n          if (top_mask[ph * pooled_width + pw] == h * width + w) {\n            gradient += top_diff_ptr[ph * pooled_width + pw];\n          }\n        }\n      }\n    }\n    bottom_diff[index] = gradient;\n  }\n}\n\n__kernel void TEMPLATE(ave_pool_forward_sk,Dtype)(\n    const int_tp nthreads, __global const Dtype* bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width,\n    const int_tp kernel_h, const int_tp kernel_w, const int_tp ext_kernel_h,\n    const int_tp ext_kernel_w, const int_tp stride_h, const int_tp stride_w,\n    const int_tp dilation_h, const int_tp dilation_w, const int_tp pad_h,\n    const int_tp pad_w,\n    __global Dtype* top_data) {\n\n  for (int_tp index = get_global_id(0); index < nthreads; index +=\n      get_global_size(0)) {\n\n    int_tp pw = index % pooled_width;\n    int_tp ph = (index / pooled_width) % pooled_height;\n    int_tp c = (index / pooled_width / pooled_height) % channels;\n    int_tp n = index / pooled_width / pooled_height / channels;\n    int_tp hstart = ph * stride_h - pad_h;\n    int_tp wstart = pw * stride_w - pad_w;\n    int_tp hend = min(hstart + ext_kernel_h, height + pad_h);\n    int_tp wend = min(wstart + ext_kernel_w, width + pad_w);\n    hstart = max(hstart, (int_tp)0);\n    wstart = max(wstart, (int_tp)0);\n    hend = min(hend, height);\n    wend = min(wend, width);\n    Dtype aveval = 0;\n    __global const Dtype* bottom_data_ptr = bottom_data;\n    bottom_data_ptr += (n * channels + c) * height * width;\n    int_tp pool_size = 0;\n    for (int_tp h = hstart; h < hend; ++h) {\n      for (int_tp w = wstart; w < wend; ++w) {\n        aveval += bottom_data_ptr[h * width + w];\n        ++pool_size;\n      }\n    }\n    top_data[index] = aveval / pool_size;\n  }\n}\n\n__kernel void TEMPLATE(sto_pool_forward_train_sk,Dtype)(\n    const int_tp nthreads, __global const Dtype* bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width,\n    const int_tp kernel_h, const int_tp kernel_w, const int_tp ext_kernel_h,\n    const int_tp ext_kernel_w, const int_tp stride_h, const int_tp stride_w,\n    const int_tp dilation_h, const int_tp dilation_w, __global Dtype* rand_idx,\n    __global Dtype* top_data) {\n\n  for (int_tp index = get_global_id(0); index < nthreads; index +=\n      get_global_size(0)) {\n    int_tp pw = index % pooled_width;\n    int_tp ph = (index / pooled_width) % pooled_height;\n    int_tp c = (index / pooled_width / pooled_height) % channels;\n    int_tp n = index / pooled_width / pooled_height / channels;\n    int_tp hstart = ph * stride_h;\n    int_tp hend = min(hstart + ext_kernel_h, height);\n    int_tp wstart = pw * stride_w;\n    int_tp wend = min(wstart + ext_kernel_w, width);\n    Dtype cumsum = 0.;\n    __global const Dtype* bottom_data_ptr = bottom_data;\n    bottom_data_ptr += (n * channels + c) * height * width;\n    // First pass: get sum\n    for (int_tp h = hstart; h < hend; h += dilation_h) {\n      for (int_tp w = wstart; w < wend; w += dilation_w) {\n        cumsum += bottom_data_ptr[h * width + w];\n      }\n    }\n    float thres = rand_idx[index] * cumsum;\n    // Second pass: get value, and set index.\n    cumsum = 0;\n    for (int_tp h = hstart; h < hend; h += dilation_h) {\n      for (int_tp w = wstart; w < wend; w += dilation_w) {\n        cumsum += bottom_data_ptr[h * width + w];\n        if (cumsum >= thres) {\n          rand_idx[index] = ((n * channels + c) * height + h) * width + w;\n          top_data[index] = bottom_data_ptr[h * width + w];\n          h = hend;\n          w = wend;\n        }\n      }\n    }\n  }\n}\n\n__kernel void TEMPLATE(sto_pool_forward_test_sk,Dtype)(\n    const int_tp nthreads, __global const Dtype* bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width,\n    const int_tp kernel_h, const int_tp kernel_w, const int_tp ext_kernel_h,\n    const int_tp ext_kernel_w, const int_tp stride_h, const int_tp stride_w,\n    const int_tp dilation_h, const int_tp dilation_w,\n    __global Dtype* top_data) {\n\n  for (int_tp index = get_global_id(0); index < nthreads; index +=\n      get_global_size(0)) {\n    int_tp pw = index % pooled_width;\n    int_tp ph = (index / pooled_width) % pooled_height;\n    int_tp c = (index / pooled_width / pooled_height) % channels;\n    int_tp n = index / pooled_width / pooled_height / channels;\n    int_tp hstart = ph * stride_h;\n    int_tp hend = min(hstart + ext_kernel_h, height);\n    int_tp wstart = pw * stride_w;\n    int_tp wend = min(wstart + ext_kernel_w, width);\n    // We set cumsum to be 0 to avoid divide-by-zero problems\n    Dtype cumsum = FLT_MIN;\n    Dtype cumvalues = 0.;\n    __global const Dtype* bottom_data_ptr = bottom_data;\n    bottom_data_ptr += (n * channels + c) * height * width;\n    // First pass: get sum\n    for (int_tp h = hstart; h < hend; h += dilation_h) {\n      for (int_tp w = wstart; w < wend; w += dilation_w) {\n        cumsum += bottom_data_ptr[h * width + w];\n        cumvalues += bottom_data_ptr[h * width + w]\n            * bottom_data_ptr[h * width + w];\n      }\n    }\n    top_data[index] = cumvalues / cumsum;\n  }\n\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(slice,Dtype)(const int_tp nthreads,\n                                    __global const Dtype* in_data,\n                                    const int forward, const int_tp num_slices,\n                                    const int_tp slice_size,\n                                    const int_tp bottom_slice_axis,\n                                    const int_tp top_slice_axis,\n                                    const int_tp offset_slice_axis,\n                                    __global Dtype* out_data) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp total_slice_size = slice_size * top_slice_axis;\n    const int_tp slice_num = index / total_slice_size;\n    const int_tp slice_index = index % total_slice_size;\n    const int_tp bottom_index = slice_index\n        + (slice_num * bottom_slice_axis + offset_slice_axis) * slice_size;\n    if (forward == 1) {\n      out_data[index] = in_data[bottom_index];\n    } else {\n      out_data[bottom_index] = in_data[index];\n    }\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(softmax_loss_forward,Dtype)(\n    int_tp n, __global const Dtype* prob_data, __global const Dtype* label,\n    __global Dtype* loss,\n    const int_tp num, const int_tp dim, const int_tp spatial_dim,\n    const int has_ignore_label_, const int_tp ignore_label_,\n    __global Dtype* counts) {\n\n  for (int_tp index = get_global_id(0); index < n;\n      index += get_global_size(0)) {\n    const int_tp n = index / spatial_dim;\n    const int_tp s = index % spatial_dim;\n    const int_tp label_value = (int_tp) (label[n * spatial_dim + s]);\n    if (has_ignore_label_ == 1 && label_value == ignore_label_) {\n      loss[index] = 0;\n      counts[index] = 0;\n    } else {\n      loss[index] = -log((Dtype)(\n          max((Dtype) (prob_data[n * dim + label_value * spatial_dim + s]),\n              (Dtype) FLT_MIN)));\n      counts[index] = 1;\n    }\n  }\n}\n\n__kernel void TEMPLATE(softmax_loss_backward,Dtype)(const int_tp nthreads,\n                                                    __global const Dtype* top,\n                                                    __global const Dtype* label,\n                                                    __global Dtype* bottom_diff,\n                                                    const int_tp num,\n                                                    const int_tp dim,\n                                                    const int_tp spatial_dim,\n                                                    const int has_ignore_label_,\n                                                    const int_tp ignore_label_,\n                                                    __global Dtype* counts) {\n\n  const int_tp channels = dim / spatial_dim;\n\n  for (int_tp index = get_global_id(0); index < nthreads; index +=\n      get_global_size(0)) {\n\n    const int_tp n = index / spatial_dim;\n    const int_tp s = index % spatial_dim;\n    const int_tp label_value = (int_tp) (label[n * spatial_dim + s]);\n\n    if (has_ignore_label_ == 1 && label_value == ignore_label_) {\n      for (int_tp c = 0; c < channels; ++c) {\n        bottom_diff[n * dim + c * spatial_dim + s] = 0;\n      }\n      counts[index] = 0;\n    } else {\n      bottom_diff[n * dim + label_value * spatial_dim + s] -= 1;\n      counts[index] = 1;\n    }\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(ada_delta_update,Dtype)(int_tp N, __global Dtype* g,\n                                               __global Dtype* h,\n                                               __global Dtype* h2,\n                                               Dtype momentum,\n                                               Dtype delta,\n                                               Dtype local_rate) {\n  for (int_tp i = get_global_id(0); i < N; i += get_global_size(0)) {\n    Dtype gi = g[i];\n    Dtype hi = h[i] = momentum * h[i] + (1.0 - momentum) * gi * gi;\n    gi = gi * sqrt((h2[i] + delta) / (hi + delta));\n    h2[i] = momentum * h2[i] + (1.0 - momentum) * gi * gi;\n    g[i] = local_rate * gi;\n  }\n}\n\n__kernel void TEMPLATE(ada_grad_update,Dtype)(int_tp N, __global Dtype* g,\n                                              __global Dtype* h,\n                                              Dtype delta,\n                                              Dtype local_rate) {\n  for (int_tp i = get_global_id(0); i < N; i += get_global_size(0)) {\n    Dtype gi = g[i];\n    Dtype hi = h[i] = h[i] + gi * gi;\n    g[i] = local_rate * gi / (sqrt(hi) + delta);\n  }\n}\n\n__kernel void TEMPLATE(adam_update,Dtype)(int_tp N, __global Dtype* g,\n                                          __global Dtype* m,\n                                          __global Dtype* v,\n                                          Dtype beta1,\n                                          Dtype beta2,\n                                          Dtype eps_hat,\n                                          Dtype corrected_local_rate) {\n  for (int_tp i = get_global_id(0); i < N; i += get_global_size(0)) {\n    Dtype gi = g[i];\n    Dtype mi = m[i] = m[i] * beta1 + gi * (1 - beta1);\n    Dtype vi = v[i] = v[i] * beta2 + gi * gi * (1 - beta2);\n    g[i] = corrected_local_rate * mi / (sqrt(vi) + eps_hat);\n  }\n}\n\n\n__kernel void TEMPLATE(nesterov_update,Dtype)(int_tp N, __global Dtype* g,\n                                              __global Dtype* h,\n                                              Dtype momentum,\n                                              Dtype local_rate) {\n  for (int_tp i = get_global_id(0); i < N; i += get_global_size(0)) {\n    Dtype hi = h[i];\n    Dtype hi_new = h[i] = momentum * hi + local_rate * g[i];\n    g[i] = (1 + momentum) * hi_new - momentum * hi;\n  }\n}\n\n__kernel void TEMPLATE(rms_prop_update,Dtype)(int_tp N, __global Dtype* g,\n                                              __global Dtype* h,\n                                              Dtype rms_decay,\n                                              Dtype delta,\n                                              Dtype local_rate) {\n  for (int_tp i = get_global_id(0); i < N; i += get_global_size(0)) {\n    Dtype gi = g[i];\n    Dtype hi = h[i] = rms_decay * h[i] + (1 - rms_decay) * gi * gi;\n    g[i] = local_rate * g[i] / (sqrt(hi) + delta);\n  }\n}\n\n__kernel void TEMPLATE(sgd_update,Dtype)(int_tp N, __global Dtype* g,\n                                         __global Dtype* h,\n                                         Dtype momentum,\n                                         Dtype local_rate) {\n  for (int_tp i = get_global_id(0); i < N; i += get_global_size(0)) {\n    g[i] = h[i] = momentum * h[i] + local_rate * g[i];\n  }\n}",   // NOLINT
    "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n\n__kernel void TEMPLATE(tile,Dtype)(const int_tp nthreads, __global const Dtype* bottom_data,\n                                   const int_tp tile_size, const int_tp num_tiles,\n                                   const int_tp bottom_tile_axis,\n                                   __global Dtype* top_data) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp d = index % tile_size;\n    const int_tp b = (index / tile_size / num_tiles) % bottom_tile_axis;\n    const int_tp n = index / tile_size / num_tiles / bottom_tile_axis;\n    const int_tp bottom_index = (n * bottom_tile_axis + b) * tile_size + d;\n    top_data[index] = bottom_data[bottom_index];\n  }\n}\n\n\n__kernel void TEMPLATE(tile_backward,Dtype)(const int_tp nthreads,\n                                            __global const Dtype* top_diff,\n                                            const int_tp tile_size,\n                                            const int_tp num_tiles,\n                                            const int_tp bottom_tile_axis,\n                                            __global Dtype* bottom_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp d = index % tile_size;\n    const int_tp b = (index / tile_size) % bottom_tile_axis;\n    const int_tp n = index / tile_size / bottom_tile_axis;\n    bottom_diff[index] = 0;\n    int_tp top_index = (n * num_tiles * bottom_tile_axis + b) * tile_size + d;\n    for (int_tp t = 0; t < num_tiles; ++t) {\n      bottom_diff[index] += top_diff[top_index];\n      top_index += bottom_tile_axis * tile_size;\n    }\n  }\n}"   // NOLINT
};
static std::string cl_kernel_names[] = {
    "activation",   // NOLINT
    "auxiliary",   // NOLINT
    "batch_reindex",   // NOLINT
    "benchmark",   // NOLINT
    "bias",   // NOLINT
    "bnll",   // NOLINT
    "channel",   // NOLINT
    "concat",   // NOLINT
    "contrastive_loss",   // NOLINT
    "conv_layer_spatial",   // NOLINT
    "conv_spatial_helper",   // NOLINT
    "crop",   // NOLINT
    "dropout",   // NOLINT
    "eltwise",   // NOLINT
    "elu",   // NOLINT
    "embed",   // NOLINT
    "fft",   // NOLINT
    "fillbuffer",   // NOLINT
    "im2col",   // NOLINT
    "im2col_nd",   // NOLINT
    "lrn",   // NOLINT
    "lstm_unit",   // NOLINT
    "math",   // NOLINT
    "mergecrop",   // NOLINT
    "pooling",   // NOLINT
    "pooling_nd",   // NOLINT
    "pooling_sk",   // NOLINT
    "slice",   // NOLINT
    "softmax_loss",   // NOLINT
    "solvers",   // NOLINT
    "tile"   // NOLINT
};
viennacl::ocl::program & RegisterKernels(viennacl::ocl::context *ctx) {
  std::stringstream ss;
#ifdef USE_INDEX_64
  ss << header << "\n\n";  // NOLINT
  ss << definitions_64 << "\n\n";  // NOLINT
#else
  ss << header << "\n\n";  // NOLINT
  ss << definitions_32 << "\n\n";  // NOLINT
#endif
  ss << "#define Dtype float" << "\n\n";  // NOLINT
  ss << "#define Dtype2 float2" << "\n\n";  // NOLINT
  ss << "#define Dtype4 float4" << "\n\n";  // NOLINT
  ss << "#define Dtype8 float8" << "\n\n";  // NOLINT
  ss << "#define Dtype16 float16" << "\n\n";  // NOLINT
  ss << "#define TYPE TYPE_FLOAT" << "\n\n";  // NOLINT
  for (int i = 0; i < std::extent<decltype(cl_kernels)>::value; ++i) {
      ss << cl_kernels[i] << "\n\n";
  }
  ss << "#ifdef DOUBLE_SUPPORT_AVAILABLE" << "\n\n";  // NOLINT
  ss << "#undef Dtype" << "\n\n";  // NOLINT
  ss << "#define Dtype double" << "\n\n";  // NOLINT
  ss << "#undef TYPE" << "\n\n";  // NOLINT
  ss << "#define TYPE TYPE_DOUBLE" << "\n\n";  // NOLINT
  for (int i = 0; i < std::extent<decltype(cl_kernels)>::value; ++i) {
    if (cl_kernel_names[i] != std::string("fft")) {
      ss << cl_kernels[i] << "\n\n";
    }
  }
  ss << "#endif  // DOUBLE_SUPPORT_AVAILABLE" << "\n\n";  // NOLINT
  std::string kernel_string = ss.str();
  const char* kernel_program = kernel_string.c_str();
  // ctx->build_options("-cl-fast-relaxed-math -cl-mad-enable");
#ifdef USE_FFT
  ctx->build_options("-DFFT");
#endif
  viennacl::ocl::program &program = ctx->add_program(kernel_program,
      "kernel_program");
  return program;
}
viennacl::ocl::program & submit_conv_spatial_program(
viennacl::ocl::context *ctx, string name, string options) {
  static const char* core_defines =
  "#define Dtype float\n"
  "#define Dtype2 float2\n"
  "#define Dtype4 float4\n"
  "#define Dtype8 float8\n"
  "#define Dtype16 float16\n"
  "#define OCL_KERNEL_LOOP(i, n)"
  " for (int i = get_global_id(0); i < (n); i += get_global_size(0))\n";
  string sources = core_defines;
#ifdef USE_INDEX_64
    sources += header + "\n";
    sources += definitions_64 + "\n";
#else
    sources += header + "\n";
    sources += definitions_32 + "\n";
#endif
    for (int i = 0; i < std::extent<decltype(cl_kernels)>::value; ++i) {
      if (cl_kernel_names[i] == "conv_layer_spatial") {
         sources += cl_kernels[i];
      }
    }
  ctx->build_options(options);
  viennacl::ocl::program &program = ctx->add_program(sources, name);
  return program;
}
int getKernelBundleCount() {
  return std::extent<decltype(cl_kernels)>::value;
}
template<typename Dtype>
std::string getKernelBundleSource(int index) {
  std::stringstream ss;
#ifdef USE_INDEX_64
  ss << header << "\n\n";  // NOLINT
  ss << definitions_64 << "\n\n";  // NOLINT
#else
  ss << header << "\n\n";  // NOLINT
  ss << definitions_32 << "\n\n";  // NOLINT
#endif
  if (std::is_same<Dtype, float>::value) {
    ss << "#define Dtype float" << "\n\n";  // NOLINT
    ss << "#define TYPE TYPE_FLOAT" << "\n\n";  // NOLINT
  } else {
    ss << "#ifdef DOUBLE_SUPPORT_AVAILABLE" << "\n\n";  // NOLINT
    ss << "#define Dtype double" << "\n\n";  // NOLINT
    ss << "#define TYPE TYPE_DOUBLE" << "\n\n";  // NOLINT
  }
  ss << cl_kernels[index] << "\n\n";
  if (std::is_same<Dtype, float>::value) {
  } else {
    ss << "#endif" << "\n\n";  // NOLINT
  }
  return ss.str();
}
template std::string getKernelBundleSource<float>(int index);
template std::string getKernelBundleSource<double>(int index);
std::string getKernelBundleName(int index) {
  return cl_kernel_names[index];
}
}  // namespace caffe
#endif
