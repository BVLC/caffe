<!doctype html>
<html>
  <head>
    <!-- MathJax -->
    <script type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>
      Caffe | Data
    </title>

    <link rel="icon" type="image/png" href="/images/caffeine-icon.png">

    <link rel="stylesheet" href="/stylesheets/reset.css">
    <link rel="stylesheet" href="/stylesheets/styles.css">
    <link rel="stylesheet" href="/stylesheets/pygment_trac.css">

    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-46255508-1', 'daggerfs.com');
    ga('send', 'pageview');
  </script>
    <div class="wrapper">
      <header>
        <h1 class="header"><a href="/">Caffe</a></h1>
        <p class="header">
          Deep learning framework by the <a class="header name" href="http://bvlc.eecs.berkeley.edu/">BVLC</a>
        </p>
        <p class="header">
          Created by
          <br>
          <a class="header name" href="http://daggerfs.com/">Yangqing Jia</a>
          <br>
          Lead Developer
          <br>
          <a class="header name" href="http://imaginarynumber.net/">Evan Shelhamer</a>
        <ul>
          <li>
            <a class="buttons github" href="https://github.com/BVLC/caffe">View On GitHub</a>
          </li>
        </ul>
      </header>
      <section>

      <h1 id="data-ins-and-outs">Data: Ins and Outs</h1>

<p>Data flows through Caffe as <a href="net_layer_blob.html#blob-storage-and-communication">Blobs</a>.
Data layers load input and save output by converting to and from Blob to other formats.
Common transformations like mean-subtraction and feature-scaling are done by data layer configuration.
New input types are supported by developing a new data layer â€“ the rest of the Net follows by the modularity of the Caffe layer catalogue.</p>

<p>This data layer definition</p>

<pre><code>layer {
  name: "mnist"
  # Data layer loads leveldb or lmdb storage DBs for high-throughput.
  type: "Data"
  # the 1st top is the data itself: the name is only convention
  top: "data"
  # the 2nd top is the ground truth: the name is only convention
  top: "label"
  # the Data layer configuration
  data_param {
    # path to the DB
    source: "examples/mnist/mnist_train_lmdb"
    # type of DB: LEVELDB or LMDB (LMDB supports concurrent reads)
    backend: LMDB
    # batch processing improves efficiency.
    batch_size: 64
  }
  # common data transformations
  transform_param {
    # feature scaling coefficient: this maps the [0, 255] MNIST data to [0, 1]
    scale: 0.00390625
  }
}
</code></pre>

<p>loads the MNIST digits.</p>

<p><strong>Tops and Bottoms</strong>: A data layer makes <strong>top</strong> blobs to output data to the model.
It does not have <strong>bottom</strong> blobs since it takes no input.</p>

<p><strong>Data and Label</strong>: a data layer has at least one top canonically named <strong>data</strong>.
For ground truth a second top can be defined that is canonically named <strong>label</strong>.
Both tops simply produce blobs and there is nothing inherently special about these names.
The (data, label) pairing is a convenience for classification models.</p>

<p><strong>Transformations</strong>: data preprocessing is parametrized by transformation messages within the data layer definition.</p>

<pre><code>layer {
  name: "data"
  type: "Data"
  [...]
  transform_param {
    scale: 0.1
    mean_file_size: mean.binaryproto
    # for images in particular horizontal mirroring and random cropping
    # can be done as simple data augmentations.
    mirror: 1  # 1 = on, 0 = off
    # crop a `crop_size` x `crop_size` patch:
    # - at random during training
    # - from the center during testing
    crop_size: 227
  }
}
</code></pre>

<p><strong>Prefetching</strong>: for throughput data layers fetch the next batch of data and prepare it in the background while the Net computes the current batch.</p>

<p><strong>Multiple Inputs</strong>: a Net can have multiple inputs of any number and type. Define as many data layers as needed giving each a unique name and top. Multiple inputs are useful for non-trivial ground truth: one data layer loads the actual data and the other data layer loads the ground truth in lock-step. In this arrangement both data and label can be any 4D array. Further applications of multiple inputs are found in multi-modal and sequence models. In these cases you may need to implement your own data preparation routines or a special data layer.</p>

<p><em>Improvements to data processing to add formats, generality, or helper utilities are welcome!</em></p>

<h2 id="formats">Formats</h2>

<p>Refer to the layer catalogue of <a href="layers.html#data-layers">data layers</a> for close-ups on each type of data Caffe understands.</p>

<h2 id="deployment-input">Deployment Input</h2>

<p>For on-the-fly computation deployment Nets define their inputs by <code>input</code> fields: these Nets then accept direct assignment of data for online or interactive computation.</p>


      </section>
  </div>
  </body>
</html>
